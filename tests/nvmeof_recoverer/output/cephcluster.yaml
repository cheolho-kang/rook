apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  creationTimestamp: "2024-07-02T13:46:55Z"
  finalizers:
  - cephcluster.ceph.rook.io
  generation: 2
  name: nvmeof-recoverer
  namespace: nvmeof-recoverer
  resourceVersion: "216570"
  uid: df55641c-85fc-436a-b539-ba2871b003e7
spec:
  cephVersion:
    image: quay.io/ceph/daemon-base:latest-reef-devel
  cleanupPolicy:
    sanitizeDisks: {}
  crashCollector: {}
  csi:
    cephfs: {}
    readAffinity:
      enabled: false
  dashboard:
    enabled: true
  dataDirHostPath: /data/rook-test/nvmeof-recoverer/test-9027133907398375811
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
  external: {}
  healthCheck:
    daemonHealth:
      mon:
        interval: 10s
        timeout: 15s
      osd:
        interval: 10s
      status:
        interval: 5s
  logCollector:
    enabled: true
    maxLogSize: 500M
    periodicity: daily
  mgr:
    allowMultiplePerNode: true
    count: 1
  mon:
    allowMultiplePerNode: true
    count: 1
  monitoring: {}
  network:
    connections:
      compression: {}
      encryption: {}
    multiClusterService: {}
  priorityClassNames:
    mgr: system-cluster-critical
    mon: system-node-critical
    osd: system-node-critical
  security:
    keyRotation:
      enabled: false
    kms: {}
  skipUpgradeChecks: true
  storage:
    backfillFullRatio: 0.91
    flappingRestartIntervalHours: 0
    fullRatio: 0.96
    nearFullRatio: 0.88
    nodes:
    - devices:
      - name: /dev/nvme2n1
      name: qemu1
      config:
        failureDomain: fabric-host-pbssd1
      resources: {}
    - devices:
      - name: /dev/nvme2n1
      name: qemu2
      resources: {}
    store: {}
status:
  ceph:
    capacity:
      bytesAvailable: 7681455153152
      bytesTotal: 7681511964672
      bytesUsed: 56811520
      lastUpdated: "2024-07-02T14:01:53Z"
    details:
      OSD_DOWN:
        message: 1 osds down
        severity: HEALTH_WARN
      OSD_HOST_DOWN:
        message: 1 host (1 osds) down
        severity: HEALTH_WARN
    fsid: 3c40ceef-b38b-4960-90c1-e4e9afd59143
    health: HEALTH_WARN
    lastChanged: "2024-07-02T14:00:16Z"
    lastChecked: "2024-07-02T14:01:53Z"
    previousHealth: HEALTH_OK
    versions:
      mgr:
        ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable): 1
      mon:
        ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable): 1
      osd:
        ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable): 1
      overall:
        ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable): 3
  conditions:
  - lastHeartbeatTime: "2024-07-02T14:00:52Z"
    lastTransitionTime: "2024-07-02T14:00:52Z"
    message: Processing OSD 1 on node "qemu2"
    reason: ClusterProgressing
    status: "True"
    type: Progressing
  - lastHeartbeatTime: "2024-07-02T14:01:54Z"
    lastTransitionTime: "2024-07-02T13:49:28Z"
    message: Cluster created successfully
    reason: ClusterCreated
    status: "True"
    type: Ready
  message: Cluster created successfully
  observedGeneration: 2
  phase: Ready
  state: Created
  storage:
    deviceClasses:
    - name: nvme
    osd:
      storeType:
        bluestore: 1
  version:
    image: quay.io/ceph/daemon-base:latest-reef-devel
    version: 18.2.2-1573
