2024/05/30 04:15:15 maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
2024-05-30 04:15:15.887072 I | rookcmd: starting Rook v1.13.0-alpha.0.407.ga536b363c with arguments '/usr/local/bin/rook ceph operator'
2024-05-30 04:15:15.887094 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO
2024-05-30 04:15:15.887097 I | cephcmd: starting Rook-Ceph operator
2024-05-30 04:15:16.039090 I | cephcmd: base ceph version inside the rook operator image is "ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)"
2024-05-30 04:15:16.045751 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="false" (env var)
2024-05-30 04:15:16.045786 I | operator: watching all namespaces for Ceph CRs
2024-05-30 04:15:16.045865 I | operator: setting up schemes
2024-05-30 04:15:16.049026 I | operator: setting up the controller-runtime manager
2024-05-30 04:15:16.050215 I | ceph-cluster-controller: successfully started
2024-05-30 04:15:16.052525 I | op-k8sutil: ROOK_DISABLE_DEVICE_HOTPLUG="false" (configmap)
2024-05-30 04:15:16.052546 I | ceph-cluster-controller: enabling hotplug orchestration
2024-05-30 04:15:16.052581 I | ceph-nodedaemon-controller: successfully started
2024-05-30 04:15:16.052605 I | ceph-block-pool-controller: successfully started
2024-05-30 04:15:16.052626 I | ceph-object-store-user-controller: successfully started
2024-05-30 04:15:16.052650 I | ceph-object-realm-controller: successfully started
2024-05-30 04:15:16.052662 I | ceph-object-zonegroup-controller: successfully started
2024-05-30 04:15:16.052695 I | ceph-object-zone-controller: successfully started
2024-05-30 04:15:16.052822 I | ceph-object-controller: successfully started
2024-05-30 04:15:16.052873 I | ceph-file-controller: successfully started
2024-05-30 04:15:16.052899 I | ceph-nfs-controller: successfully started
2024-05-30 04:15:16.052932 I | ceph-rbd-mirror-controller: successfully started
2024-05-30 04:15:16.052962 I | ceph-client-controller: successfully started
2024-05-30 04:15:16.052987 I | ceph-filesystem-mirror-controller: successfully started
2024-05-30 04:15:16.053017 I | operator: rook-ceph-operator-config-controller successfully started
2024-05-30 04:15:16.053051 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2024-05-30 04:15:16.053174 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2024-05-30 04:15:16.053206 I | ceph-bucket-topic: successfully started
2024-05-30 04:15:16.053219 I | ceph-bucket-notification: successfully started
2024-05-30 04:15:16.053239 I | ceph-bucket-notification: successfully started
2024-05-30 04:15:16.053249 I | ceph-fs-subvolumegroup-controller: successfully started
2024-05-30 04:15:16.053271 I | blockpool-rados-namespace-controller: successfully started
2024-05-30 04:15:16.053295 I | ceph-cosi-controller: successfully started
2024-05-30 04:15:16.054702 I | operator: starting the controller-runtime manager
2024-05-30 04:15:16.280221 I | op-k8sutil: ROOK_WATCH_FOR_NODE_FAILURE="true" (configmap)
2024-05-30 04:15:16.561553 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2024-05-30 04:15:16.561572 I | op-k8sutil: ROOK_LOG_LEVEL="DEBUG" (configmap)
2024-05-30 04:15:16.561583 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2024-05-30 04:15:16.564928 I | op-k8sutil: ROOK_CEPH_ALLOW_LOOP_DEVICES="false" (configmap)
2024-05-30 04:15:16.564945 I | operator: rook-ceph-operator-config-controller done reconciling
2024-05-30 04:15:16.578706 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-30 04:15:16.579021 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:15:16.579203 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:15:16.580302 D | ceph-csi: no ceph cluster found not deploying ceph csi driver
2024-05-30 04:15:16.580333 I | ceph-csi: CSI Ceph RBD driver disabled
2024-05-30 04:15:16.580344 I | op-k8sutil: removing daemonset csi-rbdplugin if it exists
2024-05-30 04:15:16.583840 D | op-k8sutil: removing csi-rbdplugin-provisioner deployment if it exists
2024-05-30 04:15:16.583888 I | op-k8sutil: removing deployment csi-rbdplugin-provisioner if it exists
2024-05-30 04:15:16.593869 D | ceph-csi: rook-ceph.rbd.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-30 04:15:16.593899 I | ceph-csi: successfully removed CSI Ceph RBD driver
2024-05-30 04:15:16.593905 I | ceph-csi: CSI CephFS driver disabled
2024-05-30 04:15:16.593915 I | op-k8sutil: removing daemonset csi-cephfsplugin if it exists
2024-05-30 04:15:16.596568 D | op-k8sutil: removing csi-cephfsplugin-provisioner deployment if it exists
2024-05-30 04:15:16.596607 I | op-k8sutil: removing deployment csi-cephfsplugin-provisioner if it exists
2024-05-30 04:15:16.605614 D | ceph-csi: rook-ceph.cephfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-30 04:15:16.605645 I | ceph-csi: successfully removed CSI CephFS driver
2024-05-30 04:15:16.605654 I | ceph-csi: CSI NFS driver disabled
2024-05-30 04:15:16.605663 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-30 04:15:16.608578 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-30 04:15:16.608593 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-30 04:15:16.617350 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-30 04:15:16.617364 I | ceph-csi: successfully removed CSI NFS driver
2024-05-30 04:15:20.081338 D | clusterdisruption-controller: create event from ceph cluster CR
2024-05-30 04:15:20.081413 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:20.081440 D | ceph-cluster-controller: create event from a CR
2024-05-30 04:15:20.081526 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:20.081639 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-30 04:15:20.081678 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "my-cluster"
2024-05-30 04:15:20.081732 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:20.084322 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2024-05-30 04:15:20.084550 I | op-k8sutil: CSI_DISABLE_HOLDER_PODS="true" (configmap)
2024-05-30 04:15:20.086792 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2024-05-30 04:15:20.086832 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:15:20.089130 I | ceph-csi: cluster info for cluster "my-cluster" is not ready yet, will retry in 10s, proceeding with ready clusters
2024-05-30 04:15:20.099419 D | ceph-spec: create event from a CR: "builtin-mgr"
2024-05-30 04:15:20.099539 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:15:20.099731 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:15:20.099807 I | ceph-spec: adding finalizer "cephblockpool.ceph.rook.io" on "builtin-mgr"
2024-05-30 04:15:20.099980 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:15:20.107486 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2024-05-30 04:15:20.107710 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:15:20.109762 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:20.109825 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:15:20.110288 D | ceph-cluster-controller: skipping resource "my-cluster" update with unchanged spec
2024-05-30 04:15:20.111526 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2024-05-30 04:15:20.113607 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-30 04:15:20.113640 D | ceph-spec: update event on CephBlockPool CR
2024-05-30 04:15:20.113772 D | ceph-spec: skipping resource "builtin-mgr" update with unchanged spec
2024-05-30 04:15:20.116248 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2024-05-30 04:15:20.119120 W | ceph-block-pool-controller: failed to set pool "builtin-mgr" status to "Progressing". failed to update object "rook-ceph/builtin-mgr" status: Operation cannot be fulfilled on cephblockpools.ceph.rook.io "builtin-mgr": the object has been modified; please apply your changes to the latest version and try again
2024-05-30 04:15:20.119286 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:15:20.119337 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:15:20.119384 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:15:20.123389 D | ceph-cluster-controller: cluster spec successfully validated
2024-05-30 04:15:20.123468 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2024-05-30 04:15:20.133011 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2024-05-30 04:15:20.133055 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2024-05-30 04:15:20.133073 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2024-05-30 04:15:20.133102 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2024-05-30 04:15:20.133122 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2024-05-30 04:15:20.133164 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2024-05-30 04:15:20.133184 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2024-05-30 04:15:20.133219 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2024-05-30 04:15:20.133251 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2024-05-30 04:15:20.133269 I | op-k8sutil: CSI_ENABLE_LIVENESS="false" (configmap)
2024-05-30 04:15:20.133288 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2024-05-30 04:15:20.133341 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2024-05-30 04:15:20.133357 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2024-05-30 04:15:20.133369 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2024-05-30 04:15:20.133378 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2024-05-30 04:15:20.133393 I | op-k8sutil: CSI_ENABLE_NFS_SNAPSHOTTER="true" (configmap)
2024-05-30 04:15:20.136743 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2024-05-30 04:15:20.136875 I | op-k8sutil: CSI_ENABLE_TOPOLOGY="false" (configmap)
2024-05-30 04:15:20.136934 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2024-05-30 04:15:20.136983 I | op-k8sutil: CSI_ENABLE_METADATA="false" (default)
2024-05-30 04:15:20.137064 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-30 04:15:20.137114 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-05-30 04:15:20.137258 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-30 04:15:20.137340 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-30 04:15:20.137404 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-05-30 04:15:20.137458 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2024-05-30 04:15:20.137506 I | ceph-csi: Kubernetes version is 1.30
2024-05-30 04:15:20.137554 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2024-05-30 04:15:20.137606 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2024-05-30 04:15:20.137673 I | op-k8sutil: CSI_LEADER_ELECTION_LEASE_DURATION="" (default)
2024-05-30 04:15:20.137806 I | op-k8sutil: CSI_LEADER_ELECTION_RENEW_DEADLINE="" (default)
2024-05-30 04:15:20.137979 I | op-k8sutil: CSI_LEADER_ELECTION_RETRY_PERIOD="" (default)
2024-05-30 04:15:20.140510 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v18...
2024-05-30 04:15:20.141720 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:20.141759 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:15:20.145049 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-05-30 04:15:20.145913 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2024-05-30 04:15:20.145953 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.10.2" (default)
2024-05-30 04:15:20.145967 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0" (default)
2024-05-30 04:15:20.146015 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v4.0.0" (default)
2024-05-30 04:15:20.146025 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v4.5.0" (default)
2024-05-30 04:15:20.146040 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v7.0.1" (default)
2024-05-30 04:15:20.146110 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.10.0" (default)
2024-05-30 04:15:20.146130 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2024-05-30 04:15:20.146254 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.8.0" (default)
2024-05-30 04:15:20.146374 I | op-k8sutil: CSI_TOPOLOGY_DOMAIN_LABELS="" (default)
2024-05-30 04:15:20.146438 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2024-05-30 04:15:20.146465 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2024-05-30 04:15:20.146503 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2024-05-30 04:15:20.146599 I | op-k8sutil: CSI_CLUSTER_NAME="" (default)
2024-05-30 04:15:20.146623 I | op-k8sutil: ROOK_CSI_IMAGE_PULL_POLICY="IfNotPresent" (default)
2024-05-30 04:15:20.146632 I | op-k8sutil: CSI_CEPHFS_KERNEL_MOUNT_OPTIONS="" (default)
2024-05-30 04:15:20.146641 I | op-k8sutil: CSI_CEPHFS_ATTACH_REQUIRED="true" (configmap)
2024-05-30 04:15:20.146731 I | op-k8sutil: CSI_RBD_ATTACH_REQUIRED="true" (configmap)
2024-05-30 04:15:20.146817 I | op-k8sutil: CSI_NFS_ATTACH_REQUIRED="true" (configmap)
2024-05-30 04:15:20.146865 I | op-k8sutil: CSI_DRIVER_NAME_PREFIX="rook-ceph" (default)
2024-05-30 04:15:20.150208 I | op-k8sutil: CSI_ENABLE_VOLUME_GROUP_SNAPSHOT="true" (configmap)
2024-05-30 04:15:20.150250 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.10.2"
2024-05-30 04:15:20.150399 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2024-05-30 04:15:20.150430 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-30 04:15:20.153942 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-30 04:15:20.256385 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-30 04:15:20.256525 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:15:20.256612 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:15:20.256657 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:15:20.256713 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:15:20.364001 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-30 04:15:30.124616 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Progressing"
2024-05-30 04:15:30.124663 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:15:30.124668 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:15:30.124675 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:15:30.124906 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-30 04:15:30.124915 D | ceph-spec: update event on CephBlockPool CR
2024-05-30 04:15:30.124979 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:15:30.125002 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:15:30.125054 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:15:30.125086 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:15:30.216008 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-30 04:15:30.257171 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:15:30.257237 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:15:30.257273 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:15:30.257311 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:15:30.341909 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-30 04:15:36.807248 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:15:40.125454 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:15:40.125471 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:15:40.125477 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:15:40.342556 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:15:40.342622 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:15:40.342652 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:15:40.342688 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:15:40.427100 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-30 04:15:49.162033 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-30 04:15:49.169245 I | ceph-csi: Detected ceph CSI image version: "v3.10.2"
2024-05-30 04:15:49.180220 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2024-05-30 04:15:49.180237 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2024-05-30 04:15:49.180240 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2024-05-30 04:15:49.180243 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2024-05-30 04:15:49.180245 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2024-05-30 04:15:49.180250 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME="" (default)
2024-05-30 04:15:49.180252 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME_MOUNT="" (default)
2024-05-30 04:15:49.180948 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:15:49.180963 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:15:49.180968 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:15:49.180975 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:15:49.188693 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2024-05-30 04:15:49.188709 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-30 04:15:49.188712 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2024-05-30 04:15:49.259285 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-30 04:15:49.259305 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2024-05-30 04:15:49.259308 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2024-05-30 04:15:49.259312 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2024-05-30 04:15:49.259315 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME="" (default)
2024-05-30 04:15:49.259317 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME_MOUNT="" (default)
2024-05-30 04:15:49.277134 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2024-05-30 04:15:49.277155 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-30 04:15:49.277160 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2024-05-30 04:15:49.310823 I | ceph-csi: successfully started CSI CephFS driver
2024-05-30 04:15:49.310843 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="File" (configmap)
2024-05-30 04:15:49.318143 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-30 04:15:49.318169 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="File" (configmap)
2024-05-30 04:15:49.324378 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-30 04:15:49.324390 I | ceph-csi: CSI NFS driver disabled
2024-05-30 04:15:49.324396 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-30 04:15:49.327408 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-30 04:15:49.327423 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-30 04:15:49.335872 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-30 04:15:49.335882 I | ceph-csi: successfully removed CSI NFS driver
2024-05-30 04:15:50.126402 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:15:50.126423 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:15:50.126432 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:15:50.427935 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:15:50.428005 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:15:50.428056 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:15:50.428097 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:15:50.512211 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-30 04:15:51.144016 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:15:53.171590 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-05-30 04:15:53.182614 I | ceph-spec: detected ceph image version: "18.2.2-0 reef"
2024-05-30 04:15:53.182631 I | ceph-cluster-controller: validating ceph version from provided image
2024-05-30 04:15:53.186427 D | ceph-cluster-controller: cluster not initialized, nothing to validate. clusterInfo is nil
2024-05-30 04:15:53.186448 I | ceph-cluster-controller: cluster "rook-ceph": version "18.2.2-0 reef" detected for image "quay.io/ceph/ceph:v18"
2024-05-30 04:15:53.194069 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:15:53.194092 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:15:53.194097 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:15:53.194111 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:15:53.201277 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:53.201292 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:15:53.204869 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2024-05-30 04:15:53.212544 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:53.212559 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:15:53.213956 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-05-30 04:15:53.217982 I | ceph-cluster-controller: created placeholder configmap for ceph overrides "rook-config-override"
2024-05-30 04:15:53.217995 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-05-30 04:15:53.218022 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2024-05-30 04:15:53.227053 D | op-mon: Acquiring lock for mon orchestration
2024-05-30 04:15:53.227067 D | op-mon: Acquired lock for mon orchestration
2024-05-30 04:15:53.227071 I | op-mon: start running mons
2024-05-30 04:15:53.227074 D | op-mon: establishing ceph cluster info
2024-05-30 04:15:53.228372 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:15:53.228387 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:15:53.228431 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/mon.keyring --gen-key -n mon. --cap mon 'allow *'
2024-05-30 04:15:53.263955 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mgr 'allow *' --cap mds 'allow'
2024-05-30 04:15:53.296142 I | ceph-spec: creating mon secrets for a new cluster
2024-05-30 04:15:53.374078 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2024-05-30 04:15:53.576194 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data: mapping:{"node":{}} maxMonId:-1 outOfQuorum:]
2024-05-30 04:15:53.774938 D | op-config: creating config secret "rook-ceph-config"
2024-05-30 04:15:53.975791 D | op-config: updating config secret "rook-ceph-config"
2024-05-30 04:15:54.375084 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:15:54.375110 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:15:54.375182 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:15:54.375205 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:15:54.980645 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:15:55.176865 D | op-cfg-keyring: creating secret for rook-ceph-mons-keyring
2024-05-30 04:15:55.575050 D | op-cfg-keyring: creating secret for rook-ceph-admin-keyring
2024-05-30 04:15:56.179511 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:15:56.378298 I | op-mon: targeting the mon count 1
2024-05-30 04:15:56.383126 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP: Port:6789 Zone: NodeName: DataPathMap:0xc0012b9920 UseHostNetwork:false}
2024-05-30 04:15:56.395001 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2024-05-30 04:15:56.418966 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-30 04:15:56.418983 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:56.446300 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-30 04:15:56.446344 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:56.462225 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-30 04:15:56.462255 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:56.778069 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to minikube-m02
2024-05-30 04:15:56.778124 I | op-mon: mon a assigned to node minikube-m02
2024-05-30 04:15:56.778141 D | op-mon: using internal IP 192.168.58.3 for node minikube-m02
2024-05-30 04:15:56.778184 D | op-mon: mons have been scheduled
2024-05-30 04:15:56.785719 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2024-05-30 04:15:56.796733 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-30 04:15:56.796767 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:56.797173 I | op-mon: creating mon a
2024-05-30 04:15:56.797239 D | op-k8sutil: creating service rook-ceph-mon-a
2024-05-30 04:15:56.813291 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-30 04:15:56.813421 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:56.865002 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-30 04:15:56.865049 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:56.989084 D | op-k8sutil: created service rook-ceph-mon-a
2024-05-30 04:15:56.989139 I | op-mon: mon "a" cluster IP is 10.105.94.142
2024-05-30 04:15:57.380156 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-30 04:15:57.580573 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-05-30 04:15:57.580593 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-05-30 04:15:57.580605 D | op-mon: mons were added or removed from the endpoints cm
2024-05-30 04:15:57.580629 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-05-30 04:15:57.580660 D | op-mon: mons were added or removed from the endpoints cm
2024-05-30 04:15:57.580705 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-05-30 04:15:57.580836 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.94.142:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.94.142:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:-1 outOfQuorum:]
2024-05-30 04:15:57.586815 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:15:57.586858 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:15:57.586877 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:15:57.774106 D | op-config: updating config secret "rook-ceph-config"
2024-05-30 04:15:57.978117 D | ceph-spec: object "rook-ceph-config" matched on update
2024-05-30 04:15:57.978133 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2024-05-30 04:15:58.174466 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:15:58.174521 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:15:58.174789 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:15:58.174822 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:15:58.584655 I | op-mon: 0 of 1 expected mons are ready. creating or updating deployments without checking quorum in attempt to achieve a healthy mon cluster
2024-05-30 04:15:58.584803 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.105.94.142 Port:6789 Zone: NodeName: DataPathMap:0xc0012b9920 UseHostNetwork:false}
2024-05-30 04:15:58.592655 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-05-30 04:15:58.592697 D | op-mon: Starting mon: rook-ceph-mon-a
2024-05-30 04:15:58.654676 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-30 04:15:58.654709 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:58.656373 D | ceph-nodedaemon-controller: "rook-ceph-mon-a-85cfd49cbd-hrkdh" is a ceph pod!
2024-05-30 04:15:58.656541 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:15:58.656940 D | ceph-nodedaemon-controller: secret "rook-ceph-crash-collector-keyring" in namespace "rook-ceph" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2024-05-30 04:15:58.677366 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-30 04:15:58.677395 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:58.694031 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-30 04:15:58.694061 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:15:58.774637 I | op-mon: updating maxMonID from -1 to 0
2024-05-30 04:15:58.978824 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-05-30 04:15:58.978871 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-05-30 04:15:59.379091 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-30 04:15:59.576724 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:15:59.777223 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.94.142:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.94.142:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-30 04:15:59.777273 I | op-mon: waiting for mon quorum with [a]
2024-05-30 04:15:59.976824 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:15:59.976915 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00117f530], assignment=&{Schedule:map[a:0xc0016cfc00]}
2024-05-30 04:15:59.976942 D | ceph-csi: cluster "rook-ceph/my-cluster": not deploying the ceph-csi plugin holder
2024-05-30 04:15:59.976953 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:16:00.127672 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:16:00.127699 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:16:00.127709 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:16:00.184155 I | op-mon: mons running: [a]
2024-05-30 04:16:00.184210 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:16:00.513061 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:16:00.513249 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:16:00.513391 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:16:00.513490 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:16:00.577409 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:16:00.978785 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:16:00.978879 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0010acff0], assignment=&{Schedule:map[a:0xc000c0f880]}
2024-05-30 04:16:00.978939 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2024-05-30 04:16:00.978967 I | op-k8sutil: ROOK_OBC_PROVISIONER_NAME_PREFIX="" (default)
2024-05-30 04:16:00.978978 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2024-05-30 04:16:00.979709 I | op-bucket-prov: successfully reconciled bucket provisioner
I0530 04:16:00.979852       1 manager.go:135] "msg"="starting provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket"
2024-05-30 04:16:01.179428 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2024-05-30 04:16:01.378553 I | ceph-csi: Kubernetes version is 1.30
2024-05-30 04:16:01.583747 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.10.2"
2024-05-30 04:16:01.776752 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-30 04:16:07.336448 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:10.128422 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:16:10.128441 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:16:10.128448 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:16:15.374423 D | op-mon: failed to get quorum_status. mon quorum status failed: exit status 1
2024-05-30 04:16:15.694895 D | clusterdisruption-controller: ceph "rook-ceph" cluster failed to check cluster health. failed to get status. . timed out: exit status 1
2024-05-30 04:16:16.046324 D | operator: number of goroutines 464
2024-05-30 04:16:17.170967 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:19.201613 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:20.128845 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:16:20.128895 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:16:20.128920 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:16:20.388721 I | op-mon: mons running: [a]
2024-05-30 04:16:20.388781 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:16:21.219006 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:21.531372 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:25.695630 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:16:25.695739 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:16:25.695815 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:16:25.695897 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:16:28.244775 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-30 04:16:28.244819 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:16:28.321120 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-30 04:16:28.321160 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-30 04:16:28.321186 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-30 04:16:28.321198 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-30 04:16:28.321205 D | ceph-spec: do not reconcile "rook-ceph-mon-a-canary" on monitor canary deployments
2024-05-30 04:16:28.321222 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-30 04:16:28.658048 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:16:28.658549 D | ceph-nodedaemon-controller: secret "rook-ceph-crash-collector-keyring" in namespace "rook-ceph" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2024-05-30 04:16:30.129782 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:16:30.129806 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:16:30.129816 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:16:35.537724 D | op-mon: failed to get quorum_status. mon quorum status failed: exit status 1
2024-05-30 04:16:38.267423 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:40.130475 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:16:40.130493 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:16:40.130500 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:16:40.541427 I | op-mon: mon a is not yet running
2024-05-30 04:16:40.541442 I | op-mon: mons running: []
2024-05-30 04:16:40.541450 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:16:40.816930 D | clusterdisruption-controller: ceph "rook-ceph" cluster failed to check cluster health. failed to get status. . timed out: exit status 1
2024-05-30 04:16:42.826837 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:44.828459 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:45.958572 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-30 04:16:45.971787 I | ceph-csi: Detected ceph CSI image version: "v3.10.2"
2024-05-30 04:16:45.984068 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:16:45.984094 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:16:45.984103 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:16:45.984114 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:16:46.009991 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-30 04:16:46.053109 I | ceph-csi: successfully started CSI CephFS driver
2024-05-30 04:16:46.057181 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-30 04:16:46.060133 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-30 04:16:46.060150 I | ceph-csi: CSI NFS driver disabled
2024-05-30 04:16:46.060156 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-30 04:16:46.061453 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-30 04:16:46.061467 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-30 04:16:46.176837 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-30 04:16:46.176850 I | ceph-csi: successfully removed CSI NFS driver
2024-05-30 04:16:47.715114 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:50.136460 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:16:50.136503 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:16:50.136528 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:16:50.818048 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:16:50.818106 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:16:50.818143 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:16:50.818176 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:16:52.083446 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:16:55.631363 D | op-mon: failed to get quorum_status. mon quorum status failed: exit status 1
2024-05-30 04:16:58.658899 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:16:58.659365 D | ceph-nodedaemon-controller: secret "rook-ceph-crash-collector-keyring" in namespace "rook-ceph" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2024-05-30 04:16:59.538267 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-30 04:16:59.538301 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:00.044911 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-30 04:17:00.046872 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:00.046956 I | clusterdisruption-controller: all PGs are active+clean. Restoring default OSD pdb settings
2024-05-30 04:17:00.046968 I | clusterdisruption-controller: creating the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-30 04:17:00.058821 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:00.058846 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-30 04:17:00.068694 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:17:00.068746 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:17:00.068779 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:17:00.068815 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:00.137520 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:17:00.137570 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:17:00.137592 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:17:00.360780 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-30 04:17:00.362622 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:00.369105 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:00.369169 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-30 04:17:00.641600 I | op-mon: mons running: [a]
2024-05-30 04:17:00.641658 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:00.926170 I | op-mon: Monitors in quorum: [a]
2024-05-30 04:17:00.926211 I | op-mon: mons created: 1
2024-05-30 04:17:00.926243 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:01.282028 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1}}
2024-05-30 04:17:01.282074 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1}}
2024-05-30 04:17:01.282141 I | op-mon: waiting for mon quorum with [a]
2024-05-30 04:17:01.291194 I | op-mon: mons running: [a]
2024-05-30 04:17:01.291249 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:01.617544 I | op-mon: Monitors in quorum: [a]
2024-05-30 04:17:01.618015 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-05-30 04:17:01.618063 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/2679430034 -o /var/lib/rook/2679430034.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:01.928036 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:17:01.928307 I | op-config: applying ceph settings:
[global]
log to file = false
2024-05-30 04:17:01.928336 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/2893155865 -o /var/lib/rook/2893155865.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:02.194633 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:17:02.194756 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-05-30 04:17:02.194792 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:02.462035 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-05-30 04:17:02.462059 I | ceph-spec: not applying network settings for cluster "rook-ceph" ceph networks
2024-05-30 04:17:02.462067 D | op-mon: mon endpoints used are: a=10.105.94.142:6789
2024-05-30 04:17:02.462070 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-05-30 04:17:02.462074 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-05-30 04:17:02.462078 D | op-mon: Released lock for mon orchestration
2024-05-30 04:17:02.462083 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-05-30 04:17:02.462089 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-05-30 04:17:02.462106 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:02.863581 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-05-30 04:17:02.863775 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:03.240553 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-05-30 04:17:03.240631 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:03.634227 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-05-30 04:17:03.634286 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:04.039408 D | op-cfg-keyring: creating secret for rook-csi-rbd-provisioner
2024-05-30 04:17:04.052810 D | op-cfg-keyring: creating secret for rook-csi-rbd-node
2024-05-30 04:17:04.066097 D | op-cfg-keyring: creating secret for rook-csi-cephfs-provisioner
2024-05-30 04:17:04.079419 D | op-cfg-keyring: creating secret for rook-csi-cephfs-node
2024-05-30 04:17:04.084231 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2024-05-30 04:17:04.084270 I | cephclient: getting or creating ceph auth key "client.crash"
2024-05-30 04:17:04.084298 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:04.447560 D | op-cfg-keyring: creating secret for rook-ceph-crash-collector-keyring
2024-05-30 04:17:04.452840 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2024-05-30 04:17:04.452854 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-05-30 04:17:04.452863 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:04.782229 D | op-cfg-keyring: creating secret for rook-ceph-exporter-keyring
2024-05-30 04:17:04.786320 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2024-05-30 04:17:04.786374 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-05-30 04:17:04.786406 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:04.984257 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-05-30 04:17:04.984275 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-05-30 04:17:04.984287 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:05.179707 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-05-30 04:17:05.179732 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-05-30 04:17:05.179750 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:05.427173 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-05-30 04:17:05.427191 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-05-30 04:17:05.427205 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:05.644176 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-05-30 04:17:05.644201 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-05-30 04:17:05.644222 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:05.844204 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-05-30 04:17:05.844599 I | op-config: applying ceph settings:
[global]
mon_data_avail_warn            = 10
mon_warn_on_pool_no_redundancy = false
osd_pool_default_size          = 1
bdev_flock_retry               = 20
bluefs_buffered_io             = false
2024-05-30 04:17:05.844641 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/1704758650 -o /var/lib/rook/1704758650.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:06.069277 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:17:06.069333 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-05-30 04:17:06.069337 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-05-30 04:17:06.069350 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:06.267927 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "my-cluster"
2024-05-30 04:17:06.267994 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-my-cluster" in namespace "rook-ceph"
2024-05-30 04:17:06.268001 D | op-k8sutil: creating secret cluster-peer-token-my-cluster
2024-05-30 04:17:06.270989 D | op-k8sutil: created secret cluster-peer-token-my-cluster
2024-05-30 04:17:06.271039 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-05-30 04:17:06.277231 I | op-mgr: start running mgr
2024-05-30 04:17:06.278068 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:06.278080 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:06.278784 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-05-30 04:17:06.278800 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:06.590986 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-05-30 04:17:06.594503 D | op-cfg-keyring: creating secret for rook-ceph-mgr-a-keyring
2024-05-30 04:17:06.599356 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc000cc8270}
2024-05-30 04:17:06.617285 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-05-30 04:17:06.617313 D | exec: Running command: ceph config set mon auth_allow_insecure_global_id_reclaim false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:06.635710 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-30 04:17:06.635853 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:06.647415 D | ceph-nodedaemon-controller: "rook-ceph-mgr-a-7d654c5c8d-qs9lk" is a ceph pod!
2024-05-30 04:17:06.647564 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:06.648447 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:06.662253 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "created"
2024-05-30 04:17:06.662299 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:06.662674 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-30 04:17:06.662865 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:06.684750 D | op-k8sutil: created service rook-ceph-exporter
2024-05-30 04:17:06.684826 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:06.686512 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:06.686536 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:06.691120 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m03-66485f55c5-2bscp" is a ceph pod!
2024-05-30 04:17:06.691373 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:06.691724 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:06.706773 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:17:06.706800 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:06.706878 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-30 04:17:06.706930 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:06.714466 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:06.714501 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:06.725443 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:06.727897 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:06.727924 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:06.731738 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:06.835639 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-05-30 04:17:06.835655 I | op-config: insecure global ID is now disabled
2024-05-30 04:17:06.839107 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7d654c5c8d" is progressing.}] CollisionCount:<nil>}
2024-05-30 04:17:09.048352 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:17:09.058701 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:09.058735 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:09.863412 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7d654c5c8d" is progressing.}] CollisionCount:<nil>}
2024-05-30 04:17:10.138900 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:17:10.138950 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:17:10.138970 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:17:12.869597 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7d654c5c8d" is progressing.}] CollisionCount:<nil>}
2024-05-30 04:17:15.875664 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7d654c5c8d" is progressing.}] CollisionCount:<nil>}
2024-05-30 04:17:18.884060 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7d654c5c8d" is progressing.}] CollisionCount:<nil>}
2024-05-30 04:17:20.139828 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:17:20.139873 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:17:20.139892 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:17:21.891245 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7d654c5c8d" is progressing.}] CollisionCount:<nil>}
2024-05-30 04:17:22.479696 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:17:24.899685 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-30 04:17:06 +0000 UTC LastTransitionTime:2024-05-30 04:17:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7d654c5c8d" is progressing.}] CollisionCount:<nil>}
2024-05-30 04:17:27.322425 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-30 04:17:27.322465 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:27.908329 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2024-05-30 04:17:27.915536 D | op-mgr: expected number 1 of mgrs found
2024-05-30 04:17:27.917303 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:27.917379 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-05-30 04:17:27.937556 D | op-k8sutil: created service rook-ceph-mgr-dashboard
2024-05-30 04:17:27.937613 D | op-k8sutil: creating service rook-ceph-mgr
2024-05-30 04:17:27.962723 D | op-k8sutil: created service rook-ceph-mgr
2024-05-30 04:17:27.968693 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2024-05-30 04:17:27.968790 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:27.968898 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-05-30 04:17:27.968980 I | op-mgr: successful modules: balancer
2024-05-30 04:17:27.968997 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:27.969134 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:27.991709 I | op-osd: start running osds in namespace "rook-ceph"
2024-05-30 04:17:27.991751 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-05-30 04:17:27.991771 D | op-osd: no OSD migration to a new backend store is requested
2024-05-30 04:17:27.993007 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:27.993038 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:28.002920 D | op-osd: 0 of 0 OSD Deployments need update
2024-05-30 04:17:28.002955 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-05-30 04:17:28.006573 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-05-30 04:17:28.006698 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-05-30 04:17:28.014807 I | op-osd: 2 of the 2 storage nodes are valid
2024-05-30 04:17:28.039306 I | op-osd: started OSD provisioning job for node "minikube-m02"
2024-05-30 04:17:28.066037 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-zth6m" is a ceph pod!
2024-05-30 04:17:28.066308 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:17:28.066901 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:28.074199 I | op-osd: started OSD provisioning job for node "minikube-m03"
2024-05-30 04:17:28.082509 I | op-osd: OSD orchestration status for node minikube-m02 is "starting"
2024-05-30 04:17:28.082744 I | op-osd: OSD orchestration status for node minikube-m03 is "starting"
2024-05-30 04:17:28.083054 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "created"
2024-05-30 04:17:28.083074 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:28.099692 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-qhqp6" is a ceph pod!
2024-05-30 04:17:28.104234 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-30 04:17:28.104246 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:28.117871 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m02-86554684d8-9f989" is a ceph pod!
2024-05-30 04:17:28.127711 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-30 04:17:28.127731 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:28.134175 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:28.137196 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-30 04:17:28.137210 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:28.445269 I | op-mgr: successful modules: prometheus
2024-05-30 04:17:28.461007 I | op-mgr: successful modules: mgr module(s) from the spec
2024-05-30 04:17:28.461102 D | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:28.523956 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:28.524020 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:28.524203 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:28.530777 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:17:28.530800 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:28.729701 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:29.120737 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:29.120856 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:17:29.121123 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:29.125944 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:17:29.125963 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:29.138048 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-30 04:17:29.138061 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:17:29.138569 I | op-osd: OSD orchestration status for node minikube-m02 is "orchestrating"
2024-05-30 04:17:29.334003 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:29.452421 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:29.709545 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-30 04:17:29.709694 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:17:29.710294 I | op-osd: OSD orchestration status for node minikube-m03 is "orchestrating"
2024-05-30 04:17:29.722135 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:30.059060 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:17:30.059116 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:17:30.059144 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:17:30.059177 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:30.076724 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-30 04:17:30.076791 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:30.140322 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:17:30.140353 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:17:30.140367 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:17:30.453549 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-30 04:17:30.454644 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:30.458644 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:30.458693 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-30 04:17:33.448562 I | op-mgr: setting ceph dashboard "admin" login creds
2024-05-30 04:17:33.449219 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/1501246160 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:36.310148 I | op-mgr: successful modules: orchestrator modules
2024-05-30 04:17:36.571809 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/1501246160 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:37.238646 I | op-mgr: successfully set ceph dashboard creds
2024-05-30 04:17:37.238706 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:37.582785 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-30 04:17:37.582887 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:17:37.582899 I | op-osd: OSD orchestration status for node minikube-m03 is "completed"
2024-05-30 04:17:37.582908 I | op-osd: creating OSD 0 on node "minikube-m03"
2024-05-30 04:17:37.583135 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:17:37.583150 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:17:37.583231 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"minikube-m03\""
2024-05-30 04:17:37.603027 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:37.603174 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:37.612086 I | op-osd: creating OSD 1 on node "minikube-m03"
2024-05-30 04:17:37.612810 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:17:37.612841 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:17:37.613095 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m03\""
2024-05-30 04:17:37.623394 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:37.627146 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:37.627300 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:37.627461 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:17:37.627492 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:17:37.627543 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:17:37.627661 D | clusterdisruption-controller: osd "0" POD is not assigned to any node. assuming node drain
2024-05-30 04:17:37.627670 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down and a possible node drain is detected
2024-05-30 04:17:37.627708 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:37.641646 D | ceph-nodedaemon-controller: "rook-ceph-osd-0-bf6ff8499-4wpk6" is a ceph pod!
2024-05-30 04:17:37.641958 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:37.642329 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:37.654689 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-30 04:17:37.654852 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.660174 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:17:37.660210 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:37.660846 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:37.660865 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.680375 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m03-status" config map changes
2024-05-30 04:17:37.680514 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:17:37.680528 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:17:37.680535 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:17:37.680660 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:17:37.684307 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-30 04:17:37.684323 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.684599 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:37.684603 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.685716 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:17:37.685727 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.687697 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-758f5c4867-fqlpk" is a ceph pod!
2024-05-30 04:17:37.700701 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:37.703134 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-30 04:17:37.703151 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.711563 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:37.711647 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:37.711929 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:37.716020 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:37.716154 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.718689 D | ceph-nodedaemon-controller: ceph exporter unchanged on node "minikube-m03"
2024-05-30 04:17:37.718702 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:37.721042 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:17:37.721053 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.731764 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:17:37.731888 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:37.967482 I | op-config: setting "mgr"="mgr/dashboard/ssl"="false" option to the mon configuration database
2024-05-30 04:17:37.967509 D | exec: Running command: ceph config set mgr mgr/dashboard/ssl false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:38.016160 I | clusterdisruption-controller: osd is down in failure domain "minikube-m03". pg health: "cluster has no PGs"
2024-05-30 04:17:38.016234 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:38.273762 I | op-config: successfully set "mgr"="mgr/dashboard/ssl"="false" option to the mon configuration database
2024-05-30 04:17:38.273793 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:38.332342 D | exec: Running command: ceph osd set-group noout minikube-m03 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:38.586272 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:38.627376 E | clusterdisruption-controller: failed to update maintenance noout in cluster "rook-ceph/my-cluster". failed to update flag on crush unit while setting noout.: failed to set flag minikube-m03 on noout: exit status 22
2024-05-30 04:17:38.629927 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:38.629972 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-30 04:17:38.631159 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:38.631197 I | clusterdisruption-controller: deleting the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-30 04:17:38.639513 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:17:38.639573 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:17:38.639644 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:17:38.639828 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-30 04:17:38.639881 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down but no node drain is detected
2024-05-30 04:17:38.639904 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:38.957646 I | op-config: setting "mgr"="mgr/dashboard/PROMETHEUS_API_SSL_VERIFY"="false" option to the mon configuration database
2024-05-30 04:17:38.957719 D | exec: Running command: ceph config set mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:39.009707 I | clusterdisruption-controller: osd is down in failure domain "minikube-m03". pg health: "cluster has no PGs"
2024-05-30 04:17:39.009737 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:39.336233 D | exec: Running command: ceph osd set-group noout minikube-m03 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:39.356547 I | op-config: successfully set "mgr"="mgr/dashboard/PROMETHEUS_API_SSL_VERIFY"="false" option to the mon configuration database
2024-05-30 04:17:39.356642 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:39.668110 E | clusterdisruption-controller: failed to update maintenance noout in cluster "rook-ceph/my-cluster". failed to update flag on crush unit while setting noout.: failed to set flag minikube-m03 on noout: exit status 22
2024-05-30 04:17:39.668932 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:39.669048 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-30 04:17:39.670100 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:39.698557 I | op-config: setting "mgr"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2024-05-30 04:17:39.698594 D | exec: Running command: ceph config set mgr mgr/dashboard/server_port 7000 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:40.004148 I | op-config: successfully set "mgr"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2024-05-30 04:17:40.004318 I | op-config: deleting "mgr.a" "mgr/dashboard/url_prefix" option from the mon configuration database
2024-05-30 04:17:40.004369 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:40.141375 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:17:40.141394 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-30 04:17:40.141402 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:17:40.294876 I | op-config: successfully deleted "mgr/dashboard/url_prefix" option from the mon configuration database
2024-05-30 04:17:40.294900 I | op-config: deleting "mgr.a" "mgr/dashboard/ssl" option from the mon configuration database
2024-05-30 04:17:40.294916 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:40.555536 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-30 04:17:40.555558 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:17:40.556369 I | op-osd: OSD orchestration status for node minikube-m02 is "completed"
2024-05-30 04:17:40.556416 I | op-osd: creating OSD 0 on node "minikube-m02"
2024-05-30 04:17:40.556747 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:17:40.556773 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:17:40.556915 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"minikube-m02\""
2024-05-30 04:17:40.565051 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:40.565066 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:40.574099 E | op-osd: failed to create OSD 0 on node "minikube-m02": failed to create deployment for OSD 0 on node "minikube-m02": deployments.apps "rook-ceph-osd-0" already exists
2024-05-30 04:17:40.574116 I | op-osd: creating OSD 1 on node "minikube-m02"
2024-05-30 04:17:40.574226 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:17:40.574234 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:17:40.574282 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m02\""
2024-05-30 04:17:40.585325 I | op-config: successfully deleted "mgr/dashboard/ssl" option from the mon configuration database
2024-05-30 04:17:40.585348 I | op-config: deleting "mgr.a" "mgr/dashboard/PROMETHEUS_API_HOST" option from the mon configuration database
2024-05-30 04:17:40.585362 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:40.587489 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:40.587503 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:40.591086 E | op-osd: failed to create OSD 1 on node "minikube-m02": failed to create deployment for OSD 1 on node "minikube-m02": deployments.apps "rook-ceph-osd-1" already exists
2024-05-30 04:17:40.591143 I | op-osd: creating OSD 2 on node "minikube-m02"
2024-05-30 04:17:40.591429 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:17:40.591456 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:17:40.591589 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 2 on node \"minikube-m02\""
2024-05-30 04:17:40.600335 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:40.600354 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:40.623531 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:17:40.623548 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:17:40.623568 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m02-status" config map changes
2024-05-30 04:17:40.623581 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:17:40.624361 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "failed to create cluster: failed to start ceph osds: 2 failures encountered while running osds on nodes in namespace \"rook-ceph\". \nfailed to create OSD 0 on node \"minikube-m02\": failed to create deployment for OSD 0 on node \"minikube-m02\": deployments.apps \"rook-ceph-osd-0\" already exists\nfailed to create OSD 1 on node \"minikube-m02\": failed to create deployment for OSD 1 on node \"minikube-m02\": deployments.apps \"rook-ceph-osd-1\" already exists"
2024-05-30 04:17:40.631197 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-30 04:17:40.631352 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:40.631832 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/my-cluster". failed to reconcile cluster "my-cluster": failed to configure local ceph cluster: failed to create cluster: failed to start ceph osds: 2 failures encountered while running osds on nodes in namespace "rook-ceph". 
failed to create OSD 0 on node "minikube-m02": failed to create deployment for OSD 0 on node "minikube-m02": deployments.apps "rook-ceph-osd-0" already exists
failed to create OSD 1 on node "minikube-m02": failed to create deployment for OSD 1 on node "minikube-m02": deployments.apps "rook-ceph-osd-1" already exists
2024-05-30 04:17:40.633242 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:40.633295 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:40.637039 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2024-05-30 04:17:40.638038 D | ceph-nodedaemon-controller: "rook-ceph-osd-2-6ccf45757b-nbbpj" is a ceph pod!
2024-05-30 04:17:40.638516 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:17:40.638898 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:40.639135 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:17:40.640677 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:17:40.640739 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000ce0630], assignment=&{Schedule:map[a:0xc0017ea140]}
2024-05-30 04:17:40.644672 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2024-05-30 04:17:40.644725 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2024-05-30 04:17:40.644750 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2024-05-30 04:17:40.644760 D | ceph-cluster-controller: cluster spec successfully validated
2024-05-30 04:17:40.644807 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2024-05-30 04:17:40.645038 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-30 04:17:40.645052 D | ceph-cluster-controller: checking health of cluster
2024-05-30 04:17:40.645067 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-30 04:17:40.645388 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:17:40.645435 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:40.650697 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-30 04:17:40.650712 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:40.655494 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v18...
2024-05-30 04:17:40.657077 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:40.657170 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:40.657667 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-05-30 04:17:40.660678 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:40.662908 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-30 04:17:40.662920 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:40.666214 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:40.746476 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m03-66485f55c5-2bscp" is a ceph pod!
2024-05-30 04:17:40.746549 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:40.746883 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:40.751408 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:17:40.751432 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:40.776447 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:40.776578 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:40.776651 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:40.778900 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m03-545888649c-zc9mb" is a ceph pod!
2024-05-30 04:17:40.789296 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:40.789309 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:40.795352 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:40.795362 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:40.823296 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:40.823359 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:40.823665 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:40.830416 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:17:40.830435 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:40.940061 I | op-config: successfully deleted "mgr/dashboard/PROMETHEUS_API_HOST" option from the mon configuration database
2024-05-30 04:17:40.940079 I | op-config: deleting "mgr.a" "mgr/dashboard/PROMETHEUS_API_SSL_VERIFY" option from the mon configuration database
2024-05-30 04:17:40.940092 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:41.030911 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:41.044613 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:5 NumOsd:3 NumUpOsd:0 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-30 04:17:41.047344 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:41.254579 I | op-config: successfully deleted "mgr/dashboard/PROMETHEUS_API_SSL_VERIFY" option from the mon configuration database
2024-05-30 04:17:41.254597 I | op-config: deleting "mgr.a" "mgr/dashboard/server_port" option from the mon configuration database
2024-05-30 04:17:41.254611 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:41.422815 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:41.464336 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-30 04:17:41.464352 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-30 04:17:41.464414 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:5 NumOsd:3 NumUpOsd:0 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-30 04:17:41.464423 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-30 04:17:41.472518 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:41.472538 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:41.601589 I | op-config: successfully deleted "mgr/dashboard/server_port" option from the mon configuration database
2024-05-30 04:17:41.601611 I | op-config: deleting "mgr.a" "mgr/dashboard/ssl_server_port" option from the mon configuration database
2024-05-30 04:17:41.601629 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:41.978307 I | op-config: successfully deleted "mgr/dashboard/ssl_server_port" option from the mon configuration database
2024-05-30 04:17:41.978383 I | op-mgr: All per-daemon mgr configuration has been deleted successfully.
2024-05-30 04:17:41.978398 I | op-mgr: dashboard config has changed. restarting the dashboard module
2024-05-30 04:17:41.978411 I | op-mgr: restarting the mgr module: dashboard
2024-05-30 04:17:41.978436 D | exec: Running command: ceph mgr module disable dashboard --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:42.037581 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-05-30 04:17:42.046080 I | ceph-spec: detected ceph image version: "18.2.2-0 reef"
2024-05-30 04:17:42.046325 I | ceph-cluster-controller: validating ceph version from provided image
2024-05-30 04:17:42.047820 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:17:42.056716 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:17:42.056743 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:17:42.056749 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:17:42.056757 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:17:42.223144 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:17:42.223239 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0014c9f20], assignment=&{Schedule:map[a:0xc001b1f280]}
2024-05-30 04:17:42.421735 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:17:42.421756 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:17:42.421870 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:17:42.421887 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:42.789351 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-30 04:17:42.789394 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-30 04:17:42.789488 D | ceph-cluster-controller: both cluster and image spec versions are identical, doing nothing 18.2.2-0 reef
2024-05-30 04:17:42.789518 I | ceph-cluster-controller: cluster "rook-ceph": version "18.2.2-0 reef" detected for image "quay.io/ceph/ceph:v18"
2024-05-30 04:17:42.805936 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2024-05-30 04:17:42.822324 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:42.822343 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:42.825109 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-05-30 04:17:42.825122 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-05-30 04:17:42.825145 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2024-05-30 04:17:42.833904 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:42.833914 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:42.835347 D | op-mon: Acquiring lock for mon orchestration
2024-05-30 04:17:42.835359 D | op-mon: Acquired lock for mon orchestration
2024-05-30 04:17:42.835362 I | op-mon: start running mons
2024-05-30 04:17:42.835364 D | op-mon: establishing ceph cluster info
2024-05-30 04:17:42.837181 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:17:42.860925 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-30 04:17:42.860938 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:43.022329 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:17:43.022373 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000d24a20], assignment=&{Schedule:map[a:0xc000aec180]}
2024-05-30 04:17:43.340529 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:43.824264 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-30 04:17:44.023110 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.94.142:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.94.142:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-30 04:17:44.223217 D | op-config: updating config secret "rook-ceph-config"
2024-05-30 04:17:44.442146 I | op-mgr: successful modules: dashboard
2024-05-30 04:17:44.622348 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:17:44.622388 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:17:44.622603 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:17:44.622625 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:17:45.227101 D | op-cfg-keyring: updating secret for rook-ceph-mons-keyring
2024-05-30 04:17:45.622815 D | op-cfg-keyring: updating secret for rook-ceph-admin-keyring
2024-05-30 04:17:46.224976 I | op-mon: targeting the mon count 1
2024-05-30 04:17:46.231064 D | op-mon: Host network for mon "a" is false
2024-05-30 04:17:46.231118 D | op-mon: mon a already scheduled
2024-05-30 04:17:46.231126 D | op-mon: mons have been scheduled
2024-05-30 04:17:46.237075 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-05-30 04:17:46.237132 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/1657614432 -o /var/lib/rook/1657614432.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:46.621497 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:17:46.621947 I | op-config: applying ceph settings:
[global]
log to file = false
2024-05-30 04:17:46.621986 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3762874930 -o /var/lib/rook/3762874930.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:46.945382 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:17:46.945486 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-05-30 04:17:46.945531 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:47.302641 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-05-30 04:17:47.302657 I | op-mon: checking for basic quorum with existing mons
2024-05-30 04:17:47.305916 D | op-k8sutil: creating service rook-ceph-mon-a
2024-05-30 04:17:47.318100 D | op-k8sutil: updating service rook-ceph-mon-a
2024-05-30 04:17:47.322289 I | op-mon: mon "a" cluster IP is 10.105.94.142
2024-05-30 04:17:47.427256 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-30 04:17:47.622518 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.94.142:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.94.142:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-30 04:17:47.821670 D | op-config: updating config secret "rook-ceph-config"
2024-05-30 04:17:48.223908 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:17:48.223957 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:17:48.224290 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:17:48.224322 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:17:48.632871 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.105.94.142 Port:6789 Zone: NodeName:minikube-m02 DataPathMap:0xc0017818c0 UseHostNetwork:false}
2024-05-30 04:17:48.641057 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-05-30 04:17:48.641073 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2024-05-30 04:17:48.650502 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2024-05-30 04:17:48.650521 I | op-mon: waiting for mon quorum with [a]
2024-05-30 04:17:48.828845 I | op-mon: mons running: [a]
2024-05-30 04:17:48.828872 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:49.286284 I | op-mon: Monitors in quorum: [a]
2024-05-30 04:17:49.286306 I | op-mon: mons created: 1
2024-05-30 04:17:49.286330 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:49.677496 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-30 04:17:49.677514 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-30 04:17:49.677545 I | op-mon: waiting for mon quorum with [a]
2024-05-30 04:17:49.685837 I | op-mon: mons running: [a]
2024-05-30 04:17:49.685857 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:50.139124 I | op-mon: Monitors in quorum: [a]
2024-05-30 04:17:50.139386 I | ceph-spec: not applying network settings for cluster "rook-ceph" ceph networks
2024-05-30 04:17:50.139417 D | op-mon: mon endpoints used are: a=10.105.94.142:6789
2024-05-30 04:17:50.139425 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-05-30 04:17:50.139433 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-05-30 04:17:50.139441 D | op-mon: Released lock for mon orchestration
2024-05-30 04:17:50.139449 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-05-30 04:17:50.139498 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-05-30 04:17:50.139537 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:50.142318 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:17:50.142354 D | ceph-spec: "ceph-block-pool-controller": ceph status is "HEALTH_OK", operator is ready to run ceph command, reconciling
2024-05-30 04:17:50.146841 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:17:50.150210 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:17:50.150619 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000d83170], assignment=&{Schedule:map[a:0xc0011237c0]}
2024-05-30 04:17:50.150799 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:50.151108 I | ceph-block-pool-controller: creating pool ".mgr" in namespace "rook-ceph"
2024-05-30 04:17:50.151184 D | exec: Running command: ceph osd crush rule create-replicated .mgr default host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:50.626151 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-05-30 04:17:50.626175 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:51.030825 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-05-30 04:17:51.030863 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:51.395699 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-05-30 04:17:51.395763 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:51.517562 D | exec: Running command: ceph osd pool get .mgr all --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:51.876407 D | exec: Running command: ceph osd pool create .mgr 0 replicated .mgr --size 1 --yes-i-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:51.936404 D | op-cfg-keyring: updating secret for rook-csi-rbd-provisioner
2024-05-30 04:17:51.944820 D | op-cfg-keyring: updating secret for rook-csi-rbd-node
2024-05-30 04:17:51.951239 D | op-cfg-keyring: updating secret for rook-csi-cephfs-provisioner
2024-05-30 04:17:51.962733 D | op-cfg-keyring: updating secret for rook-csi-cephfs-node
2024-05-30 04:17:51.966812 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2024-05-30 04:17:51.966827 I | cephclient: getting or creating ceph auth key "client.crash"
2024-05-30 04:17:51.966837 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:52.436051 D | op-cfg-keyring: updating secret for rook-ceph-crash-collector-keyring
2024-05-30 04:17:52.439102 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2024-05-30 04:17:52.439130 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-05-30 04:17:52.439147 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:52.935363 D | exec: Running command: ceph osd pool application get .mgr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:52.942970 D | op-cfg-keyring: updating secret for rook-ceph-exporter-keyring
2024-05-30 04:17:52.947707 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2024-05-30 04:17:52.947751 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-05-30 04:17:52.947778 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:53.401666 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-05-30 04:17:53.401730 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-05-30 04:17:53.401774 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:53.403210 D | exec: Running command: ceph osd pool application enable .mgr mgr --yes-i-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:53.757431 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-05-30 04:17:53.757449 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-05-30 04:17:53.757469 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:53.948920 I | cephclient: reconciling replicated pool .mgr succeeded
2024-05-30 04:17:53.949021 D | cephclient: skipping check for failure domain and deviceClass on pool ".mgr" as it is not specified
2024-05-30 04:17:53.949046 I | ceph-block-pool-controller: initializing pool ".mgr" for RBD use
2024-05-30 04:17:53.949106 D | exec: Running command: rbd pool init .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-30 04:17:53.996985 I | ceph-block-pool-controller: successfully initialized pool ".mgr" for RBD use
2024-05-30 04:17:53.997011 D | ceph-block-pool-controller: configuring RBD per-image IO statistics collection
2024-05-30 04:17:53.997124 D | exec: Running command: ceph config get mgr mgr/prometheus/rbd_stats_pools --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:54.079706 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-05-30 04:17:54.079734 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-05-30 04:17:54.079749 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:54.304347 D | ceph-block-pool-controller: RBD per-image IO statistics will be collected for pools: []
2024-05-30 04:17:54.304377 I | op-config: setting "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-30 04:17:54.304395 D | exec: Running command: ceph config set mgr mgr/prometheus/rbd_stats_pools  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:54.414928 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-05-30 04:17:54.414975 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-05-30 04:17:54.415006 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:54.671120 I | op-config: successfully set "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-30 04:17:54.671232 D | ceph-block-pool-controller: configured RBD per-image IO statistics collection
2024-05-30 04:17:54.671253 D | ceph-block-pool-controller: reconciling create rbd mirror peer configuration
2024-05-30 04:17:54.679344 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Ready"
2024-05-30 04:17:54.679370 D | ceph-block-pool-controller: done reconciling
2024-05-30 04:17:54.679408 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:17:54.679715 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-30 04:17:54.679744 D | ceph-spec: update event on CephBlockPool CR
2024-05-30 04:17:54.679877 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:17:54.679988 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:17:54.680076 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:17:54.680290 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-30 04:17:54.680375 I | clusterdisruption-controller: osd "rook-ceph-osd-2" is down but no node drain is detected
2024-05-30 04:17:54.680448 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down but no node drain is detected
2024-05-30 04:17:54.680520 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:54.883404 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-05-30 04:17:54.883644 I | op-config: applying ceph settings:
[global]
bluefs_buffered_io             = false
mon_data_avail_warn            = 10
mon_warn_on_pool_no_redundancy = false
osd_pool_default_size          = 1
bdev_flock_retry               = 20
2024-05-30 04:17:54.883663 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3516008594 -o /var/lib/rook/3516008594.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:55.135442 I | clusterdisruption-controller: osd is down in failure domain "minikube-m02". pg health: "cluster has no PGs"
2024-05-30 04:17:55.135628 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:55.193817 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:17:55.193951 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-05-30 04:17:55.193974 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-05-30 04:17:55.194021 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:55.443341 D | exec: Running command: ceph osd set-group noout minikube-m02 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:55.569343 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "my-cluster"
2024-05-30 04:17:55.569439 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-my-cluster" in namespace "rook-ceph"
2024-05-30 04:17:55.569454 D | op-k8sutil: creating secret cluster-peer-token-my-cluster
2024-05-30 04:17:55.576450 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-05-30 04:17:55.585404 I | op-mgr: start running mgr
2024-05-30 04:17:55.587047 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:55.587079 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:55.589217 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-05-30 04:17:55.589236 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:55.991247 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:55.992966 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:55.993040 I | clusterdisruption-controller: creating temporary blocking pdb "rook-ceph-osd-host-minikube-m03" with maxUnavailable=0 for "host" failure domain "minikube-m03"
2024-05-30 04:17:55.998124 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-30 04:17:55.999750 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:56.022429 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-05-30 04:17:56.024672 D | op-cfg-keyring: updating secret for rook-ceph-mgr-a-keyring
2024-05-30 04:17:56.026943 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc001b05140}
2024-05-30 04:17:56.035116 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2024-05-30 04:17:56.040198 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2024-05-30 04:17:56.045611 D | op-mgr: expected number 1 of mgrs found
2024-05-30 04:17:56.046180 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:17:56.046208 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-05-30 04:17:56.060759 D | op-k8sutil: updating service rook-ceph-mgr-dashboard
2024-05-30 04:17:56.065007 D | op-k8sutil: creating service rook-ceph-mgr
2024-05-30 04:17:56.079897 D | op-k8sutil: updating service rook-ceph-mgr
2024-05-30 04:17:56.088188 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2024-05-30 04:17:56.088260 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:56.088322 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-05-30 04:17:56.088343 I | op-mgr: successful modules: balancer
2024-05-30 04:17:56.088346 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:56.088407 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:56.099544 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:17:56.099958 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:17:56.101041 I | op-osd: start running osds in namespace "rook-ceph"
2024-05-30 04:17:56.101069 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-05-30 04:17:56.101072 D | op-osd: no OSD migration to a new backend store is requested
2024-05-30 04:17:56.115889 D | op-osd: 3 of 3 OSD Deployments need update
2024-05-30 04:17:56.115901 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-05-30 04:17:56.223470 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-05-30 04:17:56.223665 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-05-30 04:17:56.423902 I | op-osd: 2 of the 2 storage nodes are valid
2024-05-30 04:17:56.829828 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m02 to start a new one
2024-05-30 04:17:56.837804 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 still exists
2024-05-30 04:17:56.845289 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-zth6m" is a ceph pod!
2024-05-30 04:17:56.845343 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:17:56.845607 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:56.856330 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:17:56.856346 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:56.996121 I | op-mgr: successful modules: mgr module(s) from the spec
2024-05-30 04:17:56.996225 D | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:57.001375 I | op-mgr: successful modules: prometheus
2024-05-30 04:17:57.038256 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:57.427479 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:57.542589 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:17:58.563294 I | op-mgr: successful modules: orchestrator modules
2024-05-30 04:17:59.226915 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-30 04:17:59.226966 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:59.229986 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:17:59.230054 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:17:59.843065 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 deleted
2024-05-30 04:17:59.858324 I | op-osd: started OSD provisioning job for node "minikube-m02"
2024-05-30 04:17:59.874917 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m03 to start a new one
2024-05-30 04:17:59.878386 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-296kp" is a ceph pod!
2024-05-30 04:17:59.878609 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:17:59.879199 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:59.896873 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 still exists
2024-05-30 04:17:59.897441 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:17:59.897488 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:59.915952 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-qhqp6" is a ceph pod!
2024-05-30 04:17:59.927376 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:59.936810 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:17:59.936928 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:17:59.937475 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:17:59.950441 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:17:59.950478 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:17:59.973988 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:17:59.981020 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:18:00.458943 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:18:00.459041 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:18:00.459094 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:18:00.459428 I | clusterdisruption-controller: osd "rook-ceph-osd-2" is down but no node drain is detected
2024-05-30 04:18:00.459473 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:00.953198 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-30 04:18:00.953239 I | clusterdisruption-controller: osd is down in failure domain "minikube-m02". pg health: "all PGs in cluster are clean"
2024-05-30 04:18:00.953268 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:01.097935 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-30 04:18:01.097976 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:18:01.219214 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-30 04:18:01.219346 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:18:01.352545 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:18:01.353323 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:18:01.353354 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-30 04:18:01.353884 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:18:01.981797 I | op-mgr: the dashboard secret was already generated
2024-05-30 04:18:01.981823 I | op-mgr: setting ceph dashboard "admin" login creds
2024-05-30 04:18:01.981998 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2727931489 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:02.294127 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/2727931489 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:02.900461 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 deleted
2024-05-30 04:18:02.911277 I | op-osd: started OSD provisioning job for node "minikube-m03"
2024-05-30 04:18:02.914865 I | op-osd: OSD orchestration status for node minikube-m02 is "orchestrating"
2024-05-30 04:18:02.914986 I | op-osd: OSD orchestration status for node minikube-m03 is "starting"
2024-05-30 04:18:02.917055 I | op-mgr: successfully set ceph dashboard creds
2024-05-30 04:18:02.917122 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:02.926033 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-4gb6h" is a ceph pod!
2024-05-30 04:18:02.926169 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:18:02.926867 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:18:02.935743 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:18:02.935793 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:18:02.953120 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:18:02.962680 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:18:03.017618 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:03.308667 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:03.388728 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:03.670437 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:03.697647 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:03.855462 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-30 04:18:03.855479 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:18:03.958629 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:03.985118 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:04.304837 D | op-osd: updating OSDs: [0 1]
2024-05-30 04:18:04.312685 I | op-osd: updating OSD 0 on node "minikube-m03"
2024-05-30 04:18:04.313009 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:18:04.313135 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:18:04.313367 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"minikube-m03\""
2024-05-30 04:18:04.333034 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:18:04.333052 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:18:04.339287 I | op-osd: updating OSD 1 on node "minikube-m03"
2024-05-30 04:18:04.339694 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:18:04.339721 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:18:04.339855 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m03\""
2024-05-30 04:18:04.346473 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:04.349437 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:18:04.349476 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:18:04.362407 D | op-k8sutil: deployment "rook-ceph-osd-0" did not change. nothing to update
2024-05-30 04:18:04.367935 D | op-k8sutil: deployment "rook-ceph-osd-1" did not change. nothing to update
2024-05-30 04:18:04.379507 I | op-osd: OSD orchestration status for node minikube-m02 is "completed"
2024-05-30 04:18:04.379550 D | op-osd: not creating deployment for OSD 0 which already exists
2024-05-30 04:18:04.379558 D | op-osd: not creating deployment for OSD 1 which already exists
2024-05-30 04:18:04.379564 D | op-osd: not creating deployment for OSD 2 which already exists
2024-05-30 04:18:04.383572 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:18:04.383766 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:18:04.383787 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:18:04.383832 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m02-status" config map changes
2024-05-30 04:18:04.384167 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:18:04.484921 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:04.713540 I | op-mgr: successful modules: dashboard
2024-05-30 04:18:04.747207 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-30 04:18:04.747225 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:18:04.870902 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:05.273735 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:05.738424 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:06.141611 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:06.141657 I | op-osd: OSD orchestration status for node minikube-m03 is "orchestrating"
2024-05-30 04:18:06.242427 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:06.653276 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:07.094229 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:07.425314 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:07.824844 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-30 04:18:07.824864 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:18:07.888101 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:07.888348 I | op-osd: OSD orchestration status for node minikube-m03 is "completed"
2024-05-30 04:18:07.888400 D | op-osd: not creating deployment for OSD 2 which already exists
2024-05-30 04:18:07.888410 D | op-osd: not creating deployment for OSD 0 which already exists
2024-05-30 04:18:07.888427 D | op-osd: not creating deployment for OSD 1 which already exists
2024-05-30 04:18:07.893045 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:18:07.893065 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:18:07.893081 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:18:07.893105 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m03-status" config map changes
2024-05-30 04:18:07.894079 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:18:07.994578 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:08.467730 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:08.920152 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:09.283426 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:09.746944 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:09.847528 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:10.305407 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:10.791252 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:11.260994 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:11.697723 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:11.797931 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:12.266324 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:12.768475 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:13.251470 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:13.727722 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:13.827899 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:14.325007 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:14.739144 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:15.175983 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:15.648191 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:15.749268 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:16.241490 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:16.726015 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:17.204758 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:17.660995 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:17.761331 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:18.209561 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:18.700651 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:19.172282 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:19.625301 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:19.725522 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:20.171822 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:20.646128 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:21.076898 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:21.405222 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:21.505797 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:22.007639 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:22.353429 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:22.826842 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:23.218996 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:23.319392 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:23.792250 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:24.252024 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:24.700503 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:25.151426 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:25.252234 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:25.645566 D | op-mon: checking health of mons
2024-05-30 04:18:25.645609 D | op-mon: Acquiring lock for mon orchestration
2024-05-30 04:18:25.645655 D | op-mon: Acquired lock for mon orchestration
2024-05-30 04:18:25.653127 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-30 04:18:25.653177 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:25.697682 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:26.105054 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.105.94.142:6789/0 PublicAddr:10.105.94.142:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.105.94.142:3300 Nonce:0} {Type:v1 Addr:10.105.94.142:6789 Nonce:0}]}}]}}
2024-05-30 04:18:26.105074 D | op-mon: targeting the mon count 1
2024-05-30 04:18:26.105079 D | op-mon: mon "a" found in quorum
2024-05-30 04:18:26.105081 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-30 04:18:26.110542 D | op-mon: skipping check for multiple mons on same node since multiple mons are allowed
2024-05-30 04:18:26.110574 D | op-mon: Released lock for mon orchestration
2024-05-30 04:18:26.110587 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-30 04:18:26.200683 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:26.675661 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:27.154742 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:27.255221 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:27.754825 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:28.197599 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:28.670504 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:29.145989 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:29.246090 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:29.727324 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:30.195847 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:30.645250 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:31.151143 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:31.251494 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:31.357255 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:18:31.357317 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:18:31.357362 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:18:31.357494 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:31.736625 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:31.913250 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-30 04:18:31.913277 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-30 04:18:31.913293 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:32.117036 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:32.265250 D | exec: Running command: ceph osd unset-group noout minikube-m02 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:32.559195 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:32.934332 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:33.034461 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:33.497070 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:33.681193 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:18:33.681244 I | clusterdisruption-controller: all PGs are active+clean. Restoring default OSD pdb settings
2024-05-30 04:18:33.681250 I | clusterdisruption-controller: creating the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-30 04:18:33.690308 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:18:33.690345 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-30 04:18:33.691492 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:18:33.691577 I | clusterdisruption-controller: deleting temporary blocking pdb with "rook-ceph-osd-host-minikube-m03" with maxUnavailable=0 for "host" failure domain "minikube-m03"
2024-05-30 04:18:33.696565 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-30 04:18:33.701832 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:18:33.977258 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:34.434793 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:34.803906 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:34.904051 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:35.386368 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:35.810529 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:36.262771 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:36.730892 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:36.831243 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:37.267938 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:37.772962 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:38.205646 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:38.662698 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:38.763065 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:39.213816 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:39.755757 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:40.158915 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:40.624879 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:40.646018 D | op-osd: checking osd processes status.
2024-05-30 04:18:40.646127 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:40.725784 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:41.087409 D | op-osd: validating status of osd.0
2024-05-30 04:18:41.087450 D | op-osd: osd.0 is healthy.
2024-05-30 04:18:41.087459 D | op-osd: validating status of osd.1
2024-05-30 04:18:41.087466 D | op-osd: osd.1 is healthy.
2024-05-30 04:18:41.087472 D | op-osd: validating status of osd.2
2024-05-30 04:18:41.087478 D | op-osd: osd.2 is healthy.
2024-05-30 04:18:41.191709 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:41.473763 D | ceph-cluster-controller: checking health of cluster
2024-05-30 04:18:41.473881 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-30 04:18:41.602882 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:41.918506 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:16 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88002560 AvailableBytes:11522179944448 TotalBytes:11522267947008 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-30 04:18:41.926441 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:41.960293 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:42.435831 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:18:42.435850 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:18:42.435916 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:16 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88002560 AvailableBytes:11522179944448 TotalBytes:11522267947008 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-30 04:18:42.435925 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-30 04:18:42.440931 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:42.445703 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:18:42.445741 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:18:42.541623 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:42.976822 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:43.434114 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:43.885471 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:44.329747 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:44.429873 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:44.905071 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:45.370164 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:45.830577 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:46.303026 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:46.403234 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:46.879440 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:47.357999 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:47.831334 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:48.304500 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:48.404803 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:48.889929 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:49.338121 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:49.846085 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:50.228041 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:50.328397 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:50.788524 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:51.253304 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:51.765525 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:52.234309 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:52.334838 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:52.803424 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:53.318022 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:53.747288 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:54.224197 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:54.324584 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:54.726536 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:55.134687 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:55.618938 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:56.087880 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:56.188284 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:56.700683 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:57.198905 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:57.617013 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:58.020490 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:18:58.121156 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:58.575314 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:59.038470 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:59.416184 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:18:59.920689 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:00.021395 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:00.480411 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:00.982659 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:01.417061 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:01.837569 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:01.937989 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:02.412284 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:02.861160 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:03.266787 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:03.743854 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:03.743879 I | op-osd: waiting... 2 of 2 OSD prepare jobs have finished processing and 2 of 3 OSDs have been updated
2024-05-30 04:19:03.844261 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:04.352593 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:04.841060 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:05.305551 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:05.780088 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:05.880413 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:06.345405 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:06.831070 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:07.341582 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:07.788341 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:07.888627 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:08.287374 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:08.770005 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:09.256116 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:09.726554 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:09.826977 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:10.300241 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:10.776657 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:11.111034 D | op-mon: checking health of mons
2024-05-30 04:19:11.111080 D | op-mon: Acquiring lock for mon orchestration
2024-05-30 04:19:11.111090 D | op-mon: Acquired lock for mon orchestration
2024-05-30 04:19:11.122109 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-30 04:19:11.122164 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:11.320536 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:11.655153 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.105.94.142:6789/0 PublicAddr:10.105.94.142:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.105.94.142:3300 Nonce:0} {Type:v1 Addr:10.105.94.142:6789 Nonce:0}]}}]}}
2024-05-30 04:19:11.655195 D | op-mon: targeting the mon count 1
2024-05-30 04:19:11.655211 D | op-mon: mon "a" found in quorum
2024-05-30 04:19:11.655220 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-30 04:19:11.661475 D | op-mon: Released lock for mon orchestration
2024-05-30 04:19:11.661491 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-30 04:19:11.748711 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:11.849063 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:12.320021 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:12.713187 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:13.207112 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:13.617058 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:13.717233 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:14.135237 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:14.557990 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:14.949933 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:15.426482 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:15.526885 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:15.975443 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:16.460634 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:16.908983 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:17.359402 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:17.459767 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:17.891458 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:18.352239 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:18.824914 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:19.218911 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:19.319100 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:19.759325 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:20.217535 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:20.698511 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:21.182200 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:21.282628 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:21.750113 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:22.260189 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:22.723700 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:23.274323 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:23.374534 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:23.837361 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:24.315980 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:24.776704 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:25.247445 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:25.347741 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:25.790385 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:26.201613 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:26.695106 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:27.064204 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:27.164579 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:27.643526 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:28.011773 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:28.401806 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:28.859002 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:28.959448 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:29.357778 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:29.829580 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:30.298923 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:30.778592 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:30.879076 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:31.368820 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:31.834079 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:32.173238 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:32.674372 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:32.774779 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:33.199998 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:33.633229 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:34.179376 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:34.671189 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:34.771694 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:35.227588 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:35.720847 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:36.187724 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:36.617812 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:36.718001 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:37.195955 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:37.702748 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:38.172122 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:38.649205 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:38.678944 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:19:38.678973 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:19:38.750060 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:39.103537 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:39.448174 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:39.918988 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:40.337492 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:40.437874 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:40.917211 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:41.088133 D | op-osd: checking osd processes status.
2024-05-30 04:19:41.088390 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:41.371092 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:41.528630 D | op-osd: validating status of osd.0
2024-05-30 04:19:41.528646 D | op-osd: osd.0 is healthy.
2024-05-30 04:19:41.528648 D | op-osd: validating status of osd.1
2024-05-30 04:19:41.528650 D | op-osd: osd.1 is healthy.
2024-05-30 04:19:41.528651 D | op-osd: validating status of osd.2
2024-05-30 04:19:41.528653 D | op-osd: osd.2 is healthy.
2024-05-30 04:19:41.842938 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:42.315300 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:42.415723 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:42.445207 D | ceph-cluster-controller: checking health of cluster
2024-05-30 04:19:42.445261 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-30 04:19:42.826541 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:42.893805 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:24 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:32}] Version:0 NumPgs:32 DataBytes:166041 UsedBytes:92393472 AvailableBytes:11522175553536 TotalBytes:11522267947008 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-30 04:19:42.903033 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:43.152938 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:43.408996 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:19:43.409039 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:19:43.409184 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:24 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:32}] Version:0 NumPgs:32 DataBytes:166041 UsedBytes:92393472 AvailableBytes:11522175553536 TotalBytes:11522267947008 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-30 04:19:43.409218 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-30 04:19:43.431465 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:43.431505 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:43.585777 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:44.053887 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:44.154552 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:44.645986 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:45.144752 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:45.667896 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:46.055050 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:46.155696 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:46.603900 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:47.088152 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:47.573213 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:47.957392 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:48.058208 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:48.524683 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:48.901120 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:49.311811 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:49.534538 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-30 04:19:49.534572 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.534623 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:49.534707 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:19:49.534772 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:19:49.534935 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:49.535960 I | ceph-cluster-controller: CR has changed for "my-cluster". diff=v1.ClusterSpec{
	CephVersion: {Image: "quay.io/ceph/ceph:v18", AllowUnsupported: true},
	Storage: v1.StorageScopeSpec{
		... // 4 identical fields
		Selection:              {UseAllDevices: &false},
		StorageClassDeviceSets: nil,
		Store: v1.OSDStore{
-			Type:        "",
+			Type:        "bluestore",
-			UpdateStore: "",
+			UpdateStore: "yes-really-update-store",
		},
		FlappingRestartIntervalHours: 0,
	},
	Annotations: nil,
	Labels:      nil,
	... // 22 identical fields
}
2024-05-30 04:19:49.536093 I | operator: reloading operator's CRDs manager, cancelling all orchestrations!
I0530 04:19:49.536333       1 manager.go:148] "msg"="stopping provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket" "reason"="context canceled"
2024-05-30 04:19:49.536362 I | op-mon: stopping monitoring of mons in namespace "rook-ceph"
2024-05-30 04:19:49.536462 I | op-osd: stopping monitoring of OSDs in namespace "rook-ceph"
2024-05-30 04:19:49.536497 I | ceph-cluster-controller: stopping monitoring of ceph status
2024-05-30 04:19:49.539119 I | operator: watching all namespaces for Ceph CRs
2024-05-30 04:19:49.539148 I | operator: setting up schemes
2024-05-30 04:19:49.540942 I | operator: setting up the controller-runtime manager
2024-05-30 04:19:49.541237 I | ceph-cluster-controller: successfully started
2024-05-30 04:19:49.544102 I | ceph-cluster-controller: enabling hotplug orchestration
2024-05-30 04:19:49.544175 I | ceph-nodedaemon-controller: successfully started
2024-05-30 04:19:49.544187 D | ceph-nodedaemon-controller: watch for changes to the nodes
2024-05-30 04:19:49.544198 D | ceph-nodedaemon-controller: watch for changes to the ceph-crash deployments
2024-05-30 04:19:49.544210 D | ceph-nodedaemon-controller: watch for changes to the ceph pods and enqueue their nodes
2024-05-30 04:19:49.544231 I | ceph-block-pool-controller: successfully started
2024-05-30 04:19:49.544266 I | ceph-object-store-user-controller: successfully started
2024-05-30 04:19:49.544305 I | ceph-object-realm-controller: successfully started
2024-05-30 04:19:49.544324 I | ceph-object-zonegroup-controller: successfully started
2024-05-30 04:19:49.544341 I | ceph-object-zone-controller: successfully started
2024-05-30 04:19:49.544533 I | ceph-object-controller: successfully started
2024-05-30 04:19:49.544588 I | ceph-file-controller: successfully started
2024-05-30 04:19:49.544623 I | ceph-nfs-controller: successfully started
2024-05-30 04:19:49.544666 I | ceph-rbd-mirror-controller: successfully started
2024-05-30 04:19:49.544694 I | ceph-client-controller: successfully started
2024-05-30 04:19:49.544728 I | ceph-filesystem-mirror-controller: successfully started
2024-05-30 04:19:49.544755 I | operator: rook-ceph-operator-config-controller successfully started
2024-05-30 04:19:49.544786 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2024-05-30 04:19:49.544963 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2024-05-30 04:19:49.544998 I | ceph-bucket-topic: successfully started
2024-05-30 04:19:49.545016 I | ceph-bucket-notification: successfully started
2024-05-30 04:19:49.545046 I | ceph-bucket-notification: successfully started
2024-05-30 04:19:49.545063 I | ceph-fs-subvolumegroup-controller: successfully started
2024-05-30 04:19:49.545090 I | blockpool-rados-namespace-controller: successfully started
2024-05-30 04:19:49.545106 I | ceph-cosi-controller: successfully started
2024-05-30 04:19:49.546174 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:49.546197 I | operator: starting the controller-runtime manager
2024-05-30 04:19:49.555739 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.555780 D | ceph-cluster-controller: create event from a CR
2024-05-30 04:19:49.556375 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.556619 D | clusterdisruption-controller: create event from ceph cluster CR
2024-05-30 04:19:49.592992 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-30 04:19:49.593011 D | ceph-spec: update event on CephBlockPool CR
2024-05-30 04:19:49.593069 D | ceph-spec: skipping resource "builtin-mgr" update with unchanged spec
2024-05-30 04:19:49.650878 D | operator: reconciling rook-ceph/rook-ceph-operator-config
2024-05-30 04:19:49.652679 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.653333 I | operator: rook-ceph-operator-config-controller done reconciling
2024-05-30 04:19:49.655189 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:19:49.655316 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-4gb6h" is a ceph pod!
2024-05-30 04:19:49.655952 D | ceph-spec: create event from a CR: "builtin-mgr"
2024-05-30 04:19:49.656043 D | ceph-nodedaemon-controller: "rook-ceph-osd-2-6ccf45757b-nbbpj" is a ceph pod!
2024-05-30 04:19:49.656108 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m02-86554684d8-9f989" is a ceph pod!
2024-05-30 04:19:49.656119 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-758f5c4867-fqlpk" is a ceph pod!
2024-05-30 04:19:49.656128 D | ceph-nodedaemon-controller: "rook-ceph-osd-0-bf6ff8499-4wpk6" is a ceph pod!
2024-05-30 04:19:49.656143 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m03-545888649c-zc9mb" is a ceph pod!
2024-05-30 04:19:49.656192 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-296kp" is a ceph pod!
2024-05-30 04:19:49.656199 D | ceph-nodedaemon-controller: "rook-ceph-mgr-a-7d654c5c8d-qs9lk" is a ceph pod!
2024-05-30 04:19:49.656203 D | ceph-nodedaemon-controller: "rook-ceph-mon-a-85cfd49cbd-hrkdh" is a ceph pod!
2024-05-30 04:19:49.656466 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-30 04:19:49.656545 D | ceph-spec: "ceph-block-pool-controller": ceph status is "HEALTH_OK", operator is ready to run ceph command, reconciling
2024-05-30 04:19:49.657938 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2024-05-30 04:19:49.658729 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:19:49.659132 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:19:49.659323 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:19:49.659454 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00119a2a0], assignment=&{Schedule:map[a:0xc001b1e980]}
2024-05-30 04:19:49.659536 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2024-05-30 04:19:49.659797 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-30 04:19:49.659947 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:19:49.660101 I | op-bucket-prov: successfully reconciled bucket provisioner
I0530 04:19:49.660153       1 manager.go:135] "msg"="starting provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket"
2024-05-30 04:19:49.660556 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:19:49.660653 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:19:49.660724 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:19:49.661142 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:19:49.661633 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:19:49.661665 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc001136b10], assignment=&{Schedule:map[a:0xc0014b0240]}
2024-05-30 04:19:49.661674 D | ceph-csi: cluster "rook-ceph/rook-ceph-operator-config": not deploying the ceph-csi plugin holder
2024-05-30 04:19:49.661694 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:19:49.661804 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:19:49.661880 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00077f140], assignment=&{Schedule:map[a:0xc001a9e680]}
2024-05-30 04:19:49.661955 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:19:49.662099 I | ceph-block-pool-controller: creating pool ".mgr" in namespace "rook-ceph"
2024-05-30 04:19:49.662137 D | exec: Running command: ceph osd crush rule create-replicated .mgr default host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:49.662386 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:19:49.668222 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:19:49.668376 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:19:49.671482 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-30 04:19:49.671642 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:19:49.671793 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:19:49.671948 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:19:49.671972 I | op-osd: context cancelled, exiting OSD update and create loop
2024-05-30 04:19:49.672041 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "failed to create cluster: failed to start ceph osds: failed to update/create OSDs: context canceled"
2024-05-30 04:19:49.672153 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:49.682486 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.682633 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:49.682861 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.682893 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:49.683901 I | ceph-cluster-controller: context cancelled, exiting reconcile
2024-05-30 04:19:49.684003 D | ceph-cluster-controller: successfully configured CephCluster "rook-ceph/my-cluster"
2024-05-30 04:19:49.738850 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:19:49.738901 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000dceab0], assignment=&{Schedule:map[a:0xc0014b0b80]}
2024-05-30 04:19:49.742785 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2024-05-30 04:19:49.742812 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2024-05-30 04:19:49.742819 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2024-05-30 04:19:49.742824 D | ceph-cluster-controller: cluster spec successfully validated
2024-05-30 04:19:49.742857 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2024-05-30 04:19:49.742892 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-30 04:19:49.742921 I | op-mon: stopping monitoring of mons in namespace "rook-ceph"
2024-05-30 04:19:49.742941 D | ceph-cluster-controller: checking health of cluster
2024-05-30 04:19:49.742986 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-30 04:19:49.751233 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v18...
2024-05-30 04:19:49.753444 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.753630 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:49.753815 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:49.753844 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:49.979672 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:32}]
2024-05-30 04:19:49.979695 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-30 04:19:49.980576 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:49.981291 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:49.981309 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-30 04:19:49.982078 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:49.982146 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
W0530 04:19:49.982555       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: watch of *v1.CephObjectRealm ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0530 04:19:49.982716       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: watch of *v1.CephFilesystemSubVolumeGroup ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
2024-05-30 04:19:49.983042 I | operator: successfully started the controller-runtime manager
2024-05-30 04:19:50.009559 D | exec: Running command: ceph osd pool get .mgr all --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:50.051791 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:32}]
2024-05-30 04:19:50.051810 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-30 04:19:50.052652 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.059734 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.059758 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-30 04:19:50.060523 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.060573 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-30 04:19:50.063188 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.063311 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:19:50.063373 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:19:50.063443 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:19:50.063689 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:50.140902 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:19:50.143144 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:24 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:32}] Version:0 NumPgs:32 DataBytes:456823 UsedBytes:92393472 AvailableBytes:11522175553536 TotalBytes:11522267947008 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-30 04:19:50.146517 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:50.356777 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:19:50.366616 D | exec: Running command: ceph osd pool application get .mgr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:50.458384 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:32}]
2024-05-30 04:19:50.458417 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-30 04:19:50.459455 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.460070 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.460088 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-30 04:19:50.460880 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.460980 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-30 04:19:50.464608 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:50.528343 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:19:50.528466 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:19:50.528591 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:d8c583a9-142f-4f28-8765-03db8b78c537 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:24 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:32}] Version:0 NumPgs:32 DataBytes:456823 UsedBytes:92393472 AvailableBytes:11522175553536 TotalBytes:11522267947008 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-30 04:19:50.528603 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-30 04:19:50.538489 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-05-30 04:19:50.540931 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:50.540972 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:50.680438 I | cephclient: application "mgr" is already set on pool ".mgr"
2024-05-30 04:19:50.680458 I | cephclient: reconciling replicated pool .mgr succeeded
2024-05-30 04:19:50.680462 D | cephclient: skipping check for failure domain and deviceClass on pool ".mgr" as it is not specified
2024-05-30 04:19:50.680467 I | ceph-block-pool-controller: initializing pool ".mgr" for RBD use
2024-05-30 04:19:50.680479 D | exec: Running command: rbd pool init .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-30 04:19:50.720523 I | ceph-block-pool-controller: successfully initialized pool ".mgr" for RBD use
2024-05-30 04:19:50.720539 D | ceph-block-pool-controller: configuring RBD per-image IO statistics collection
2024-05-30 04:19:50.720591 D | exec: Running command: ceph config get mgr mgr/prometheus/rbd_stats_pools --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:50.945018 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-30 04:19:51.126225 D | ceph-block-pool-controller: RBD per-image IO statistics will be collected for pools: []
2024-05-30 04:19:51.126260 I | op-config: setting "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-30 04:19:51.126288 D | exec: Running command: ceph config set mgr mgr/prometheus/rbd_stats_pools  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:51.512320 I | op-config: successfully set "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-30 04:19:51.512351 D | ceph-block-pool-controller: configured RBD per-image IO statistics collection
2024-05-30 04:19:51.512360 D | ceph-block-pool-controller: reconciling create rbd mirror peer configuration
2024-05-30 04:19:51.519451 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-30 04:19:51.519514 D | ceph-spec: update event on CephBlockPool CR
2024-05-30 04:19:51.519544 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Ready"
2024-05-30 04:19:51.519568 D | ceph-block-pool-controller: done reconciling
2024-05-30 04:19:51.519585 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-30 04:19:51.519676 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:19:51.519771 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:19:51.519834 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:19:51.520061 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:51.542181 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2024-05-30 04:19:51.742898 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:19:51.743029 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:19:51.743835 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:19:51.754630 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:19:51.754658 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:19:51.945073 I | ceph-csi: Kubernetes version is 1.30
2024-05-30 04:19:51.949025 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:32}]
2024-05-30 04:19:51.949070 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-30 04:19:51.950286 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:51.952172 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:51.952240 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-30 04:19:51.953043 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:51.953068 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-30 04:19:51.958546 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:19:52.157313 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:19:52.345638 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.10.2"
2024-05-30 04:19:52.725334 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-05-30 04:19:52.739312 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-30 04:19:52.943458 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:19:53.542823 I | ceph-spec: detected ceph image version: "18.2.2-0 reef"
2024-05-30 04:19:53.542857 I | ceph-cluster-controller: validating ceph version from provided image
2024-05-30 04:19:53.555595 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:19:53.555688 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:19:53.555712 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:19:53.555721 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-30 04:19:53.741086 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:19:53.940700 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:19:53.940786 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc001b60720], assignment=&{Schedule:map[a:0xc00064f640]}
2024-05-30 04:19:54.141220 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:19:54.141274 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:19:54.141575 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:19:54.141619 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:54.683335 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:19:54.683363 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:19:54.683416 D | ceph-cluster-controller: both cluster and image spec versions are identical, doing nothing 18.2.2-0 reef
2024-05-30 04:19:54.683423 I | ceph-cluster-controller: cluster "rook-ceph": version "18.2.2-0 reef" detected for image "quay.io/ceph/ceph:v18"
2024-05-30 04:19:54.696908 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2024-05-30 04:19:54.705035 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:54.705057 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:54.708069 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-05-30 04:19:54.708086 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-05-30 04:19:54.708122 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2024-05-30 04:19:54.716589 D | op-mon: Acquiring lock for mon orchestration
2024-05-30 04:19:54.716602 D | op-mon: Acquired lock for mon orchestration
2024-05-30 04:19:54.716605 I | op-mon: start running mons
2024-05-30 04:19:54.716607 D | op-mon: establishing ceph cluster info
2024-05-30 04:19:54.716647 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:19:54.716666 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:19:54.718504 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-30 04:19:54.738496 I | ceph-spec: parsing mon endpoints: a=10.105.94.142:6789
2024-05-30 04:19:54.738526 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0020192c0], assignment=&{Schedule:map[a:0xc00201e5c0]}
2024-05-30 04:19:54.764146 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-30 04:19:55.544417 I | ceph-csi: Detected ceph CSI image version: "v3.10.2"
2024-05-30 04:19:55.560189 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:19:55.560255 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:19:55.560270 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:19:55.560307 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-30 04:19:55.643370 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-30 04:19:55.723703 I | ceph-csi: successfully started CSI CephFS driver
2024-05-30 04:19:55.732629 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-30 04:19:55.743245 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-30 04:19:55.743287 I | ceph-csi: CSI NFS driver disabled
2024-05-30 04:19:55.743300 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-30 04:19:55.749408 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-30 04:19:55.749443 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-30 04:19:55.944959 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-30 04:19:56.144105 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-30 04:19:56.144148 I | ceph-csi: successfully removed CSI NFS driver
2024-05-30 04:19:56.343524 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.94.142:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.94.142:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-30 04:19:56.540564 D | op-config: updating config secret "rook-ceph-config"
2024-05-30 04:19:56.941907 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:19:56.941962 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:19:56.942230 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:19:56.942262 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:19:57.540485 D | op-cfg-keyring: updating secret for rook-ceph-mons-keyring
2024-05-30 04:19:57.940658 D | op-cfg-keyring: updating secret for rook-ceph-admin-keyring
2024-05-30 04:19:58.541937 I | op-mon: targeting the mon count 1
2024-05-30 04:19:58.549185 D | op-mon: Host network for mon "a" is false
2024-05-30 04:19:58.549235 D | op-mon: mon a already scheduled
2024-05-30 04:19:58.549244 D | op-mon: mons have been scheduled
2024-05-30 04:19:58.555997 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-05-30 04:19:58.556045 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3970356456 -o /var/lib/rook/3970356456.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:58.961014 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:19:58.961464 I | op-config: applying ceph settings:
[global]
log to file = false
2024-05-30 04:19:58.961500 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/591033502 -o /var/lib/rook/591033502.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:59.315217 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:19:59.315460 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-05-30 04:19:59.315512 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:19:59.741635 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-05-30 04:19:59.741665 I | op-mon: checking for basic quorum with existing mons
2024-05-30 04:19:59.745919 D | op-k8sutil: creating service rook-ceph-mon-a
2024-05-30 04:19:59.768948 D | op-k8sutil: updating service rook-ceph-mon-a
2024-05-30 04:19:59.789828 I | op-mon: mon "a" cluster IP is 10.105.94.142
2024-05-30 04:19:59.813557 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-30 04:19:59.942724 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.94.142:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.94.142:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-30 04:20:00.141845 D | op-config: updating config secret "rook-ceph-config"
2024-05-30 04:20:00.540403 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-30 04:20:00.540443 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-30 04:20:00.540659 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-30 04:20:00.540684 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-30 04:20:00.950360 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.105.94.142 Port:6789 Zone: NodeName:minikube-m02 DataPathMap:0xc001e91f80 UseHostNetwork:false}
2024-05-30 04:20:00.958686 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-05-30 04:20:00.958731 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2024-05-30 04:20:00.974314 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2024-05-30 04:20:00.974362 I | op-mon: waiting for mon quorum with [a]
2024-05-30 04:20:01.147896 I | op-mon: mons running: [a]
2024-05-30 04:20:01.147952 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:01.717622 I | op-mon: Monitors in quorum: [a]
2024-05-30 04:20:01.717664 I | op-mon: mons created: 1
2024-05-30 04:20:01.717692 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:02.243149 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:20:02.243196 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-30 04:20:02.243280 I | op-mon: waiting for mon quorum with [a]
2024-05-30 04:20:02.256763 I | op-mon: mons running: [a]
2024-05-30 04:20:02.256834 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:02.727347 I | op-mon: Monitors in quorum: [a]
2024-05-30 04:20:02.727395 I | ceph-spec: not applying network settings for cluster "rook-ceph" ceph networks
2024-05-30 04:20:02.727429 D | op-mon: mon endpoints used are: a=10.105.94.142:6789
2024-05-30 04:20:02.727437 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-05-30 04:20:02.727446 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-05-30 04:20:02.727454 D | op-mon: Released lock for mon orchestration
2024-05-30 04:20:02.727462 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-05-30 04:20:02.727473 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-05-30 04:20:02.727501 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:03.298790 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-05-30 04:20:03.298855 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:03.768903 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-05-30 04:20:03.768968 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:04.204826 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-05-30 04:20:04.204892 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:04.619118 D | op-cfg-keyring: updating secret for rook-csi-cephfs-node
2024-05-30 04:20:04.624921 D | op-cfg-keyring: updating secret for rook-csi-rbd-provisioner
2024-05-30 04:20:04.630889 D | op-cfg-keyring: updating secret for rook-csi-rbd-node
2024-05-30 04:20:04.637017 D | op-cfg-keyring: updating secret for rook-csi-cephfs-provisioner
2024-05-30 04:20:04.640704 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2024-05-30 04:20:04.640717 I | cephclient: getting or creating ceph auth key "client.crash"
2024-05-30 04:20:04.640727 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:05.065381 D | op-cfg-keyring: updating secret for rook-ceph-crash-collector-keyring
2024-05-30 04:20:05.068407 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2024-05-30 04:20:05.068425 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-05-30 04:20:05.068438 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:05.620948 D | op-cfg-keyring: updating secret for rook-ceph-exporter-keyring
2024-05-30 04:20:05.624018 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2024-05-30 04:20:05.624032 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-05-30 04:20:05.624041 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:06.003257 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-05-30 04:20:06.003302 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-05-30 04:20:06.003337 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:06.489447 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-05-30 04:20:06.489490 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-05-30 04:20:06.489522 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:06.841944 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-05-30 04:20:06.841987 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-05-30 04:20:06.842022 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:07.262305 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-05-30 04:20:07.262328 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-05-30 04:20:07.262359 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:07.699287 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-05-30 04:20:07.699877 I | op-config: applying ceph settings:
[global]
bluefs_buffered_io             = false
mon_data_avail_warn            = 10
mon_warn_on_pool_no_redundancy = false
osd_pool_default_size          = 1
bdev_flock_retry               = 20
2024-05-30 04:20:07.699929 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/1257696821 -o /var/lib/rook/1257696821.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:08.132491 I | op-config: successfully applied settings to the mon configuration database
2024-05-30 04:20:08.132648 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-05-30 04:20:08.132670 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-05-30 04:20:08.132700 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:08.646462 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "my-cluster"
2024-05-30 04:20:08.646539 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-my-cluster" in namespace "rook-ceph"
2024-05-30 04:20:08.646546 D | op-k8sutil: creating secret cluster-peer-token-my-cluster
2024-05-30 04:20:08.657055 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-05-30 04:20:08.671529 I | op-mgr: start running mgr
2024-05-30 04:20:08.673847 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:20:08.673967 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:20:08.677697 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-05-30 04:20:08.677838 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:09.232628 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-05-30 04:20:09.235260 D | op-cfg-keyring: updating secret for rook-ceph-mgr-a-keyring
2024-05-30 04:20:09.239069 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc001446570}
2024-05-30 04:20:09.246783 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2024-05-30 04:20:09.252092 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2024-05-30 04:20:09.257987 D | op-mgr: expected number 1 of mgrs found
2024-05-30 04:20:09.259342 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:20:09.259432 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-05-30 04:20:09.275881 D | op-k8sutil: updating service rook-ceph-mgr-dashboard
2024-05-30 04:20:09.282045 D | op-k8sutil: creating service rook-ceph-mgr
2024-05-30 04:20:09.297796 D | op-k8sutil: updating service rook-ceph-mgr
2024-05-30 04:20:09.306131 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2024-05-30 04:20:09.306163 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:09.306261 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:09.306360 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-05-30 04:20:09.306495 I | op-mgr: successful modules: balancer
2024-05-30 04:20:09.306501 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:09.324782 I | op-osd: start running osds in namespace "rook-ceph"
2024-05-30 04:20:09.324836 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-05-30 04:20:09.327013 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:20:09.327048 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:20:09.433718 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:09.873517 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:32}]
2024-05-30 04:20:09.873535 I | op-osd: placement group status: "all PGs in cluster are clean"
2024-05-30 04:20:09.884536 I | op-osd: 1 osd(s) require migration to new backend store "bluestore".
2024-05-30 04:20:09.884561 I | op-osd: starting migration of the OSD.1
2024-05-30 04:20:09.884567 I | op-osd: removing the OSD deployment "rook-ceph-osd-1"
2024-05-30 04:20:09.884572 D | op-k8sutil: removing rook-ceph-osd-1 deployment if it exists
2024-05-30 04:20:09.884576 I | op-k8sutil: removing deployment rook-ceph-osd-1 if it exists
2024-05-30 04:20:09.894717 I | op-k8sutil: Removed deployment rook-ceph-osd-1
2024-05-30 04:20:09.894849 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:20:09.894861 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:20:09.898218 I | op-k8sutil: "rook-ceph-osd-1" still found. waiting...
2024-05-30 04:20:09.917540 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:20:09.917563 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:20:09.931904 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:20:09.931985 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:20:09.932073 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:20:09.932310 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:09.950594 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:20:09.950665 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:20:10.312454 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:32}]
2024-05-30 04:20:10.312507 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-30 04:20:10.313975 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:20:10.315746 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:20:10.315812 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-30 04:20:10.317276 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:20:10.317293 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-30 04:20:10.322958 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:20:10.322976 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-30 04:20:10.740157 I | op-mgr: successful modules: mgr module(s) from the spec
2024-05-30 04:20:10.740400 D | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:10.749144 I | op-mgr: successful modules: prometheus
2024-05-30 04:20:11.771041 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:12.022235 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-758f5c4867-fqlpk" is a ceph pod!
2024-05-30 04:20:12.024116 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:20:12.026517 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:20:12.038614 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:20:12.038634 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:20:12.063668 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:20:12.069029 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:20:12.069222 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:20:12.069774 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:20:12.127078 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-05-30 04:20:12.127203 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-05-30 04:20:12.127222 I | ceph-spec: object "rook-ceph-osd-1" matched on delete, reconciling
2024-05-30 04:20:12.127300 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-05-30 04:20:12.127319 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-05-30 04:20:12.127326 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-05-30 04:20:12.272649 I | op-mgr: successful modules: orchestrator modules
2024-05-30 04:20:13.915383 I | op-k8sutil: confirmed rook-ceph-osd-1 does not exist
2024-05-30 04:20:13.934575 D | op-osd: 2 of 2 OSD Deployments need update
2024-05-30 04:20:13.934594 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-05-30 04:20:13.936905 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-05-30 04:20:13.936923 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-05-30 04:20:13.940263 I | op-osd: 2 of the 2 storage nodes are valid
2024-05-30 04:20:13.951120 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m02 to start a new one
2024-05-30 04:20:13.961585 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 still exists
2024-05-30 04:20:13.980332 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-296kp" is a ceph pod!
2024-05-30 04:20:13.980565 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:20:13.981243 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:20:13.990003 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:20:13.990101 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:20:14.002685 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:20:14.011577 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:20:15.752773 I | op-mgr: the dashboard secret was already generated
2024-05-30 04:20:15.752824 I | op-mgr: setting ceph dashboard "admin" login creds
2024-05-30 04:20:15.753266 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/936011492 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:16.156673 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/936011492 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:16.965225 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 deleted
2024-05-30 04:20:16.976580 I | op-osd: started OSD provisioning job for node "minikube-m02"
2024-05-30 04:20:16.993415 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m03 to start a new one
2024-05-30 04:20:16.998619 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-g4g6b" is a ceph pod!
2024-05-30 04:20:16.998669 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:20:16.998911 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:20:16.999810 I | op-mgr: successfully set ceph dashboard creds
2024-05-30 04:20:16.999928 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:17.011969 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 still exists
2024-05-30 04:20:17.014788 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:20:17.014804 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:20:17.029014 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-4gb6h" is a ceph pod!
2024-05-30 04:20:17.033851 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:20:17.040463 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:20:17.040575 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:20:17.041054 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:20:17.047235 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:20:17.047248 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:20:17.059109 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:20:17.065386 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:20:17.364334 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:17.719045 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:18.119854 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:18.196171 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-30 04:20:18.196193 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:20:18.533917 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:18.882759 I | op-mgr: successful modules: dashboard
2024-05-30 04:20:20.014821 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 deleted
2024-05-30 04:20:20.026218 I | op-osd: started OSD provisioning job for node "minikube-m03"
2024-05-30 04:20:20.029129 I | op-osd: OSD orchestration status for node minikube-m02 is "orchestrating"
2024-05-30 04:20:20.029176 I | op-osd: OSD orchestration status for node minikube-m03 is "starting"
2024-05-30 04:20:20.043490 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-rvhhr" is a ceph pod!
2024-05-30 04:20:20.043681 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-30 04:20:20.044214 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:20:20.059107 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-30 04:20:20.059128 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:20:20.074841 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:20:20.083689 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:20:20.131830 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:20.593634 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:20.909911 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:21.196558 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-30 04:20:21.196574 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:20:21.212397 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:21.497463 I | op-osd: OSD 0 is not ok-to-stop. will try updating it again later
2024-05-30 04:20:21.497579 I | op-osd: OSD orchestration status for node minikube-m02 is "completed"
2024-05-30 04:20:21.497592 D | op-osd: not creating deployment for OSD 0 which already exists
2024-05-30 04:20:21.497596 I | op-osd: creating OSD 1 on node "minikube-m02"
2024-05-30 04:20:21.497756 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-30 04:20:21.497766 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-30 04:20:21.497859 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m02\""
2024-05-30 04:20:21.517240 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-30 04:20:21.517254 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-30 04:20:21.538015 D | op-osd: not creating deployment for OSD 2 which already exists
2024-05-30 04:20:21.542196 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:20:21.542214 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:20:21.542222 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m02-status" config map changes
2024-05-30 04:20:21.542229 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-30 04:20:21.543021 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m02-status"
2024-05-30 04:20:21.554047 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:20:21.554169 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:20:21.571516 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-30 04:20:21.571648 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-30 04:20:21.571721 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-30 04:20:21.571939 D | clusterdisruption-controller: osd "1" POD is not assigned to any node. assuming node drain
2024-05-30 04:20:21.572043 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down and a possible node drain is detected
2024-05-30 04:20:21.572111 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:21.575007 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-57665bf87-v6jph" is a ceph pod!
2024-05-30 04:20:21.575139 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-30 04:20:21.575457 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-30 04:20:21.588311 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-30 04:20:21.588359 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-30 04:20:21.597815 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:20:21.597829 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:20:21.601649 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-30 04:20:21.606265 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-30 04:20:21.632713 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-30 04:20:21.632728 D | ceph-spec: do not reconcile deployments updates
2024-05-30 04:20:21.643778 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:21.963967 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:22.035640 I | clusterdisruption-controller: osd is down in failure domain "minikube-m02". pg health: "cluster is not fully clean. PGs: [{StateName:active+clean Count:20} {StateName:stale+active+clean Count:12}]"
2024-05-30 04:20:22.035665 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:22.268053 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:22.340913 D | exec: Running command: ceph osd set-group noout minikube-m02 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:22.594370 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:22.701957 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:20:22.701992 I | clusterdisruption-controller: creating temporary blocking pdb "rook-ceph-osd-host-minikube-m03" with maxUnavailable=0 for "host" failure domain "minikube-m03"
2024-05-30 04:20:22.704374 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-30 04:20:22.704863 D | op-k8sutil: kubernetes version fetched 1.30.0
2024-05-30 04:20:22.704905 I | clusterdisruption-controller: deleting the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-30 04:20:22.874451 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:20:22.975232 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:23.039230 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-30 04:20:23.039322 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:20:23.281346 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:23.660883 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:23.967678 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:24.265380 I | op-osd: OSD 0 is not ok-to-stop. will try updating it again later
2024-05-30 04:20:24.265425 I | op-osd: OSD orchestration status for node minikube-m03 is "orchestrating"
2024-05-30 04:20:24.366486 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:24.683347 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:25.011469 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:25.341622 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:25.679654 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:20:25.779878 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:26.096173 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:26.461866 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:26.581510 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-30 04:20:26.581555 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:20:26.882316 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:27.347079 I | op-osd: OSD 0 is not ok-to-stop. will try updating it again later
2024-05-30 04:20:27.347199 I | op-osd: OSD orchestration status for node minikube-m03 is "failed"
2024-05-30 04:20:27.347239 E | op-osd: failed to provision OSD(s) on node minikube-m03. &{OSDs:[] Status:failed PvcBackedOSD:false Message:failed to configure devices: failed to initialize osd: failed to run ceph-volume raw command. Running command: /usr/bin/ceph-authtool --gen-print-key
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd tree -f json
Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 42f51d9b-c3c5-4c1f-bbda-c1ea1db4caa5 1
 stderr: Error EEXIST: entity osd.1 exists but key does not match
Traceback (most recent call last):
  File "/usr/sbin/ceph-volume", line 11, in <module>
    load_entry_point('ceph-volume==1.0.0', 'console_scripts', 'ceph-volume')()
  File "/usr/lib/python3.6/site-packages/ceph_volume/main.py", line 41, in __init__
    self.main(self.argv)
  File "/usr/lib/python3.6/site-packages/ceph_volume/decorators.py", line 59, in newfunc
    return f(*a, **kw)
  File "/usr/lib/python3.6/site-packages/ceph_volume/main.py", line 153, in main
    terminal.dispatch(self.mapper, subcommand_args)
  File "/usr/lib/python3.6/site-packages/ceph_volume/terminal.py", line 194, in dispatch
    instance.main()
  File "/usr/lib/python3.6/site-packages/ceph_volume/devices/raw/main.py", line 32, in main
    terminal.dispatch(self.mapper, self.argv)
  File "/usr/lib/python3.6/site-packages/ceph_volume/terminal.py", line 194, in dispatch
    instance.main()
  File "/usr/lib/python3.6/site-packages/ceph_volume/devices/raw/prepare.py", line 160, in main
    self.safe_prepare(self.args)
  File "/usr/lib/python3.6/site-packages/ceph_volume/devices/raw/prepare.py", line 80, in safe_prepare
    self.prepare()
  File "/usr/lib/python3.6/site-packages/ceph_volume/decorators.py", line 16, in is_root
    return func(*a, **kw)
  File "/usr/lib/python3.6/site-packages/ceph_volume/devices/raw/prepare.py", line 116, in prepare
    osd_id=self.args.osd_id)
  File "/usr/lib/python3.6/site-packages/ceph_volume/util/prepare.py", line 154, in create_id
    raise RuntimeError('Unable to create a new OSD id')
RuntimeError: Unable to create a new OSD id: exit status 1}
2024-05-30 04:20:27.351579 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:20:27.351667 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:20:27.351670 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m03-status" config map changes
2024-05-30 04:20:27.351672 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-30 04:20:27.352336 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m03-status"
2024-05-30 04:20:27.452666 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:27.887547 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:28.269743 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:28.675497 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:28.997575 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-30 04:20:29.098380 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:29.431563 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:29.904951 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-30 04:20:30.366300 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
