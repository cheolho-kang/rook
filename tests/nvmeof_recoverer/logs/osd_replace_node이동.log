2024/05/28 13:42:08 maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
2024-05-28 13:42:08.924492 I | rookcmd: starting Rook v1.14.0-alpha.0.129.gbd90e21c8-dirty with arguments '/usr/local/bin/rook ceph operator'
2024-05-28 13:42:08.924530 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO
2024-05-28 13:42:08.924538 I | cephcmd: starting Rook-Ceph operator
2024-05-28 13:42:09.100779 I | cephcmd: base ceph version inside the rook operator image is "ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)"
2024-05-28 13:42:09.110289 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="false" (env var)
2024-05-28 13:42:09.110310 I | operator: watching all namespaces for Ceph CRs
2024-05-28 13:42:09.110386 I | operator: setting up schemes
2024-05-28 13:42:09.120184 I | operator: setting up the controller-runtime manager
2024-05-28 13:42:09.121503 I | ceph-cluster-controller: successfully started
2024-05-28 13:42:09.126105 I | op-k8sutil: ROOK_DISABLE_DEVICE_HOTPLUG="false" (configmap)
2024-05-28 13:42:09.126160 I | ceph-cluster-controller: enabling hotplug orchestration
2024-05-28 13:42:09.126242 I | ceph-nodedaemon-controller: successfully started
2024-05-28 13:42:09.126305 I | ceph-block-pool-controller: successfully started
2024-05-28 13:42:09.126410 I | ceph-object-store-user-controller: successfully started
2024-05-28 13:42:09.126492 I | ceph-object-realm-controller: successfully started
2024-05-28 13:42:09.126538 I | ceph-object-zonegroup-controller: successfully started
2024-05-28 13:42:09.126589 I | ceph-object-zone-controller: successfully started
2024-05-28 13:42:09.126880 I | ceph-object-controller: successfully started
2024-05-28 13:42:09.127037 I | ceph-file-controller: successfully started
2024-05-28 13:42:09.127125 I | ceph-nfs-controller: successfully started
2024-05-28 13:42:09.127218 I | ceph-rbd-mirror-controller: successfully started
2024-05-28 13:42:09.127310 I | ceph-client-controller: successfully started
2024-05-28 13:42:09.127377 I | ceph-filesystem-mirror-controller: successfully started
2024-05-28 13:42:09.127434 I | operator: rook-ceph-operator-config-controller successfully started
2024-05-28 13:42:09.127487 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2024-05-28 13:42:09.127910 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2024-05-28 13:42:09.128061 I | ceph-bucket-topic: successfully started
2024-05-28 13:42:09.128143 I | ceph-bucket-notification: successfully started
2024-05-28 13:42:09.128193 I | ceph-bucket-notification: successfully started
2024-05-28 13:42:09.128237 I | ceph-fs-subvolumegroup-controller: successfully started
2024-05-28 13:42:09.128288 I | blockpool-rados-namespace-controller: successfully started
2024-05-28 13:42:09.128329 I | ceph-cosi-controller: successfully started
2024-05-28 13:42:09.128380 I | nvmeofstorage-controller: successfully started
2024-05-28 13:42:09.128472 I | operator: starting the controller-runtime manager
2024-05-28 13:42:09.245580 I | op-k8sutil: ROOK_WATCH_FOR_NODE_FAILURE="true" (configmap)
2024-05-28 13:42:09.356189 I | ceph-spec: adding finalizer "cephblockpool.ceph.rook.io" on "builtin-mgr"
2024-05-28 13:42:09.367404 W | ceph-block-pool-controller: failed to set pool "builtin-mgr" status to "Progressing". failed to update object "rook-ceph/builtin-mgr" status: Operation cannot be fulfilled on cephblockpools.ceph.rook.io "builtin-mgr": the object has been modified; please apply your changes to the latest version and try again
2024-05-28 13:42:09.381843 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2024-05-28 13:42:09.381908 I | op-k8sutil: ROOK_LOG_LEVEL="DEBUG" (configmap)
2024-05-28 13:42:09.381947 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2024-05-28 13:42:09.392060 I | op-k8sutil: ROOK_CEPH_ALLOW_LOOP_DEVICES="false" (configmap)
2024-05-28 13:42:09.392115 I | operator: rook-ceph-operator-config-controller done reconciling
2024-05-28 13:42:09.401981 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2024-05-28 13:42:09.402109 I | op-k8sutil: ROOK_CSI_DISABLE_DRIVER="false" (configmap)
2024-05-28 13:42:09.427976 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2024-05-28 13:42:09.428038 I | op-k8sutil: CSI_DISABLE_HOLDER_PODS="true" (configmap)
2024-05-28 13:42:09.533858 I | ceph-csi: cluster info for cluster "my-cluster" is not ready yet, will retry in 10s, proceeding with ready clusters
2024-05-28 13:42:09.533884 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2024-05-28 13:42:09.533894 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2024-05-28 13:42:09.533908 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2024-05-28 13:42:09.533919 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2024-05-28 13:42:09.533929 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2024-05-28 13:42:09.533937 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2024-05-28 13:42:09.533946 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2024-05-28 13:42:09.533955 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2024-05-28 13:42:09.534000 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2024-05-28 13:42:09.534014 I | op-k8sutil: CSI_ENABLE_LIVENESS="false" (configmap)
2024-05-28 13:42:09.534022 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2024-05-28 13:42:09.534030 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2024-05-28 13:42:09.534038 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2024-05-28 13:42:09.534046 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2024-05-28 13:42:09.534056 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2024-05-28 13:42:09.534065 I | op-k8sutil: CSI_ENABLE_NFS_SNAPSHOTTER="true" (configmap)
2024-05-28 13:42:09.536125 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2024-05-28 13:42:09.536142 I | op-k8sutil: CSI_ENABLE_TOPOLOGY="false" (configmap)
2024-05-28 13:42:09.536147 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2024-05-28 13:42:09.536152 I | op-k8sutil: CSI_ENABLE_METADATA="false" (default)
2024-05-28 13:42:09.536160 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-28 13:42:09.536165 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-05-28 13:42:09.536237 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-28 13:42:09.536246 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-28 13:42:09.536250 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-05-28 13:42:09.536259 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2024-05-28 13:42:09.536264 I | ceph-csi: Kubernetes version is 1.30
2024-05-28 13:42:09.536270 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2024-05-28 13:42:09.536275 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2024-05-28 13:42:09.536281 I | op-k8sutil: CSI_LEADER_ELECTION_LEASE_DURATION="" (default)
2024-05-28 13:42:09.536285 I | op-k8sutil: CSI_LEADER_ELECTION_RENEW_DEADLINE="" (default)
2024-05-28 13:42:09.536292 I | op-k8sutil: CSI_LEADER_ELECTION_RETRY_PERIOD="" (default)
2024-05-28 13:42:09.637780 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:42:09.638116 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "my-cluster"
2024-05-28 13:42:09.638501 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:42:09.638851 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-28 13:42:09.644221 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:42:09.648228 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2024-05-28 13:42:09.648249 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:09.648369 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:09.651100 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:09.659563 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:42:09.659711 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:42:09.660071 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2024-05-28 13:42:09.660833 D | ceph-cluster-controller: skipping resource "my-cluster" update with unchanged spec
2024-05-28 13:42:09.734186 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2024-05-28 13:42:09.734253 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.11.0" (default)
2024-05-28 13:42:09.734280 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1" (default)
2024-05-28 13:42:09.734315 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v4.0.1" (default)
2024-05-28 13:42:09.734338 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v4.5.1" (default)
2024-05-28 13:42:09.734360 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2" (default)
2024-05-28 13:42:09.734382 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.10.1" (default)
2024-05-28 13:42:09.734432 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2024-05-28 13:42:09.734471 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.8.0" (default)
2024-05-28 13:42:09.734508 I | op-k8sutil: CSI_TOPOLOGY_DOMAIN_LABELS="" (default)
2024-05-28 13:42:09.734529 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2024-05-28 13:42:09.734546 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2024-05-28 13:42:09.734563 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2024-05-28 13:42:09.734580 I | op-k8sutil: CSI_CLUSTER_NAME="" (default)
2024-05-28 13:42:09.734599 I | op-k8sutil: ROOK_CSI_IMAGE_PULL_POLICY="IfNotPresent" (default)
2024-05-28 13:42:09.734629 I | op-k8sutil: CSI_CEPHFS_KERNEL_MOUNT_OPTIONS="" (default)
2024-05-28 13:42:09.734647 I | op-k8sutil: CSI_CEPHFS_ATTACH_REQUIRED="true" (configmap)
2024-05-28 13:42:09.734676 I | op-k8sutil: CSI_RBD_ATTACH_REQUIRED="true" (configmap)
2024-05-28 13:42:09.734694 I | op-k8sutil: CSI_NFS_ATTACH_REQUIRED="true" (configmap)
2024-05-28 13:42:09.734723 I | op-k8sutil: CSI_DRIVER_NAME_PREFIX="rook-ceph" (default)
2024-05-28 13:42:09.737617 I | op-k8sutil: CSI_ENABLE_VOLUME_GROUP_SNAPSHOT="true" (configmap)
2024-05-28 13:42:09.737639 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-05-28 13:42:09.737751 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2024-05-28 13:42:09.737767 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-28 13:42:09.799589 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:09.799914 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:42:09.800062 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:09.800171 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:09.800292 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:09.985124 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:10.129901 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2024-05-28 13:42:10.134491 D | ceph-cluster-controller: cluster spec successfully validated
2024-05-28 13:42:10.134631 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2024-05-28 13:42:10.151093 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v18...
2024-05-28 13:42:10.154090 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:42:10.154145 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:42:10.335737 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-28 13:42:10.530502 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-05-28 13:42:14.390118 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:42:14.390141 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:42:19.381770 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Progressing"
2024-05-28 13:42:19.381835 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:42:19.381848 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:42:19.381860 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:42:19.381907 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-28 13:42:19.381929 D | ceph-spec: update event on CephBlockPool CR
2024-05-28 13:42:19.382007 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:42:19.382072 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:19.382114 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:19.382168 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:19.475227 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:19.800463 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:42:19.800537 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:19.800587 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:19.800644 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:19.888132 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:19.985326 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:42:19.985400 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:19.985445 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:19.985493 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:20.068925 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:29.382870 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:42:29.383033 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:42:29.383074 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:42:29.889480 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:42:29.889687 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:29.890081 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:29.890278 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:30.057378 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:30.069990 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:42:30.070127 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:30.070241 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:30.070369 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:30.262609 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:30.531790 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:42:30.532444 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:42:39.383884 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:42:39.383912 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:42:39.383926 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:42:40.058289 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:42:40.058402 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:40.058465 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:40.058538 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:40.191628 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:40.263225 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:42:40.263351 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:40.263431 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:40.263524 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:40.371891 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-28 13:42:47.700834 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-05-28 13:42:47.726308 I | ceph-spec: detected ceph image version: "18.2.2-0 reef"
2024-05-28 13:42:47.726371 I | ceph-cluster-controller: validating ceph version from provided image
2024-05-28 13:42:47.735039 D | ceph-cluster-controller: cluster not initialized, nothing to validate. clusterInfo is nil
2024-05-28 13:42:47.735108 I | ceph-cluster-controller: cluster "rook-ceph": version "18.2.2-0 reef" detected for image "quay.io/ceph/ceph:v18"
2024-05-28 13:42:47.750562 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:42:47.750643 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:42:47.750708 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:42:47.750748 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:42:47.765237 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2024-05-28 13:42:47.767391 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:42:47.767438 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:42:47.773222 E | ceph-spec: failed to update cluster condition to {Type:Progressing Status:True Reason:ClusterProgressing Message:Configuring the Ceph cluster LastHeartbeatTime:2024-05-28 13:42:47.765214047 +0000 UTC m=+38.943374851 LastTransitionTime:2024-05-28 13:42:47.765213642 +0000 UTC m=+38.943374517}. failed to update object "rook-ceph/my-cluster" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "my-cluster": the object has been modified; please apply your changes to the latest version and try again
2024-05-28 13:42:47.776806 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-05-28 13:42:47.781481 I | ceph-cluster-controller: created placeholder configmap for ceph overrides "rook-config-override"
2024-05-28 13:42:47.781531 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-05-28 13:42:47.781631 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2024-05-28 13:42:47.798027 D | op-mon: Acquiring lock for mon orchestration
2024-05-28 13:42:47.798085 D | op-mon: Acquired lock for mon orchestration
2024-05-28 13:42:47.798110 I | op-mon: start running mons
2024-05-28 13:42:47.798125 D | op-mon: establishing ceph cluster info
2024-05-28 13:42:47.801378 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/mon.keyring --gen-key -n mon. --cap mon 'allow *'
2024-05-28 13:42:47.802474 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:42:47.802528 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:42:47.853374 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mgr 'allow *' --cap mds 'allow'
2024-05-28 13:42:47.927926 I | ceph-spec: creating mon secrets for a new cluster
2024-05-28 13:42:47.950849 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2024-05-28 13:42:48.109672 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data: mapping:{"node":{}} maxMonId:-1 outOfQuorum:]
2024-05-28 13:42:48.305600 D | op-config: creating config secret "rook-ceph-config"
2024-05-28 13:42:48.507993 D | op-config: updating config secret "rook-ceph-config"
2024-05-28 13:42:48.906229 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:42:48.906309 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:42:48.906534 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:42:48.906586 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:42:49.385021 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:42:49.385090 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:42:49.385126 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:42:49.515896 D | op-cfg-keyring: creating secret for rook-ceph-mons-keyring
2024-05-28 13:42:49.905696 D | op-cfg-keyring: creating secret for rook-ceph-admin-keyring
2024-05-28 13:42:50.193146 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:42:50.193332 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:50.193443 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:50.193553 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:50.305717 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:42:50.450415 D | clusterdisruption-controller: ceph "rook-ceph" cluster failed to check cluster health. failed to get status. . unable to get monitor info from DNS SRV with service name: ceph-mon
2024-05-28T13:42:50.442+0000 7e3fe49a3700 -1 failed for service _ceph-mon._tcp
2024-05-28T13:42:50.442+0000 7e3fe49a3700 -1 monclient: get_monmap_and_config cannot identify monitors to contact
[errno 2] RADOS object not found (error connecting to the cluster): exit status 1
2024-05-28 13:42:50.450675 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:42:50.450809 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:42:50.450930 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:42:50.451065 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:50.643474 D | clusterdisruption-controller: ceph "rook-ceph" cluster failed to check cluster health. failed to get status. . unable to get monitor info from DNS SRV with service name: ceph-mon
2024-05-28T13:42:50.634+0000 7da669948700 -1 failed for service _ceph-mon._tcp
2024-05-28T13:42:50.634+0000 7da669948700 -1 monclient: get_monmap_and_config cannot identify monitors to contact
[errno 2] RADOS object not found (error connecting to the cluster): exit status 1
2024-05-28 13:42:50.706083 I | ceph-spec: parsing mon endpoints: 
2024-05-28 13:42:50.706143 W | ceph-spec: ignoring invalid monitor 
2024-05-28 13:42:50.706202 D | ceph-spec: loaded: maxMonID=-1, mons=map[], assignment=&{Schedule:map[]}
2024-05-28 13:42:50.706278 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2024-05-28 13:42:50.706321 I | op-k8sutil: ROOK_OBC_PROVISIONER_NAME_PREFIX="" (default)
2024-05-28 13:42:50.706345 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2024-05-28 13:42:50.707546 I | op-bucket-prov: successfully reconciled bucket provisioner
I0528 13:42:50.707824       1 manager.go:135] "msg"="starting provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket"
2024-05-28 13:42:50.907221 I | op-mon: targeting the mon count 1
2024-05-28 13:42:50.912607 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP: Port:6789 Zone: NodeName: DataPathMap:0xc000796cf0 UseHostNetwork:false}
2024-05-28 13:42:50.927846 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2024-05-28 13:42:50.949141 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-28 13:42:50.949207 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:50.978602 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-28 13:42:50.978754 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:51.012176 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-28 13:42:51.012220 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:51.307234 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to minikube-m02
2024-05-28 13:42:51.307278 I | op-mon: mon a assigned to node minikube-m02
2024-05-28 13:42:51.307291 D | op-mon: using internal IP 192.168.58.3 for node minikube-m02
2024-05-28 13:42:51.307330 D | op-mon: mons have been scheduled
2024-05-28 13:42:51.316269 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2024-05-28 13:42:51.334572 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-28 13:42:51.334634 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:51.334945 I | op-mon: creating mon a
2024-05-28 13:42:51.335020 D | op-k8sutil: creating service rook-ceph-mon-a
2024-05-28 13:42:51.365998 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-28 13:42:51.366031 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:51.409913 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-28 13:42:51.409949 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:51.515768 D | op-k8sutil: created service rook-ceph-mon-a
2024-05-28 13:42:51.515825 I | op-mon: mon "a" cluster IP is 10.105.31.173
2024-05-28 13:42:51.908331 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-28 13:42:52.109993 D | op-mon: mons were added or removed from the endpoints cm
2024-05-28 13:42:52.110054 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-05-28 13:42:52.110135 D | op-mon: mons were added or removed from the endpoints cm
2024-05-28 13:42:52.110230 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-05-28 13:42:52.110253 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-05-28 13:42:52.110304 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-05-28 13:42:52.110747 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.31.173:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.31.173:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:-1 outOfQuorum:]
2024-05-28 13:42:52.117435 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:42:52.117502 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:42:52.117536 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:42:52.306106 D | op-config: updating config secret "rook-ceph-config"
2024-05-28 13:42:52.508029 D | ceph-spec: object "rook-ceph-config" matched on update
2024-05-28 13:42:52.508084 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2024-05-28 13:42:52.706736 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:42:52.706840 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:42:52.707158 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:42:52.707209 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:42:53.117696 I | op-mon: 0 of 1 expected mons are ready. creating or updating deployments without checking quorum in attempt to achieve a healthy mon cluster
2024-05-28 13:42:53.117882 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.105.31.173 Port:6789 Zone: NodeName: DataPathMap:0xc000796cf0 UseHostNetwork:false}
2024-05-28 13:42:53.161962 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-05-28 13:42:53.162011 D | op-mon: Starting mon: rook-ceph-mon-a
2024-05-28 13:42:53.205307 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-28 13:42:53.205383 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:53.211029 D | ceph-nodedaemon-controller: "rook-ceph-mon-a-7cc7cb8bdb-pwj7d" is a ceph pod!
2024-05-28 13:42:53.211296 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:42:53.212002 D | ceph-nodedaemon-controller: secret "rook-ceph-crash-collector-keyring" in namespace "rook-ceph" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2024-05-28 13:42:53.240658 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-28 13:42:53.240779 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:53.261000 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-28 13:42:53.261102 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:42:53.306943 I | op-mon: updating maxMonID from -1 to 0
2024-05-28 13:42:53.508558 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-05-28 13:42:53.508622 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-05-28 13:42:53.909988 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-28 13:42:54.106294 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.31.173:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.31.173:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-28 13:42:54.106347 I | op-mon: waiting for mon quorum with [a]
2024-05-28 13:42:54.307380 I | op-mon: mons running: [a]
2024-05-28 13:42:54.307430 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:42:59.386640 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:42:59.386709 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:42:59.386785 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:43:00.451502 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:43:00.451632 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:43:00.451701 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:43:00.451774 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:01.075842 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:01.075866 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:09.110796 D | operator: number of goroutines 485
2024-05-28 13:43:09.387966 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:43:09.387989 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:43:09.388000 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:43:09.418156 D | op-mon: failed to get quorum_status. mon quorum status failed: exit status 1
2024-05-28 13:43:10.297131 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-28 13:43:10.314882 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-05-28 13:43:10.332095 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:10.332168 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:10.332192 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:10.332222 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:10.350395 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2024-05-28 13:43:10.350441 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2024-05-28 13:43:10.350455 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2024-05-28 13:43:10.350467 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2024-05-28 13:43:10.350479 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2024-05-28 13:43:10.350491 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME="" (default)
2024-05-28 13:43:10.350504 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME_MOUNT="" (default)
2024-05-28 13:43:10.411580 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2024-05-28 13:43:10.411705 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-28 13:43:10.411729 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2024-05-28 13:43:10.438946 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-28 13:43:10.439028 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2024-05-28 13:43:10.439051 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2024-05-28 13:43:10.439070 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2024-05-28 13:43:10.439088 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME="" (default)
2024-05-28 13:43:10.439105 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME_MOUNT="" (default)
2024-05-28 13:43:10.473944 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2024-05-28 13:43:10.473982 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-28 13:43:10.473995 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2024-05-28 13:43:10.498895 I | ceph-csi: successfully started CSI CephFS driver
2024-05-28 13:43:10.498945 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="File" (configmap)
2024-05-28 13:43:10.511310 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-28 13:43:10.511356 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="File" (configmap)
2024-05-28 13:43:10.521612 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-28 13:43:10.521638 I | ceph-csi: CSI NFS driver disabled
2024-05-28 13:43:10.521646 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-28 13:43:10.529022 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-28 13:43:10.529042 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-28 13:43:10.542493 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-28 13:43:10.542520 I | ceph-csi: successfully removed CSI NFS driver
2024-05-28 13:43:10.553209 D | ceph-csi: csi config map "rook-ceph-csi-config" (in "rook-ceph") has the expected owner; owner id: "f8c7b9c5-b6c8-4f59-9057-a054fd88c045"
2024-05-28 13:43:10.560158 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:43:10.562175 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:43:10.562247 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0017c5590], assignment=&{Schedule:map[a:0xc00106b300]}
2024-05-28 13:43:10.562282 D | ceph-csi: cluster "rook-ceph/my-cluster": not deploying the ceph-csi plugin holder
2024-05-28 13:43:10.562295 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:43:10.565544 I | ceph-csi: Kubernetes version is 1.30
2024-05-28 13:43:10.570630 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-05-28 13:43:10.699678 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-28 13:43:10.701491 I | op-k8sutil: Removing previous job rook-ceph-csi-detect-version to start a new one
2024-05-28 13:43:10.706911 I | op-k8sutil: batch job rook-ceph-csi-detect-version still exists
2024-05-28 13:43:13.687852 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-28 13:43:13.687874 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:13.709293 I | op-k8sutil: batch job rook-ceph-csi-detect-version deleted
2024-05-28 13:43:14.036795 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-28 13:43:14.036853 I | clusterdisruption-controller: all PGs are active+clean. Restoring default OSD pdb settings
2024-05-28 13:43:14.036859 I | clusterdisruption-controller: creating the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-28 13:43:14.332553 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-28 13:43:14.332649 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:43:14.332697 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:43:14.332736 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:43:14.332782 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:14.424108 I | op-mon: mons running: [a]
2024-05-28 13:43:14.424143 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:14.513204 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-28 13:43:14.516594 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-28 13:43:14.516680 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:43:14.516728 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:43:14.516767 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:43:14.516813 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:14.598926 I | op-mon: Monitors in quorum: [a]
2024-05-28 13:43:14.598950 I | op-mon: mons created: 1
2024-05-28 13:43:14.598968 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:14.686689 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-28 13:43:14.688719 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-28 13:43:14.777115 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1}}
2024-05-28 13:43:14.777142 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1}}
2024-05-28 13:43:14.777184 I | op-mon: waiting for mon quorum with [a]
2024-05-28 13:43:14.782105 I | op-mon: mons running: [a]
2024-05-28 13:43:14.782137 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:14.954851 I | op-mon: Monitors in quorum: [a]
2024-05-28 13:43:14.990450 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-05-28 13:43:14.990485 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/1290111304 -o /var/lib/rook/1290111304.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:15.173720 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:43:15.174003 I | op-config: applying ceph settings:
[global]
log to file = false
2024-05-28 13:43:15.174042 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3183385200 -o /var/lib/rook/3183385200.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:15.395086 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:43:15.395148 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-05-28 13:43:15.395173 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:15.596510 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-05-28 13:43:15.596541 I | ceph-spec: not applying network settings for cluster "rook-ceph" ceph networks
2024-05-28 13:43:15.596549 D | op-mon: mon endpoints used are: a=10.105.31.173:6789
2024-05-28 13:43:15.596553 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-05-28 13:43:15.596558 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-05-28 13:43:15.596563 D | op-mon: Released lock for mon orchestration
2024-05-28 13:43:15.596569 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-05-28 13:43:15.596577 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-05-28 13:43:15.596600 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:15.780799 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-05-28 13:43:15.780834 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:15.846131 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-28 13:43:15.869891 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-05-28 13:43:15.886713 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:15.886743 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:15.886749 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:15.886783 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:43:15.917073 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-28 13:43:15.955220 I | ceph-csi: successfully started CSI CephFS driver
2024-05-28 13:43:15.980179 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-28 13:43:15.984054 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-28 13:43:15.984074 I | ceph-csi: CSI NFS driver disabled
2024-05-28 13:43:15.984080 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-28 13:43:15.986923 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-28 13:43:15.986943 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-28 13:43:15.995018 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-05-28 13:43:15.995071 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:16.075871 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-28 13:43:16.075889 I | ceph-csi: successfully removed CSI NFS driver
2024-05-28 13:43:16.173966 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-05-28 13:43:16.174012 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:16.351955 D | op-cfg-keyring: creating secret for rook-csi-rbd-provisioner
2024-05-28 13:43:16.356720 D | op-cfg-keyring: creating secret for rook-csi-rbd-node
2024-05-28 13:43:16.360697 D | op-cfg-keyring: creating secret for rook-csi-cephfs-provisioner
2024-05-28 13:43:16.363828 D | op-cfg-keyring: creating secret for rook-csi-cephfs-node
2024-05-28 13:43:16.366233 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2024-05-28 13:43:16.366252 I | cephclient: getting or creating ceph auth key "client.crash"
2024-05-28 13:43:16.366270 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:16.551793 D | op-cfg-keyring: creating secret for rook-ceph-crash-collector-keyring
2024-05-28 13:43:16.554289 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2024-05-28 13:43:16.554309 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-05-28 13:43:16.554325 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:16.742193 D | op-cfg-keyring: creating secret for rook-ceph-exporter-keyring
2024-05-28 13:43:16.850731 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2024-05-28 13:43:16.850756 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-05-28 13:43:16.850774 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:17.016768 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-05-28 13:43:17.016800 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-05-28 13:43:17.016820 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:17.190722 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-05-28 13:43:17.190747 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-05-28 13:43:17.190766 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:17.356323 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-05-28 13:43:17.356347 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-05-28 13:43:17.356367 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:17.524802 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-05-28 13:43:17.524827 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-05-28 13:43:17.524846 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:17.692841 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-05-28 13:43:17.693051 I | op-config: applying ceph settings:
[global]
mon_data_avail_warn            = 10
mon_warn_on_pool_no_redundancy = false
osd_pool_default_size          = 1
bdev_flock_retry               = 20
bluefs_buffered_io             = false
2024-05-28 13:43:17.693084 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3936583245 -o /var/lib/rook/3936583245.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:18.029620 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:18.029644 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:18.114331 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:43:18.114395 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-05-28 13:43:18.114402 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-05-28 13:43:18.114435 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:18.296017 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "my-cluster"
2024-05-28 13:43:18.296225 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-my-cluster" in namespace "rook-ceph"
2024-05-28 13:43:18.296238 D | op-k8sutil: creating secret cluster-peer-token-my-cluster
2024-05-28 13:43:18.300195 D | op-k8sutil: created secret cluster-peer-token-my-cluster
2024-05-28 13:43:18.300255 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-05-28 13:43:18.306325 I | op-mgr: start running mgr
2024-05-28 13:43:18.306945 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:43:18.306962 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:43:18.308100 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-05-28 13:43:18.308135 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:18.484267 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-05-28 13:43:18.485451 D | op-cfg-keyring: creating secret for rook-ceph-mgr-a-keyring
2024-05-28 13:43:18.487531 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc001239260}
2024-05-28 13:43:18.495821 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-05-28 13:43:18.495850 D | exec: Running command: ceph config set mon auth_allow_insecure_global_id_reclaim false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:18.504065 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-28 13:43:18.504084 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:18.505734 D | ceph-nodedaemon-controller: "rook-ceph-mgr-a-7c9d496659-ncb9h" is a ceph pod!
2024-05-28 13:43:18.505838 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:18.506152 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:18.511318 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "created"
2024-05-28 13:43:18.511347 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:18.517605 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-28 13:43:18.517623 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:18.521275 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:18.521294 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:18.521500 D | op-k8sutil: created service rook-ceph-exporter
2024-05-28 13:43:18.521514 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:18.532624 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m02-6b965ff7fc-jh9nm" is a ceph pod!
2024-05-28 13:43:18.532702 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:18.532999 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:18.536733 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-28 13:43:18.536753 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:18.538497 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:43:18.538520 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:18.539981 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:18.539996 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:18.545972 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:43:18.549072 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:18.549089 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:18.649132 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:18.673298 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-05-28 13:43:18.673322 I | op-config: insecure global ID is now disabled
2024-05-28 13:43:18.676851 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7c9d496659" is progressing.}] CollisionCount:<nil>}
2024-05-28 13:43:19.388674 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:43:19.388704 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:43:19.388719 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:43:20.552832 D | ceph-csi: csi config map "rook-ceph-csi-config" (in "rook-ceph") has the expected owner; owner id: "f8c7b9c5-b6c8-4f59-9057-a054fd88c045"
2024-05-28 13:43:20.558309 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:43:20.559661 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:43:20.559710 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000c18990], assignment=&{Schedule:map[a:0xc000133780]}
2024-05-28 13:43:20.559751 D | ceph-csi: cluster "rook-ceph/rook-ceph-operator-config": not deploying the ceph-csi plugin holder
2024-05-28 13:43:20.559765 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:43:20.562356 I | ceph-csi: Kubernetes version is 1.30
2024-05-28 13:43:20.565901 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-05-28 13:43:20.567238 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-28 13:43:20.679170 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:20.679195 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:21.687064 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7c9d496659" is progressing.}] CollisionCount:<nil>}
2024-05-28 13:43:22.784312 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-28 13:43:22.784354 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:22.821615 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-28 13:43:22.821680 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-28 13:43:22.821701 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-28 13:43:22.821716 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-28 13:43:22.821731 D | ceph-spec: do not reconcile "rook-ceph-mon-a-canary" on monitor canary deployments
2024-05-28 13:43:22.821747 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-28 13:43:23.213015 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:23.213521 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:23.227716 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:43:23.227791 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:23.243790 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:43:23.249687 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:24.690744 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7c9d496659" is progressing.}] CollisionCount:<nil>}
2024-05-28 13:43:27.699540 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7c9d496659" is progressing.}] CollisionCount:<nil>}
2024-05-28 13:43:29.389471 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:43:29.389544 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:43:29.389618 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:43:31.166835 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7c9d496659" is progressing.}] CollisionCount:<nil>}
2024-05-28 13:43:31.711985 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:31.712009 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:34.170071 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7c9d496659" is progressing.}] CollisionCount:<nil>}
2024-05-28 13:43:37.174369 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-28 13:43:18 +0000 UTC LastTransitionTime:2024-05-28 13:43:18 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-7c9d496659" is progressing.}] CollisionCount:<nil>}
2024-05-28 13:43:38.810268 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:38.810335 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:39.186734 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-28 13:43:39.186758 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:39.390463 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:43:39.390501 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:43:39.390519 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:43:39.552358 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:39.552383 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:40.182160 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2024-05-28 13:43:40.194034 D | op-mgr: expected number 1 of mgrs found
2024-05-28 13:43:40.194158 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-05-28 13:43:40.218572 D | op-k8sutil: created service rook-ceph-mgr-dashboard
2024-05-28 13:43:40.218686 D | op-k8sutil: creating service rook-ceph-mgr
2024-05-28 13:43:40.233674 D | op-k8sutil: created service rook-ceph-mgr
2024-05-28 13:43:40.238780 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-05-28 13:43:40.238845 I | op-mgr: successful modules: balancer
2024-05-28 13:43:40.238854 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:40.238921 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:40.238986 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2024-05-28 13:43:40.239111 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:40.252200 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:43:40.252231 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:43:40.253322 I | op-osd: start running osds in namespace "rook-ceph"
2024-05-28 13:43:40.253662 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-05-28 13:43:40.257639 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:40.713662 I | op-osd: placement group status: "cluster has no PGs"
2024-05-28 13:43:40.716634 I | op-osd: all osds have already been migrated to backend store "bluestore"
2024-05-28 13:43:40.716654 I | op-osd: no osd to replace
2024-05-28 13:43:40.718485 D | op-osd: !!osds to skip reconcilemap[]
2024-05-28 13:43:40.720067 D | op-osd: 0 of 0 OSD Deployments need update
2024-05-28 13:43:40.720082 D | op-osd: !!updateConfig: provisionConfig: &{/var/lib/rook  /var/lib/rook/rook-ceph}, numUpdatesNeeded: 0, deployments: &{map[]}, osdsToSkipReconcile: map[]
2024-05-28 13:43:40.720100 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-05-28 13:43:40.721750 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-05-28 13:43:40.721766 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-05-28 13:43:40.724580 I | op-osd: 2 of the 2 storage nodes are valid
2024-05-28 13:43:40.736366 I | op-osd: started OSD provisioning job for node "minikube-m02"
2024-05-28 13:43:40.743815 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-z99rr" is a ceph pod!
2024-05-28 13:43:40.743948 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:40.744258 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:40.747903 I | op-osd: started OSD provisioning job for node "minikube-m03"
2024-05-28 13:43:40.747921 D | op-osd: !!statusConfigMapsmap[rook-ceph-osd-minikube-m02-status:{} rook-ceph-osd-minikube-m03-status:{}]
2024-05-28 13:43:40.747955 D | op-osd: !!createConfig: provisionConfig: &{/var/lib/rook  /var/lib/rook/rook-ceph}, awaitingStatusConfigMaps: map[rook-ceph-osd-minikube-m02-status:{} rook-ceph-osd-minikube-m03-status:{}], deployments: &{map[]}, finishedStatusConfigMaps: map[], deployments: &{map[]}
2024-05-28 13:43:40.751165 D | op-osd: !!createOSDsForStatusMap: status: &{[] starting false }
2024-05-28 13:43:40.751291 I | op-osd: OSD orchestration status for node minikube-m02 is "starting"
2024-05-28 13:43:40.751345 D | op-osd: !!createOSDsForStatusMap: status: &{[] starting false }
2024-05-28 13:43:40.751668 I | op-osd: OSD orchestration status for node minikube-m03 is "starting"
2024-05-28 13:43:40.751730 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:43:40.751775 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:40.755262 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-hhtrp" is a ceph pod!
2024-05-28 13:43:40.763583 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:43:40.997771 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:40.997846 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:43:40.998159 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:40.998204 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:41.972397 I | op-mgr: successful modules: prometheus
2024-05-28 13:43:41.976860 I | op-mgr: successful modules: mgr module(s) from the spec
2024-05-28 13:43:41.976972 D | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:42.205191 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-28 13:43:42.205214 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-28 13:43:42.205505 D | op-osd: !!createOSDsForStatusMap: status: &{[] orchestrating false }
2024-05-28 13:43:42.205519 I | op-osd: OSD orchestration status for node minikube-m02 is "orchestrating"
2024-05-28 13:43:42.471405 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:42.471466 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:42.556627 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:42.556695 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:42.963688 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:44.333331 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:43:44.333418 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:43:44.333467 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:43:44.333516 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:44.835371 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-28 13:43:44.838429 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-28 13:43:44.838542 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:43:44.838604 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:43:44.838662 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:43:44.838744 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:45.303592 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-28 13:43:45.307277 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-28 13:43:45.717099 I | op-mgr: setting ceph dashboard "admin" login creds
2024-05-28 13:43:45.717657 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/880069048 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:46.492450 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:43:46.492474 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:43:49.391370 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:43:49.391398 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:43:49.391409 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:43:55.693109 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-28 13:43:55.693137 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-28 13:43:55.693292 D | op-osd: !!createOSDsForStatusMap: status: &{[{0 ceph 651e5d0a-ef0b-41cc-99a1-d0999d81f639  nvme /dev/nvme1n1p2   true root=default host=minikube-m02 false raw bluestore  false false  } {1 ceph 77de06c2-fc87-46ec-98f9-cdc97175dde9  nvme /dev/nvme0n1p2   true root=default host=minikube-m02 false raw bluestore  false false  }] completed false }
2024-05-28 13:43:55.693332 I | op-osd: OSD orchestration status for node minikube-m02 is "completed"
2024-05-28 13:43:55.693342 I | op-osd: creating OSD 0 on node "minikube-m02"
2024-05-28 13:43:55.693638 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:43:55.693650 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:43:55.693739 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"minikube-m02\""
2024-05-28 13:43:55.772143 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:43:55.772164 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:43:55.851554 I | op-osd: creating OSD 1 on node "minikube-m02"
2024-05-28 13:43:55.851820 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:43:55.851837 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:43:55.851909 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m02\""
2024-05-28 13:43:55.868247 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:43:55.868267 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:43:55.873415 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:43:55.875559 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.876323 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:43:55.876379 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:43:55.876430 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:43:55.876548 D | clusterdisruption-controller: osd "0" POD is not assigned to any node. assuming node drain
2024-05-28 13:43:55.876563 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down and a possible node drain is detected
2024-05-28 13:43:55.876589 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:55.879835 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:43:55.881666 D | ceph-nodedaemon-controller: "rook-ceph-osd-0-66fdb58bd7-9x9dz" is a ceph pod!
2024-05-28 13:43:55.881736 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:55.881995 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:55.889363 I | op-mgr: successful modules: orchestrator modules
2024-05-28 13:43:55.897843 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:43:55.897872 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:55.898222 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:55.898230 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.900150 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-28 13:43:55.900169 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-28 13:43:55.900212 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-28 13:43:55.900217 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m02-status" config map changes
2024-05-28 13:43:55.901196 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m02-status"
2024-05-28 13:43:55.901226 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:43:55.901231 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.919316 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-28 13:43:55.919334 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.922230 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:43:55.922540 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:55.922557 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.922587 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:43:55.922592 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.932273 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:43:55.932309 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-5b8d759dc8-t8l6v" is a ceph pod!
2024-05-28 13:43:55.941434 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:43:55.947755 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-28 13:43:55.947782 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.952481 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:43:55.952704 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:55.952791 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:55.953099 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:55.958874 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:43:55.958897 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:55.965733 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:55.965749 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.977689 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-28 13:43:55.977706 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:55.980471 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:43:55.983758 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:56.132948 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/880069048 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:56.253251 I | clusterdisruption-controller: osd is down in failure domain "minikube-m02". pg health: "cluster has no PGs"
2024-05-28 13:43:56.253293 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:56.549433 D | exec: Running command: ceph osd set-group noout minikube-m02 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:56.695975 I | op-mgr: successfully set ceph dashboard creds
2024-05-28 13:43:56.696052 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:56.839396 E | clusterdisruption-controller: failed to update maintenance noout in cluster "rook-ceph/my-cluster". failed to update flag on crush unit while setting noout.: failed to set flag minikube-m02 on noout: exit status 22
2024-05-28 13:43:56.839474 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:43:56.839505 I | clusterdisruption-controller: deleting the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-28 13:43:56.844627 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:43:56.844681 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:43:56.844720 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:43:56.844886 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-28 13:43:56.844937 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down but no node drain is detected
2024-05-28 13:43:56.844958 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:56.984527 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:57.198356 I | clusterdisruption-controller: osd is down in failure domain "minikube-m02". pg health: "cluster has no PGs"
2024-05-28 13:43:57.198400 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:57.283084 I | op-config: setting "mgr"="mgr/dashboard/ssl"="false" option to the mon configuration database
2024-05-28 13:43:57.283197 D | exec: Running command: ceph config set mgr mgr/dashboard/ssl false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:57.400160 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:43:57.434845 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:43:57.523227 D | exec: Running command: ceph osd set-group noout minikube-m02 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:57.589892 I | op-config: successfully set "mgr"="mgr/dashboard/ssl"="false" option to the mon configuration database
2024-05-28 13:43:57.590012 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:57.817179 E | clusterdisruption-controller: failed to update maintenance noout in cluster "rook-ceph/my-cluster". failed to update flag on crush unit while setting noout.: failed to set flag minikube-m02 on noout: exit status 22
2024-05-28 13:43:57.817222 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:43:57.874963 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:58.172273 I | op-config: setting "mgr"="mgr/dashboard/PROMETHEUS_API_SSL_VERIFY"="false" option to the mon configuration database
2024-05-28 13:43:58.172321 D | exec: Running command: ceph config set mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:58.392324 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:43:58.460018 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:43:58.476329 I | op-config: successfully set "mgr"="mgr/dashboard/PROMETHEUS_API_SSL_VERIFY"="false" option to the mon configuration database
2024-05-28 13:43:58.476369 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:58.682272 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m02-6b965ff7fc-jh9nm" is a ceph pod!
2024-05-28 13:43:58.682365 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:58.682739 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:58.693334 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:43:58.693362 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:58.696983 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:58.697004 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:58.701381 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m02-56bcb7b547-mzztk" is a ceph pod!
2024-05-28 13:43:58.710790 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:43:58.713295 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:58.713310 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:58.717572 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:43:58.717586 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:43:58.717821 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:58.717880 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:43:58.718173 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:43:58.722996 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:43:58.723019 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:43:58.731274 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:43:58.734528 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:43:58.789365 I | op-config: setting "mgr"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2024-05-28 13:43:58.789404 D | exec: Running command: ceph config set mgr mgr/dashboard/server_port 7000 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:59.254957 I | op-config: successfully set "mgr"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2024-05-28 13:43:59.254991 I | op-config: deleting "mgr.a" "mgr/dashboard/url_prefix" option from the mon configuration database
2024-05-28 13:43:59.255010 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:59.392613 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:43:59.392636 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:43:59.392647 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:43:59.424534 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:43:59.514468 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:43:59.599244 I | op-config: successfully deleted "mgr/dashboard/url_prefix" option from the mon configuration database
2024-05-28 13:43:59.599271 I | op-config: deleting "mgr.a" "mgr/dashboard/ssl" option from the mon configuration database
2024-05-28 13:43:59.599289 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:43:59.885317 I | op-config: successfully deleted "mgr/dashboard/ssl" option from the mon configuration database
2024-05-28 13:43:59.885343 I | op-config: deleting "mgr.a" "mgr/dashboard/PROMETHEUS_API_HOST" option from the mon configuration database
2024-05-28 13:43:59.885378 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:00.224025 I | op-config: successfully deleted "mgr/dashboard/PROMETHEUS_API_HOST" option from the mon configuration database
2024-05-28 13:44:00.224051 I | op-config: deleting "mgr.a" "mgr/dashboard/PROMETHEUS_API_SSL_VERIFY" option from the mon configuration database
2024-05-28 13:44:00.224073 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:00.506175 I | op-config: successfully deleted "mgr/dashboard/PROMETHEUS_API_SSL_VERIFY" option from the mon configuration database
2024-05-28 13:44:00.506201 I | op-config: deleting "mgr.a" "mgr/dashboard/server_port" option from the mon configuration database
2024-05-28 13:44:00.506223 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:00.815128 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:44:00.820184 I | op-config: successfully deleted "mgr/dashboard/server_port" option from the mon configuration database
2024-05-28 13:44:00.820214 I | op-config: deleting "mgr.a" "mgr/dashboard/ssl_server_port" option from the mon configuration database
2024-05-28 13:44:00.820240 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:00.835510 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:44:01.113635 I | op-config: successfully deleted "mgr/dashboard/ssl_server_port" option from the mon configuration database
2024-05-28 13:44:01.113659 I | op-mgr: All per-daemon mgr configuration has been deleted successfully.
2024-05-28 13:44:01.113664 I | op-mgr: dashboard config has changed. restarting the dashboard module
2024-05-28 13:44:01.113668 I | op-mgr: restarting the mgr module: dashboard
2024-05-28 13:44:01.113687 D | exec: Running command: ceph mgr module disable dashboard --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:01.644052 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:44:01.688750 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:44:01.707966 D | ceph-spec: object "rook-ceph-exporter-minikube-m02" matched on update
2024-05-28 13:44:01.707989 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:02.064553 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:44:02.064607 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:44:02.292414 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:02.692038 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:44:03.334676 I | op-mgr: successful modules: dashboard
2024-05-28 13:44:03.534062 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-28 13:44:03.546379 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-05-28 13:44:03.563301 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:44:03.563370 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:44:03.563393 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:44:03.563425 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:44:03.618620 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-28 13:44:03.671486 I | ceph-csi: successfully started CSI CephFS driver
2024-05-28 13:44:03.678078 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-28 13:44:03.684201 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-28 13:44:03.684239 I | ceph-csi: CSI NFS driver disabled
2024-05-28 13:44:03.684256 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-28 13:44:03.686663 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-28 13:44:03.686712 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-28 13:44:03.752993 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-28 13:44:03.753011 I | ceph-csi: successfully removed CSI NFS driver
2024-05-28 13:44:07.156600 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:44:07.156671 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:44:09.393429 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:44:09.393501 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:44:09.393538 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:44:13.146506 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:44:13.146573 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:44:14.156496 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:44:14.156563 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:44:14.839005 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:44:14.839192 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:44:14.839311 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:44:14.839855 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-28 13:44:14.840055 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down but no node drain is detected
2024-05-28 13:44:14.840152 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:15.375048 I | clusterdisruption-controller: osd is down in failure domain "minikube-m02". pg health: "cluster has no PGs"
2024-05-28 13:44:15.375150 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:15.832547 D | exec: Running command: ceph osd set-group noout minikube-m02 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:16.412253 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:44:16.415219 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:44:16.415278 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:44:16.415313 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:44:16.415490 I | clusterdisruption-controller: osd "rook-ceph-osd-1" is down but no node drain is detected
2024-05-28 13:44:16.415544 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-28 13:44:16.415568 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:16.465416 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 1
2024-05-28 13:44:16.487342 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-05-28 13:44:16.487369 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:16.487382 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:44:16.502080 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:44:16.502123 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:16.783170 I | clusterdisruption-controller: osd is down in failure domain "minikube-m02". pg health: "cluster has no PGs"
2024-05-28 13:44:16.783209 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:17.094954 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:44:17.103794 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:44:17.103816 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:44:19.394593 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:44:19.394662 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:44:19.394697 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:44:19.406964 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-28 13:44:19.407028 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:44:19.407289 D | op-osd: !!createOSDsForStatusMap: status: &{[] orchestrating false }
2024-05-28 13:44:19.407353 I | op-osd: OSD orchestration status for node minikube-m03 is "orchestrating"
2024-05-28 13:44:29.396018 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:44:29.396089 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-28 13:44:29.396127 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:44:31.250157 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-28 13:44:31.250223 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:44:31.250978 D | op-osd: !!createOSDsForStatusMap: status: &{[{2 ceph 5046a442-9f1f-46a5-b89d-8311c0aa1cd4  nvme /dev/nvme2n1p2   true root=default host=minikube-m03 false raw bluestore  false false  } {0 ceph 651e5d0a-ef0b-41cc-99a1-d0999d81f639  nvme /dev/nvme1n1p2   true root=default host=minikube-m03 false raw bluestore  false false  } {1 ceph 77de06c2-fc87-46ec-98f9-cdc97175dde9  nvme /dev/nvme0n1p2   true root=default host=minikube-m03 false raw bluestore  false false  }] completed false }
2024-05-28 13:44:31.251115 I | op-osd: OSD orchestration status for node minikube-m03 is "completed"
2024-05-28 13:44:31.251146 I | op-osd: creating OSD 2 on node "minikube-m03"
2024-05-28 13:44:31.251965 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:44:31.252010 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:44:31.252221 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 2 on node \"minikube-m03\""
2024-05-28 13:44:31.274309 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:31.274364 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:31.300980 I | op-osd: creating OSD 0 on node "minikube-m03"
2024-05-28 13:44:31.301715 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:44:31.301766 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:44:31.301968 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"minikube-m03\""
2024-05-28 13:44:31.318130 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:31.318186 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:31.332731 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:31.332866 D | ceph-nodedaemon-controller: "rook-ceph-osd-2-598dc65dd6-nlt27" is a ceph pod!
2024-05-28 13:44:31.333012 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:44:31.333914 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:44:31.335950 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-28 13:44:31.335982 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:31.349979 E | op-osd: failed to create OSD 0 on node "minikube-m03": failed to create deployment for OSD 0 on node "minikube-m03": deployments.apps "rook-ceph-osd-0" already exists
2024-05-28 13:44:31.350045 I | op-osd: creating OSD 1 on node "minikube-m03"
2024-05-28 13:44:31.350614 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:44:31.350639 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:44:31.350801 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m03\""
2024-05-28 13:44:31.351225 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "created"
2024-05-28 13:44:31.351279 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:44:31.376642 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:31.389842 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-28 13:44:31.389873 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:31.394883 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:31.394911 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:31.405367 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-28 13:44:31.405393 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:31.407589 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:44:31.407905 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m03-68f6448cbb-78lj9" is a ceph pod!
2024-05-28 13:44:31.416229 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-28 13:44:31.416252 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:31.419465 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:44:31.419540 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:44:31.419962 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:44:31.422229 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-28 13:44:31.422244 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:31.422668 E | op-osd: failed to create OSD 1 on node "minikube-m03": failed to create deployment for OSD 1 on node "minikube-m03": deployments.apps "rook-ceph-osd-1" already exists
2024-05-28 13:44:31.425331 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m03-status" config map changes
2024-05-28 13:44:31.425354 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:44:31.425361 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:44:31.425394 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:44:31.425776 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "failed to create cluster: failed to start ceph osds: 2 failures encountered while running osds on nodes in namespace \"rook-ceph\". \nfailed to create OSD 0 on node \"minikube-m03\": failed to create deployment for OSD 0 on node \"minikube-m03\": deployments.apps \"rook-ceph-osd-0\" already exists\nfailed to create OSD 1 on node \"minikube-m03\": failed to create deployment for OSD 1 on node \"minikube-m03\": deployments.apps \"rook-ceph-osd-1\" already exists"
2024-05-28 13:44:31.427241 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-28 13:44:31.427269 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:44:31.429710 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-28 13:44:31.429729 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:31.435586 E | ceph-cluster-controller: failed to reconcile CephCluster "rook-ceph/my-cluster". failed to reconcile cluster "my-cluster": failed to configure local ceph cluster: failed to create cluster: failed to start ceph osds: 2 failures encountered while running osds on nodes in namespace "rook-ceph". 
failed to create OSD 0 on node "minikube-m03": failed to create deployment for OSD 0 on node "minikube-m03": deployments.apps "rook-ceph-osd-0" already exists
failed to create OSD 1 on node "minikube-m03": failed to create deployment for OSD 1 on node "minikube-m03": deployments.apps "rook-ceph-osd-1" already exists
2024-05-28 13:44:31.436971 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:31.437007 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:31.438676 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:44:31.440840 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2024-05-28 13:44:31.442765 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:44:31.443547 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:44:31.444760 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:44:31.444811 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc001629d10], assignment=&{Schedule:map[a:0xc001f85b40]}
2024-05-28 13:44:31.449953 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2024-05-28 13:44:31.449996 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2024-05-28 13:44:31.450011 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2024-05-28 13:44:31.450027 D | ceph-cluster-controller: cluster spec successfully validated
2024-05-28 13:44:31.450070 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2024-05-28 13:44:31.450103 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-28 13:44:31.450206 D | ceph-cluster-controller: checking health of cluster
2024-05-28 13:44:31.450242 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:44:31.463417 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v18...
2024-05-28 13:44:31.463788 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:31.463809 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:31.465760 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-05-28 13:44:31.861488 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_FLAGS:{Severity:HEALTH_WARN Summary:{Message:1 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set}}]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:16 NumOsd:3 NumUpOsd:2 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56070144 AvailableBytes:7466706825216 TotalBytes:7466762895360 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-28 13:44:31.866038 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:32.268459 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-28 13:44:32.268493 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-28 13:44:32.268640 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_FLAGS:{Severity:HEALTH_WARN Summary:{Message:1 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set}}]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:16 NumOsd:3 NumUpOsd:2 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56070144 AvailableBytes:7466706825216 TotalBytes:7466762895360 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-28 13:44:32.268661 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-28 13:44:32.366969 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:32.367108 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:32.367548 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-05-28 13:44:32.377255 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_FLAGS", message: "1 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set"
2024-05-28 13:44:32.745946 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-05-28 13:44:32.759246 I | ceph-spec: detected ceph image version: "18.2.2-0 reef"
2024-05-28 13:44:32.759304 I | ceph-cluster-controller: validating ceph version from provided image
2024-05-28 13:44:32.760986 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:44:32.763516 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:44:32.763575 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00168c750], assignment=&{Schedule:map[a:0xc001f842c0]}
2024-05-28 13:44:32.767208 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:44:32.767287 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:44:32.767744 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:44:32.767827 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:32.769312 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:44:32.769367 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:44:32.769385 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:44:32.769400 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:44:33.146171 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:33.154386 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-28 13:44:33.154454 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-28 13:44:33.154631 D | ceph-cluster-controller: both cluster and image spec versions are identical, doing nothing 18.2.2-0 reef
2024-05-28 13:44:33.154680 I | ceph-cluster-controller: cluster "rook-ceph": version "18.2.2-0 reef" detected for image "quay.io/ceph/ceph:v18"
2024-05-28 13:44:33.168679 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2024-05-28 13:44:33.180800 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-05-28 13:44:33.180864 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-05-28 13:44:33.180977 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2024-05-28 13:44:33.182278 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:33.182336 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:33.185540 E | ceph-spec: failed to update cluster condition to {Type:Progressing Status:True Reason:ClusterProgressing Message:Configuring Ceph Mons LastHeartbeatTime:2024-05-28 13:44:33.180958825 +0000 UTC m=+144.359119629 LastTransitionTime:2024-05-28 13:44:33.18095846 +0000 UTC m=+144.359119336}. failed to update object "rook-ceph/my-cluster" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "my-cluster": the object has been modified; please apply your changes to the latest version and try again
2024-05-28 13:44:33.185560 D | op-mon: Acquiring lock for mon orchestration
2024-05-28 13:44:33.185565 D | op-mon: Acquired lock for mon orchestration
2024-05-28 13:44:33.185570 I | op-mon: start running mons
2024-05-28 13:44:33.185573 D | op-mon: establishing ceph cluster info
2024-05-28 13:44:33.188159 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:44:33.355791 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:44:33.355925 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00211ebd0], assignment=&{Schedule:map[a:0xc00180d440]}
2024-05-28 13:44:34.159673 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-28 13:44:34.256717 D | ceph-spec: object "rook-ceph-exporter-minikube-m03" matched on update
2024-05-28 13:44:34.256759 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:34.269373 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:34.356558 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.31.173:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.31.173:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-28 13:44:34.556209 D | op-config: updating config secret "rook-ceph-config"
2024-05-28 13:44:34.956294 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:44:34.956369 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:44:34.956701 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:44:34.956750 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:44:35.319998 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:35.556929 D | op-cfg-keyring: updating secret for rook-ceph-mons-keyring
2024-05-28 13:44:35.956687 D | op-cfg-keyring: updating secret for rook-ceph-admin-keyring
2024-05-28 13:44:36.379068 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:36.555276 I | op-mon: targeting the mon count 1
2024-05-28 13:44:36.558277 D | op-mon: Host network for mon "a" is false
2024-05-28 13:44:36.558349 D | op-mon: mon a already scheduled
2024-05-28 13:44:36.558362 D | op-mon: mons have been scheduled
2024-05-28 13:44:36.562785 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-05-28 13:44:36.562820 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3434685963 -o /var/lib/rook/3434685963.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:37.041768 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:44:37.042333 I | op-config: applying ceph settings:
[global]
log to file = false
2024-05-28 13:44:37.042413 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/2771122120 -o /var/lib/rook/2771122120.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:37.446044 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:37.543258 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:44:37.543413 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-05-28 13:44:37.543493 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:38.009708 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-05-28 13:44:38.009771 I | op-mon: checking for basic quorum with existing mons
2024-05-28 13:44:38.014983 D | op-k8sutil: creating service rook-ceph-mon-a
2024-05-28 13:44:38.034959 D | op-k8sutil: updating service rook-ceph-mon-a
2024-05-28 13:44:38.044970 I | op-mon: mon "a" cluster IP is 10.105.31.173
2024-05-28 13:44:38.055823 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-28 13:44:38.059477 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.31.173:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.31.173:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-28 13:44:38.155886 D | op-config: updating config secret "rook-ceph-config"
2024-05-28 13:44:38.513178 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:38.556178 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:44:38.556237 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:44:38.556493 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:44:38.556530 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:44:38.965422 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.105.31.173 Port:6789 Zone: NodeName:minikube-m02 DataPathMap:0xc002018cf0 UseHostNetwork:false}
2024-05-28 13:44:38.976204 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-05-28 13:44:38.976269 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2024-05-28 13:44:38.997717 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2024-05-28 13:44:38.997771 I | op-mon: waiting for mon quorum with [a]
2024-05-28 13:44:39.162916 I | op-mon: mons running: [a]
2024-05-28 13:44:39.163004 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:39.396860 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:44:39.396891 D | ceph-spec: "ceph-block-pool-controller": ceph status is "HEALTH_WARN", operator is ready to run ceph command, reconciling
2024-05-28 13:44:39.400974 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:44:39.559015 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:44:39.559162 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0014d0570], assignment=&{Schedule:map[a:0xc000a7f5c0]}
2024-05-28 13:44:39.559287 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:44:39.559458 I | ceph-block-pool-controller: creating pool ".mgr" in namespace "rook-ceph"
2024-05-28 13:44:39.559567 D | exec: Running command: ceph osd crush rule create-replicated .mgr default host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:39.622615 I | op-mon: Monitors in quorum: [a]
2024-05-28 13:44:39.622676 I | op-mon: mons created: 1
2024-05-28 13:44:39.622727 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:40.020060 D | exec: Running command: ceph osd pool get .mgr all --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:40.086321 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-28 13:44:40.086400 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":4}}
2024-05-28 13:44:40.086532 I | op-mon: waiting for mon quorum with [a]
2024-05-28 13:44:40.098833 I | op-mon: mons running: [a]
2024-05-28 13:44:40.098916 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:40.450227 D | exec: Running command: ceph osd pool application get .mgr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:40.563134 I | op-mon: Monitors in quorum: [a]
2024-05-28 13:44:40.563202 I | ceph-spec: not applying network settings for cluster "rook-ceph" ceph networks
2024-05-28 13:44:40.563225 D | op-mon: mon endpoints used are: a=10.105.31.173:6789
2024-05-28 13:44:40.563249 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-05-28 13:44:40.563262 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-05-28 13:44:40.563275 D | op-mon: Released lock for mon orchestration
2024-05-28 13:44:40.563288 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-05-28 13:44:40.563307 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-05-28 13:44:40.563361 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:40.913683 I | cephclient: application "mgr" is already set on pool ".mgr"
2024-05-28 13:44:40.913741 I | cephclient: reconciling replicated pool .mgr succeeded
2024-05-28 13:44:40.913759 D | cephclient: skipping check for failure domain and deviceClass on pool ".mgr" as it is not specified
2024-05-28 13:44:40.913776 I | ceph-block-pool-controller: initializing pool ".mgr" for RBD use
2024-05-28 13:44:40.913824 D | exec: Running command: rbd pool init .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:44:40.956493 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-05-28 13:44:40.956597 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:40.987115 I | ceph-block-pool-controller: successfully initialized pool ".mgr" for RBD use
2024-05-28 13:44:40.987174 D | ceph-block-pool-controller: configuring RBD per-image IO statistics collection
2024-05-28 13:44:40.987353 D | exec: Running command: ceph config get mgr mgr/prometheus/rbd_stats_pools --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:41.332165 D | ceph-block-pool-controller: RBD per-image IO statistics will be collected for pools: []
2024-05-28 13:44:41.332253 I | op-config: setting "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-28 13:44:41.332316 D | exec: Running command: ceph config set mgr mgr/prometheus/rbd_stats_pools  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:41.376773 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-05-28 13:44:41.376890 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:41.673847 I | op-config: successfully set "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-28 13:44:41.673898 D | ceph-block-pool-controller: configured RBD per-image IO statistics collection
2024-05-28 13:44:41.673914 D | ceph-block-pool-controller: reconciling create rbd mirror peer configuration
2024-05-28 13:44:41.673927 D | cephclient: retrieving mirroring pool ".mgr" info
2024-05-28 13:44:41.673966 D | exec: Running command: rbd mirror pool info .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:41.747587 I | cephclient: disabling mirroring for pool ".mgr"
2024-05-28 13:44:41.747718 D | exec: Running command: rbd mirror pool disable .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:44:41.780153 I | ceph-block-pool-controller: successfully disabled mirroring on the pool ".mgr"
2024-05-28 13:44:41.787569 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Ready"
2024-05-28 13:44:41.787635 D | ceph-block-pool-controller: done reconciling
2024-05-28 13:44:41.787661 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:44:41.788272 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-28 13:44:41.788313 D | ceph-spec: update event on CephBlockPool CR
2024-05-28 13:44:41.788374 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:44:41.788622 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:44:41.788830 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:44:41.789460 I | clusterdisruption-controller: osd "rook-ceph-osd-2" is down but no node drain is detected
2024-05-28 13:44:41.789638 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:41.998414 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-05-28 13:44:41.998454 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:42.148485 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:44:42.148554 I | clusterdisruption-controller: osd is down in failure domain "minikube-m03". pg health: "all PGs in cluster are clean"
2024-05-28 13:44:42.148585 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:42.378679 D | op-cfg-keyring: updating secret for rook-csi-rbd-provisioner
2024-05-28 13:44:42.385978 D | op-cfg-keyring: updating secret for rook-csi-rbd-node
2024-05-28 13:44:42.392355 D | op-cfg-keyring: updating secret for rook-csi-cephfs-provisioner
2024-05-28 13:44:42.399036 D | op-cfg-keyring: updating secret for rook-csi-cephfs-node
2024-05-28 13:44:42.401490 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2024-05-28 13:44:42.401508 I | cephclient: getting or creating ceph auth key "client.crash"
2024-05-28 13:44:42.401524 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:42.434213 D | exec: Running command: ceph osd unset-group noout minikube-m02 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:42.870050 D | op-cfg-keyring: updating secret for rook-ceph-crash-collector-keyring
2024-05-28 13:44:42.874430 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2024-05-28 13:44:42.874449 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-05-28 13:44:42.874463 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:43.250035 D | exec: Running command: ceph osd set-group noout minikube-m03 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:43.492121 D | op-cfg-keyring: updating secret for rook-ceph-exporter-keyring
2024-05-28 13:44:43.498134 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2024-05-28 13:44:43.498199 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-05-28 13:44:43.498246 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:43.888206 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-05-28 13:44:43.888272 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-05-28 13:44:43.888325 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:44.261199 I | clusterdisruption-controller: creating temporary blocking pdb "rook-ceph-osd-host-minikube-m02" with maxUnavailable=0 for "host" failure domain "minikube-m02"
2024-05-28 13:44:44.267262 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:44:44.280342 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-05-28 13:44:44.280404 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-05-28 13:44:44.280489 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:44.765457 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-05-28 13:44:44.765522 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-05-28 13:44:44.765576 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:45.251352 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-05-28 13:44:45.251419 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-05-28 13:44:45.251473 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:45.728238 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-05-28 13:44:45.728872 I | op-config: applying ceph settings:
[global]
mon_data_avail_warn            = 10
mon_warn_on_pool_no_redundancy = false
osd_pool_default_size          = 1
bdev_flock_retry               = 20
bluefs_buffered_io             = false
2024-05-28 13:44:45.728949 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/1706140511 -o /var/lib/rook/1706140511.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:46.163310 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:44:46.163477 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-05-28 13:44:46.163517 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-05-28 13:44:46.163568 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:46.416289 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:44:46.416391 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:44:46.416445 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:44:46.416735 I | clusterdisruption-controller: osd "rook-ceph-osd-2" is down but no node drain is detected
2024-05-28 13:44:46.416775 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:46.705126 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "my-cluster"
2024-05-28 13:44:46.705324 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-my-cluster" in namespace "rook-ceph"
2024-05-28 13:44:46.705359 D | op-k8sutil: creating secret cluster-peer-token-my-cluster
2024-05-28 13:44:46.716796 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-05-28 13:44:46.736721 I | op-mgr: start running mgr
2024-05-28 13:44:46.736977 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:46.736996 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:46.742976 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-05-28 13:44:46.743063 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:46.806763 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:44:46.806790 I | clusterdisruption-controller: osd is down in failure domain "minikube-m03". pg health: "all PGs in cluster are clean"
2024-05-28 13:44:46.806813 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:47.166802 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-05-28 13:44:47.169931 D | op-cfg-keyring: updating secret for rook-ceph-mgr-a-keyring
2024-05-28 13:44:47.174134 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc001a5d020}
2024-05-28 13:44:47.182363 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:44:47.186518 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:44:47.186674 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:44:47.186802 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:44:47.187337 I | clusterdisruption-controller: osd "rook-ceph-osd-2" is down but no node drain is detected
2024-05-28 13:44:47.187378 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2024-05-28 13:44:47.187439 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:47.194925 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2024-05-28 13:44:47.201453 D | op-mgr: expected number 1 of mgrs found
2024-05-28 13:44:47.201486 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-05-28 13:44:47.217841 D | op-k8sutil: updating service rook-ceph-mgr-dashboard
2024-05-28 13:44:47.226168 D | op-k8sutil: creating service rook-ceph-mgr
2024-05-28 13:44:47.242136 D | op-k8sutil: updating service rook-ceph-mgr
2024-05-28 13:44:47.253936 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2024-05-28 13:44:47.254079 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:47.254131 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-05-28 13:44:47.254281 I | op-mgr: successful modules: balancer
2024-05-28 13:44:47.254304 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:47.254610 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:47.261490 I | op-osd: start running osds in namespace "rook-ceph"
2024-05-28 13:44:47.261508 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-05-28 13:44:47.266000 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:47.266087 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:47.366129 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:47.611267 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:44:47.611294 I | clusterdisruption-controller: osd is down in failure domain "minikube-m03". pg health: "all PGs in cluster are clean"
2024-05-28 13:44:47.611314 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:47.759341 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:44:47.759369 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:44:47.833472 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:44:47.833523 I | op-osd: placement group status: "all PGs in cluster are clean"
2024-05-28 13:44:47.851015 I | op-osd: all osds have already been migrated to backend store "bluestore"
2024-05-28 13:44:47.851060 I | op-osd: no osd to replace
2024-05-28 13:44:47.854193 D | op-osd: !!osds to skip reconcilemap[]
2024-05-28 13:44:47.872144 D | op-osd: 3 of 3 OSD Deployments need update
2024-05-28 13:44:47.872184 D | op-osd: !!updateConfig: provisionConfig: &{/var/lib/rook  /var/lib/rook/rook-ceph}, numUpdatesNeeded: 3, deployments: &{map[0:true 1:true 2:true]}, osdsToSkipReconcile: map[]
2024-05-28 13:44:47.872230 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-05-28 13:44:47.874344 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-05-28 13:44:47.874383 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-05-28 13:44:47.960200 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:44:47.969816 I | op-osd: 2 of the 2 storage nodes are valid
2024-05-28 13:44:48.298118 I | op-mgr: successful modules: mgr module(s) from the spec
2024-05-28 13:44:48.298264 D | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:48.299466 I | op-mgr: successful modules: prometheus
2024-05-28 13:44:48.377996 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m02 to start a new one
2024-05-28 13:44:48.392694 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 still exists
2024-05-28 13:44:48.417058 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-z99rr" is a ceph pod!
2024-05-28 13:44:48.417260 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:44:48.418837 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:44:48.438398 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:44:48.438469 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:44:48.593974 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:44:48.970654 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:44:49.304049 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:49.742768 I | op-mgr: successful modules: orchestrator modules
2024-05-28 13:44:51.404303 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 deleted
2024-05-28 13:44:51.418481 I | op-osd: started OSD provisioning job for node "minikube-m02"
2024-05-28 13:44:51.434747 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-wnrwm" is a ceph pod!
2024-05-28 13:44:51.434929 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:44:51.435980 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:44:51.438198 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m03 to start a new one
2024-05-28 13:44:51.453921 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:44:51.453998 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:44:51.460881 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 still exists
2024-05-28 13:44:51.477941 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-hhtrp" is a ceph pod!
2024-05-28 13:44:51.479737 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:44:51.489913 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:44:51.490145 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:44:51.491043 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:44:51.510537 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-28 13:44:51.510613 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:44:51.531179 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:44:51.540401 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:44:51.981204 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:52.000529 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 2
2024-05-28 13:44:52.031822 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2024-05-28 13:44:52.031851 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:44:53.141504 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-28 13:44:53.141565 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-28 13:44:53.302701 I | op-mgr: the dashboard secret was already generated
2024-05-28 13:44:53.302748 I | op-mgr: setting ceph dashboard "admin" login creds
2024-05-28 13:44:53.303029 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/3365751376 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:53.766183 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/3365751376 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:54.464952 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 deleted
2024-05-28 13:44:54.474655 I | op-osd: started OSD provisioning job for node "minikube-m03"
2024-05-28 13:44:54.474719 D | op-osd: !!statusConfigMapsmap[rook-ceph-osd-minikube-m02-status:{} rook-ceph-osd-minikube-m03-status:{}]
2024-05-28 13:44:54.474759 D | op-osd: !!createConfig: provisionConfig: &{/var/lib/rook  /var/lib/rook/rook-ceph}, awaitingStatusConfigMaps: map[rook-ceph-osd-minikube-m02-status:{} rook-ceph-osd-minikube-m03-status:{}], deployments: &{map[0:true 1:true 2:true]}, finishedStatusConfigMaps: map[], deployments: &{map[0:true 1:true 2:true]}
2024-05-28 13:44:54.482004 D | op-osd: !!createOSDsForStatusMap: status: &{[] orchestrating false }
2024-05-28 13:44:54.482083 I | op-osd: OSD orchestration status for node minikube-m02 is "orchestrating"
2024-05-28 13:44:54.482118 D | op-osd: !!createOSDsForStatusMap: status: &{[] starting false }
2024-05-28 13:44:54.482143 I | op-osd: OSD orchestration status for node minikube-m03 is "starting"
2024-05-28 13:44:54.486667 I | op-mgr: successfully set ceph dashboard creds
2024-05-28 13:44:54.486787 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-wh979" is a ceph pod!
2024-05-28 13:44:54.486903 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:54.487007 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:44:54.488163 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:44:54.506573 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-28 13:44:54.506600 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:44:54.532434 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:44:54.542081 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:44:54.584247 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:54.881163 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:54.958667 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:55.210643 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:55.299091 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:55.522198 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:55.579863 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:55.852294 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:55.925154 D | op-osd: updating OSDs: [0 1]
2024-05-28 13:44:55.940795 I | op-osd: CRUSH location=root=default host=minikube-m02
2024-05-28 13:44:55.940860 I | op-osd: updating OSD 0 on node "minikube-m02"
2024-05-28 13:44:55.941541 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:44:55.941585 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:44:55.941805 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"minikube-m02\""
2024-05-28 13:44:55.951713 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:55.951740 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:55.968660 I | op-osd: CRUSH location=root=default host=minikube-m02
2024-05-28 13:44:55.968698 I | op-osd: updating OSD 1 on node "minikube-m02"
2024-05-28 13:44:55.969148 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:44:55.969195 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:44:55.969841 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m02\""
2024-05-28 13:44:55.980506 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:44:55.980523 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:44:55.989171 D | op-k8sutil: deployment "rook-ceph-osd-0" did not change. nothing to update
2024-05-28 13:44:55.999424 D | op-k8sutil: deployment "rook-ceph-osd-1" did not change. nothing to update
2024-05-28 13:44:56.039344 D | op-osd: !!createOSDsForStatusMap: status: &{[] orchestrating false }
2024-05-28 13:44:56.039374 I | op-osd: OSD orchestration status for node minikube-m03 is "orchestrating"
2024-05-28 13:44:56.039397 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-28 13:44:56.039403 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:44:56.140373 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:56.205877 I | op-mgr: successful modules: dashboard
2024-05-28 13:44:56.426076 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:56.731151 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:57.023585 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:57.393019 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:44:57.493254 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:57.838629 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:57.896686 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" matched on update
2024-05-28 13:44:57.896734 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m02-status"
2024-05-28 13:44:58.183359 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:58.619269 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:59.021481 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:44:59.021594 D | op-osd: !!createOSDsForStatusMap: status: &{[{2 ceph 5046a442-9f1f-46a5-b89d-8311c0aa1cd4  nvme /dev/nvme2n1p2   true root=default host=minikube-m02 false raw bluestore  false false  } {0 ceph 651e5d0a-ef0b-41cc-99a1-d0999d81f639  nvme /dev/nvme1n1p2   true root=default host=minikube-m02 false raw bluestore  false false  } {1 ceph 77de06c2-fc87-46ec-98f9-cdc97175dde9  nvme /dev/nvme0n1p2   true root=default host=minikube-m02 false raw bluestore  false false  }] completed false }
2024-05-28 13:44:59.021625 I | op-osd: OSD orchestration status for node minikube-m02 is "completed"
2024-05-28 13:44:59.021633 D | op-osd: not creating deployment for OSD 2 which already exists
2024-05-28 13:44:59.021637 D | op-osd: not creating deployment for OSD 0 which already exists
2024-05-28 13:44:59.021640 D | op-osd: not creating deployment for OSD 1 which already exists
2024-05-28 13:44:59.025391 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-28 13:44:59.025413 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m02-status" config map changes
2024-05-28 13:44:59.025431 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-28 13:44:59.025443 D | ceph-spec: object "rook-ceph-osd-minikube-m02-status" did not match on delete
2024-05-28 13:44:59.025742 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m02-status"
2024-05-28 13:44:59.126913 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:59.485378 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:44:59.820187 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:00.247803 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:00.631711 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:00.732730 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:00.949256 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-28 13:45:00.949311 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:45:01.100564 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:01.505354 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:01.923146 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:02.343434 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:02.343610 D | op-osd: !!createOSDsForStatusMap: status: &{[{2 ceph 5046a442-9f1f-46a5-b89d-8311c0aa1cd4  nvme /dev/nvme2n1p2   true root=default host=minikube-m03 false raw bluestore  false false  } {0 ceph 651e5d0a-ef0b-41cc-99a1-d0999d81f639  nvme /dev/nvme1n1p2   true root=default host=minikube-m03 false raw bluestore  false false  } {1 ceph 77de06c2-fc87-46ec-98f9-cdc97175dde9  nvme /dev/nvme0n1p2   true root=default host=minikube-m03 false raw bluestore  false false  }] completed false }
2024-05-28 13:45:02.343667 I | op-osd: OSD orchestration status for node minikube-m03 is "completed"
2024-05-28 13:45:02.343679 D | op-osd: not creating deployment for OSD 2 which already exists
2024-05-28 13:45:02.343685 D | op-osd: not creating deployment for OSD 0 which already exists
2024-05-28 13:45:02.343690 D | op-osd: not creating deployment for OSD 1 which already exists
2024-05-28 13:45:02.347534 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:45:02.347563 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:45:02.347577 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:45:02.347591 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m03-status" config map changes
2024-05-28 13:45:02.348101 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:45:02.448586 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:02.839766 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:03.299973 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:03.754647 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:04.257057 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:04.357623 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:04.745809 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:05.228102 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:05.712167 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:06.188841 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:06.289232 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:06.679947 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:07.122249 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:07.458903 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:07.939833 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:08.040155 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:08.519439 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:08.969082 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:09.379940 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:09.858183 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:09.958714 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:10.445844 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:10.914115 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:11.309856 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:11.688530 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:11.788880 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:12.190323 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:12.625840 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:13.072488 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:13.559949 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:13.660265 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:14.128680 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:14.498384 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:15.004899 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:15.479747 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:15.579986 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:16.076994 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:16.450778 D | op-mon: checking health of mons
2024-05-28 13:45:16.450838 D | op-mon: Acquiring lock for mon orchestration
2024-05-28 13:45:16.450844 D | op-mon: Acquired lock for mon orchestration
2024-05-28 13:45:16.455556 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-28 13:45:16.455624 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:16.497345 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:16.896142 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.105.31.173:6789/0 PublicAddr:10.105.31.173:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.105.31.173:3300 Nonce:0} {Type:v1 Addr:10.105.31.173:6789 Nonce:0}]}}]}}
2024-05-28 13:45:16.896205 D | op-mon: targeting the mon count 1
2024-05-28 13:45:16.896231 D | op-mon: mon "a" found in quorum
2024-05-28 13:45:16.896246 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-28 13:45:16.902514 D | op-mon: skipping check for multiple mons on same node since multiple mons are allowed
2024-05-28 13:45:16.902586 D | op-mon: Released lock for mon orchestration
2024-05-28 13:45:16.902627 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-28 13:45:16.931392 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:17.187698 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:45:17.188060 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:45:17.188190 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:45:17.188670 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:17.280006 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:17.380141 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:17.757199 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:45:17.757270 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-28 13:45:17.757339 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:17.987922 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:18.229038 D | exec: Running command: ceph osd unset-group noout minikube-m03 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:18.441299 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:18.770759 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:19.049298 I | clusterdisruption-controller: all PGs are active+clean. Restoring default OSD pdb settings
2024-05-28 13:45:19.049363 I | clusterdisruption-controller: creating the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-28 13:45:19.056752 I | clusterdisruption-controller: deleting temporary blocking pdb with "rook-ceph-osd-host-minikube-m02" with maxUnavailable=0 for "host" failure domain "minikube-m02"
2024-05-28 13:45:19.062303 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-28 13:45:19.062376 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-28 13:45:19.066540 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:45:19.066601 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:45:19.066643 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:45:19.066825 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:19.231550 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:19.333094 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:19.552684 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:45:19.552758 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-28 13:45:19.552869 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-28 13:45:19.552920 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-28 13:45:19.720163 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:20.158079 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:20.666288 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:21.139438 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:21.240671 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:21.694613 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:22.054162 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:22.546068 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:22.928874 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:23.029293 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:23.524206 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:23.974719 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:24.431827 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:24.884384 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:24.985657 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:25.476145 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:25.870348 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:26.312268 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:26.670037 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:26.770290 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:27.181493 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:27.563287 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:28.019296 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:28.467894 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:28.568990 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:29.036356 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:29.493245 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:29.954760 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:30.433249 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:30.533365 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:30.985861 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:31.450060 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:31.450367 D | op-osd: checking osd processes status.
2024-05-28 13:45:31.450507 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:31.881809 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:31.888213 D | op-osd: validating status of osd.0
2024-05-28 13:45:31.888232 D | op-osd: osd.0 is healthy.
2024-05-28 13:45:31.888236 D | op-osd: validating status of osd.1
2024-05-28 13:45:31.888239 D | op-osd: osd.1 is healthy.
2024-05-28 13:45:31.888243 D | op-osd: validating status of osd.2
2024-05-28 13:45:31.888246 D | op-osd: osd.2 is healthy.
2024-05-28 13:45:32.322973 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:32.378218 D | ceph-cluster-controller: checking health of cluster
2024-05-28 13:45:32.378260 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:45:32.423748 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:32.814566 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:32.899932 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:25 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88223744 AvailableBytes:11200056119296 TotalBytes:11200144343040 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-28 13:45:32.906886 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:33.215376 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:33.425066 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:45:33.425132 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:45:33.425377 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:25 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88223744 AvailableBytes:11200056119296 TotalBytes:11200144343040 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-28 13:45:33.425438 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-28 13:45:33.448833 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:45:33.448879 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:45:33.647032 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:34.107143 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:34.207732 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:34.697254 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:35.190931 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:35.639090 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:36.069577 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:36.170544 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:36.564400 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:36.998715 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:37.329853 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:37.720083 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:37.820522 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:38.282423 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:38.759948 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:39.201990 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:39.675517 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:39.776022 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:40.261483 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:40.735035 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:41.121784 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:41.546925 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:41.648082 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:42.121680 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:42.657184 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:43.161757 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:43.639868 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:43.741157 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:44.202507 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:44.666251 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:45.095935 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:45.582401 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:45.683255 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:46.164046 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:46.601725 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:46.993914 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:47.423942 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:47.524308 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:47.936399 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:48.370804 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:48.859344 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:49.225942 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:49.326205 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:49.759544 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:50.252927 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:50.711459 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:51.114408 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:51.215269 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:51.700472 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:52.085914 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:52.554501 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:52.986326 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:53.086693 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:53.556975 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:53.991551 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:54.474323 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:54.920782 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:54.920857 I | op-osd: waiting... 2 of 2 OSD prepare jobs have finished processing and 2 of 3 OSDs have been updated
2024-05-28 13:45:55.020918 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:55.494359 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:55.968730 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:56.513030 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:56.873784 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:56.974845 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:57.408407 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:57.808902 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:58.311854 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:58.777004 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:45:58.878077 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:59.283773 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:45:59.719139 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:00.130084 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:00.611935 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:00.712389 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:01.102749 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:01.558863 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:01.903637 D | op-mon: checking health of mons
2024-05-28 13:46:01.903666 D | op-mon: Acquiring lock for mon orchestration
2024-05-28 13:46:01.903670 D | op-mon: Acquired lock for mon orchestration
2024-05-28 13:46:01.910020 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-28 13:46:01.910107 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:01.927574 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:02.346319 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:02.375418 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.105.31.173:6789/0 PublicAddr:10.105.31.173:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.105.31.173:3300 Nonce:0} {Type:v1 Addr:10.105.31.173:6789 Nonce:0}]}}]}}
2024-05-28 13:46:02.375503 D | op-mon: targeting the mon count 1
2024-05-28 13:46:02.375684 D | op-mon: mon "a" found in quorum
2024-05-28 13:46:02.375731 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-28 13:46:02.381976 D | op-mon: Released lock for mon orchestration
2024-05-28 13:46:02.382040 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-28 13:46:02.447120 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:02.844804 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:03.190200 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:03.551198 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:04.007457 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:04.107686 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:04.600681 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:05.082229 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:05.546440 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:06.005963 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:06.106227 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:06.569527 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:07.034780 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:07.580786 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:08.050855 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:08.152043 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:08.655089 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:09.067074 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:09.518076 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:10.003215 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:10.103971 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:10.563231 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:10.994230 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:11.467545 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:12.003327 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:12.103570 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:12.580319 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:13.033410 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:13.442759 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:13.845383 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:13.945752 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:14.321075 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:14.805322 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:15.272133 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:15.709826 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:15.811102 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:16.296822 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:16.650465 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:17.002530 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:17.430527 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:17.531014 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:17.972451 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:18.500907 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:18.971935 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:19.405711 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:19.506892 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:19.930946 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:20.417789 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:20.894142 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:21.372715 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:21.473961 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:21.887932 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:22.336843 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:22.810550 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:23.219686 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:23.319886 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:23.786001 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:24.200469 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:24.674764 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:25.174043 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:25.274296 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:25.687284 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:26.153285 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:26.480337 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:26.882127 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:26.982332 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:27.479092 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:27.888443 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:28.274674 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:28.742615 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:28.842941 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:28.943096 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:46:28.943148 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:46:29.227156 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:29.644120 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:30.121862 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:30.582697 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:30.683513 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:31.160552 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:31.652763 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:31.889033 D | op-osd: checking osd processes status.
2024-05-28 13:46:31.889168 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:32.085964 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:32.281378 D | op-osd: validating status of osd.0
2024-05-28 13:46:32.281402 D | op-osd: osd.0 is healthy.
2024-05-28 13:46:32.281406 D | op-osd: validating status of osd.1
2024-05-28 13:46:32.281409 D | op-osd: osd.1 is healthy.
2024-05-28 13:46:32.281412 D | op-osd: validating status of osd.2
2024-05-28 13:46:32.281416 D | op-osd: osd.2 is healthy.
2024-05-28 13:46:32.400956 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:32.501206 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:32.971395 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:33.415177 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:33.444627 D | ceph-cluster-controller: checking health of cluster
2024-05-28 13:46:33.444753 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:46:33.900904 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:33.982371 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:25 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88223744 AvailableBytes:11200056119296 TotalBytes:11200144343040 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-28 13:46:33.992098 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:34.324834 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:34.425224 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:34.444021 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:46:34.444085 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:46:34.444342 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:25 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88223744 AvailableBytes:11200056119296 TotalBytes:11200144343040 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-28 13:46:34.444392 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-28 13:46:34.469801 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:34.469852 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:34.881223 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:35.370299 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:35.831961 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:36.267364 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:36.367760 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:36.798812 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:37.194932 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:37.660293 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:38.118598 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:38.219439 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:38.690709 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:39.167970 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:39.564782 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:40.052330 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:40.153421 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:40.627162 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:41.121752 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:41.578615 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:41.990223 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:42.091457 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:42.590955 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:43.074938 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:43.573618 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:43.981949 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:44.082167 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:44.584346 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:45.060733 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:45.453102 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:45.918417 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:46.018808 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:46.489483 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:46.952355 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:47.379447 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:47.383068 D | op-mon: checking health of mons
2024-05-28 13:46:47.383094 D | op-mon: Acquiring lock for mon orchestration
2024-05-28 13:46:47.383098 D | op-mon: Acquired lock for mon orchestration
2024-05-28 13:46:47.386591 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-28 13:46:47.386635 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:47.734371 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:47.834766 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:47.870901 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.105.31.173:6789/0 PublicAddr:10.105.31.173:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.105.31.173:3300 Nonce:0} {Type:v1 Addr:10.105.31.173:6789 Nonce:0}]}}]}}
2024-05-28 13:46:47.870925 D | op-mon: targeting the mon count 1
2024-05-28 13:46:47.870934 D | op-mon: mon "a" found in quorum
2024-05-28 13:46:47.870938 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-28 13:46:47.876074 D | op-mon: Released lock for mon orchestration
2024-05-28 13:46:47.876097 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-28 13:46:47.956377 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:47.956463 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:47.956688 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:46:47.956842 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:46:47.956965 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:46:47.957368 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:47.958793 I | ceph-cluster-controller: CR has changed for "my-cluster". diff=  v1.ClusterSpec{
  	CephVersion: {Image: "quay.io/ceph/ceph:v18", AllowUnsupported: true},
  	Storage: v1.StorageScopeSpec{
  		Nodes: []v1.Node{
  			{
  				Name:      "minikube-m02",
  				Resources: {},
  				Config:    nil,
  				Selection: v1.Selection{
  					UseAllDevices:    nil,
  					DeviceFilter:     "",
  					DevicePathFilter: "",
  					Devices: []v1.Device{
  						{Name: "/dev/nvme0n1p2"},
- 						{Name: "/dev/nvme1n1p2"},
  					},
  					VolumeClaimTemplates: nil,
  				},
  			},
  			{
  				Name:      "minikube-m03",
  				Resources: {},
  				Config:    nil,
  				Selection: v1.Selection{
  					UseAllDevices:    nil,
  					DeviceFilter:     "",
  					DevicePathFilter: "",
  					Devices: []v1.Device{
+ 						{Name: "/dev/nvme1n1p2"},
  						{Name: "/dev/nvme2n1p2"},
  					},
  					VolumeClaimTemplates: nil,
  				},
  			},
  		},
  		UseAllNodes:           false,
  		OnlyApplyOSDPlacement: false,
  		... // 5 identical fields
  	},
  	Annotations: nil,
  	Labels:      nil,
  	... // 23 identical fields
  }
2024-05-28 13:46:47.958970 I | operator: reloading operator's CRDs manager, cancelling all orchestrations!
I0528 13:46:47.959182       1 manager.go:148] "msg"="stopping provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket" "reason"="context canceled"
2024-05-28 13:46:47.959194 I | ceph-cluster-controller: stopping monitoring of ceph status
2024-05-28 13:46:47.959286 I | op-mon: stopping monitoring of mons in namespace "rook-ceph"
2024-05-28 13:46:47.959332 I | op-osd: stopping monitoring of OSDs in namespace "rook-ceph"
2024-05-28 13:46:47.964111 I | operator: watching all namespaces for Ceph CRs
2024-05-28 13:46:47.964248 I | operator: setting up schemes
2024-05-28 13:46:47.973381 I | operator: setting up the controller-runtime manager
2024-05-28 13:46:47.974364 I | ceph-cluster-controller: successfully started
2024-05-28 13:46:47.979346 I | ceph-cluster-controller: enabling hotplug orchestration
2024-05-28 13:46:47.979445 I | ceph-nodedaemon-controller: successfully started
2024-05-28 13:46:47.979474 D | ceph-nodedaemon-controller: watch for changes to the nodes
2024-05-28 13:46:47.979502 D | ceph-nodedaemon-controller: watch for changes to the ceph-crash deployments
2024-05-28 13:46:47.979537 D | ceph-nodedaemon-controller: watch for changes to the ceph pods and enqueue their nodes
2024-05-28 13:46:47.979576 I | ceph-block-pool-controller: successfully started
2024-05-28 13:46:47.979697 I | ceph-object-store-user-controller: successfully started
2024-05-28 13:46:47.979758 I | ceph-object-realm-controller: successfully started
2024-05-28 13:46:47.979803 I | ceph-object-zonegroup-controller: successfully started
2024-05-28 13:46:47.979892 I | ceph-object-zone-controller: successfully started
2024-05-28 13:46:47.980201 I | ceph-object-controller: successfully started
2024-05-28 13:46:47.980287 I | ceph-file-controller: successfully started
2024-05-28 13:46:47.980351 I | ceph-nfs-controller: successfully started
2024-05-28 13:46:47.980445 I | ceph-rbd-mirror-controller: successfully started
2024-05-28 13:46:47.980515 I | ceph-client-controller: successfully started
2024-05-28 13:46:47.980567 I | ceph-filesystem-mirror-controller: successfully started
2024-05-28 13:46:47.980706 I | operator: rook-ceph-operator-config-controller successfully started
2024-05-28 13:46:47.980755 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2024-05-28 13:46:47.981155 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2024-05-28 13:46:47.981237 I | ceph-bucket-topic: successfully started
2024-05-28 13:46:47.981298 I | ceph-bucket-notification: successfully started
2024-05-28 13:46:47.981403 I | ceph-bucket-notification: successfully started
2024-05-28 13:46:47.981487 I | ceph-fs-subvolumegroup-controller: successfully started
2024-05-28 13:46:47.981582 I | blockpool-rados-namespace-controller: successfully started
2024-05-28 13:46:47.986339 I | ceph-cosi-controller: successfully started
2024-05-28 13:46:47.986639 I | nvmeofstorage-controller: successfully started
2024-05-28 13:46:47.986721 I | operator: starting the controller-runtime manager
2024-05-28 13:46:48.027122 D | clusterdisruption-controller: create event from ceph cluster CR
2024-05-28 13:46:48.035799 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-28 13:46:48.035825 D | ceph-spec: update event on CephBlockPool CR
2024-05-28 13:46:48.035981 D | ceph-spec: skipping resource "builtin-mgr" update with unchanged spec
2024-05-28 13:46:48.092622 D | ceph-nodedaemon-controller: "rook-ceph-osd-0-66fdb58bd7-9x9dz" is a ceph pod!
2024-05-28 13:46:48.092693 D | ceph-nodedaemon-controller: "rook-ceph-osd-2-598dc65dd6-nlt27" is a ceph pod!
2024-05-28 13:46:48.092713 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-wnrwm" is a ceph pod!
2024-05-28 13:46:48.092746 D | ceph-nodedaemon-controller: "rook-ceph-mgr-a-7c9d496659-ncb9h" is a ceph pod!
2024-05-28 13:46:48.092760 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m02-56bcb7b547-mzztk" is a ceph pod!
2024-05-28 13:46:48.092770 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-m03-68f6448cbb-78lj9" is a ceph pod!
2024-05-28 13:46:48.092781 D | ceph-nodedaemon-controller: "rook-ceph-mon-a-7cc7cb8bdb-pwj7d" is a ceph pod!
2024-05-28 13:46:48.092790 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-5b8d759dc8-t8l6v" is a ceph pod!
2024-05-28 13:46:48.092801 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-wh979" is a ceph pod!
2024-05-28 13:46:48.093042 D | operator: reconciling rook-ceph/rook-ceph-operator-config
2024-05-28 13:46:48.096220 I | operator: rook-ceph-operator-config-controller done reconciling
2024-05-28 13:46:48.096929 D | ceph-spec: create event from a CR: "builtin-mgr"
2024-05-28 13:46:48.097268 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-28 13:46:48.097374 D | ceph-spec: "ceph-block-pool-controller": ceph status is "HEALTH_OK", operator is ready to run ceph command, reconciling
2024-05-28 13:46:48.099243 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:46:48.100244 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:46:48.100731 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.100774 D | ceph-cluster-controller: create event from a CR
2024-05-28 13:46:48.100792 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:46:48.101211 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:46:48.101290 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2024-05-28 13:46:48.101442 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0016e8d50], assignment=&{Schedule:map[a:0xc0015c8280]}
2024-05-28 13:46:48.101746 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:46:48.101831 I | ceph-block-pool-controller: creating pool ".mgr" in namespace "rook-ceph"
2024-05-28 13:46:48.101895 D | exec: Running command: ceph osd crush rule create-replicated .mgr default host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:48.102489 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.102666 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.103070 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:46:48.104437 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:46:48.104469 D | ceph-cluster-controller: node "minikube" is ready, checking if it can run OSDs
2024-05-28 13:46:48.104497 D | exec: Running command: ceph osd crush ls minikube --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:48.105044 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:46:48.105087 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00106f8c0], assignment=&{Schedule:map[a:0xc001a9cfc0]}
2024-05-28 13:46:48.110000 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:46:48.112065 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2024-05-28 13:46:48.112142 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2024-05-28 13:46:48.112180 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2024-05-28 13:46:48.112211 D | ceph-cluster-controller: cluster spec successfully validated
2024-05-28 13:46:48.112281 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2024-05-28 13:46:48.112438 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:46:48.112514 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:46:48.112689 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2024-05-28 13:46:48.112747 I | op-mon: stopping monitoring of mons in namespace "rook-ceph"
2024-05-28 13:46:48.112790 D | ceph-cluster-controller: checking health of cluster
2024-05-28 13:46:48.112826 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:46:48.119230 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:46:48.119286 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc001fe7890], assignment=&{Schedule:map[a:0xc001a9d500]}
2024-05-28 13:46:48.119314 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2024-05-28 13:46:48.119838 I | op-bucket-prov: successfully reconciled bucket provisioner
I0528 13:46:48.119921       1 manager.go:135] "msg"="starting provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket"
2024-05-28 13:46:48.122353 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.122432 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:48.122500 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v18...
2024-05-28 13:46:48.125386 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.125469 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:48.130427 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:46:48.130588 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:46:48.130705 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:46:48.130954 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:48.171492 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:46:48.216811 W | cephclient: failed to determine if all osds are running on the same host. will check if OSDs are ok-to-stop. if all OSDs are running on one host the user will likely need to set continueUpgradeAfterChecksEvenIfNotHealthy to allow OSD updates to proceed. failed to get the osd tree: failed to get osd tree: context canceled
2024-05-28 13:46:48.216841 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:46:48.216865 D | op-osd: not processing ERROR event for object "[could not determine name]"
2024-05-28 13:46:48.216874 I | op-osd: context cancelled, exiting OSD update and create loop
2024-05-28 13:46:48.216955 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "failed to create cluster: failed to start ceph osds: failed to update/create OSDs: context canceled"
2024-05-28 13:46:48.226531 I | ceph-cluster-controller: context cancelled, exiting reconcile
2024-05-28 13:46:48.226575 D | ceph-cluster-controller: successfully configured CephCluster "rook-ceph/my-cluster"
2024-05-28 13:46:48.230981 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.231032 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:48.231057 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.231070 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:48.415792 D | ceph-cluster-controller: failed to get osds on node "minikube", assume reconcile is necessary
2024-05-28 13:46:48.424363 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:46:48.424389 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-28 13:46:48.424442 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-28 13:46:48.424457 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
W0528 13:46:48.424674       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.NvmeOfStorage ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424735       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.CephCluster ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424741       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.CephFilesystemMirror ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424785       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424804       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.CephBlockPoolRadosNamespace ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424825       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.Deployment ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424846       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.CephObjectZoneGroup ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424864       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.CephBlockPool ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424942       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.424997       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
W0528 13:46:48.425051       1 reflector.go:462] pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
2024-05-28 13:46:48.425414 I | operator: successfully started the controller-runtime manager
2024-05-28 13:46:48.434628 D | exec: Running command: ceph osd pool get .mgr all --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:48.490195 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:25 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88223744 AvailableBytes:11200056119296 TotalBytes:11200144343040 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-28 13:46:48.498331 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:46:48.498425 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-28 13:46:48.498588 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-28 13:46:48.498781 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-28 13:46:48.498839 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:48.501423 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-28 13:46:48.501496 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:46:48.501545 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:46:48.501718 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:48.563365 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-05-28 13:46:48.756287 D | exec: Running command: ceph osd pool application get .mgr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:48.870842 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:46:48.870918 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-28 13:46:48.871030 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-28 13:46:48.871082 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-28 13:46:48.932136 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:46:48.932164 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:46:48.932271 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:25 NumOsd:3 NumUpOsd:3 NumInOsd:3 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:88223744 AvailableBytes:11200056119296 TotalBytes:11200144343040 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-28 13:46:48.932288 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-28 13:46:48.941669 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:48.941758 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:48.961483 D | ceph-csi: csi config map "rook-ceph-csi-config" (in "rook-ceph") has the expected owner; owner id: "f8c7b9c5-b6c8-4f59-9057-a054fd88c045"
2024-05-28 13:46:49.083593 I | cephclient: application "mgr" is already set on pool ".mgr"
2024-05-28 13:46:49.083709 I | cephclient: reconciling replicated pool .mgr succeeded
2024-05-28 13:46:49.083726 D | cephclient: skipping check for failure domain and deviceClass on pool ".mgr" as it is not specified
2024-05-28 13:46:49.083752 I | ceph-block-pool-controller: initializing pool ".mgr" for RBD use
2024-05-28 13:46:49.083806 D | exec: Running command: rbd pool init .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:46:49.118355 I | ceph-block-pool-controller: successfully initialized pool ".mgr" for RBD use
2024-05-28 13:46:49.118383 D | ceph-block-pool-controller: configuring RBD per-image IO statistics collection
2024-05-28 13:46:49.118459 D | exec: Running command: ceph config get mgr mgr/prometheus/rbd_stats_pools --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:49.164642 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:46:49.164703 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-28 13:46:49.438642 D | ceph-block-pool-controller: RBD per-image IO statistics will be collected for pools: []
2024-05-28 13:46:49.438675 I | op-config: setting "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-28 13:46:49.438702 D | exec: Running command: ceph config set mgr mgr/prometheus/rbd_stats_pools  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:49.564244 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:46:49.564364 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:46:49.564859 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:46:49.574409 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-28 13:46:49.574437 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:46:49.842652 I | op-config: successfully set "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-28 13:46:49.842710 D | ceph-block-pool-controller: configured RBD per-image IO statistics collection
2024-05-28 13:46:49.842728 D | ceph-block-pool-controller: reconciling create rbd mirror peer configuration
2024-05-28 13:46:49.842745 D | cephclient: retrieving mirroring pool ".mgr" info
2024-05-28 13:46:49.842792 D | exec: Running command: rbd mirror pool info .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:49.931294 I | cephclient: disabling mirroring for pool ".mgr"
2024-05-28 13:46:49.931389 D | exec: Running command: rbd mirror pool disable .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:46:49.966847 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-28 13:46:49.966911 D | ceph-cluster-controller: node "minikube-m03" is ready, checking if it can run OSDs
2024-05-28 13:46:49.966971 D | exec: Running command: ceph osd crush ls minikube-m03 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:50.014184 I | ceph-block-pool-controller: successfully disabled mirroring on the pool ".mgr"
2024-05-28 13:46:50.020872 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-28 13:46:50.020891 D | ceph-spec: update event on CephBlockPool CR
2024-05-28 13:46:50.020922 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:46:50.021000 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:46:50.021058 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:46:50.021309 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:50.022117 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Ready"
2024-05-28 13:46:50.022133 D | ceph-block-pool-controller: done reconciling
2024-05-28 13:46:50.022151 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-28 13:46:50.176448 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:46:50.334523 D | ceph-cluster-controller: node watcher: node "minikube-m03" is already an OSD node with "[\"osd.2\"]"
2024-05-28 13:46:50.361782 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:46:50.418807 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:46:50.418875 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-28 13:46:50.419002 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-28 13:46:50.419086 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-28 13:46:50.754179 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-05-28 13:46:50.764355 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:46:50.764511 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00126b2c0], assignment=&{Schedule:map[a:0xc001c00480]}
2024-05-28 13:46:50.764569 D | ceph-csi: cluster "rook-ceph/rook-ceph-operator-config": not deploying the ceph-csi plugin holder
2024-05-28 13:46:50.764594 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:46:50.966031 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:46:50.966259 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-28 13:46:50.967247 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:46:50.967409 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:46:51.364088 I | ceph-csi: Kubernetes version is 1.30
2024-05-28 13:46:51.565915 I | ceph-spec: detected ceph image version: "18.2.2-0 reef"
2024-05-28 13:46:51.565941 I | ceph-cluster-controller: validating ceph version from provided image
2024-05-28 13:46:51.576105 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:46:51.576129 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:46:51.576155 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:46:51.576228 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-28 13:46:51.771462 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-05-28 13:46:51.963842 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:46:52.163404 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-28 13:46:52.363425 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:46:52.363486 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0017886f0], assignment=&{Schedule:map[a:0xc001c01980]}
2024-05-28 13:46:52.763628 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:46:52.763663 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:46:52.763810 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:46:52.763845 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:53.259876 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:46:53.259937 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:46:53.260120 D | ceph-cluster-controller: both cluster and image spec versions are identical, doing nothing 18.2.2-0 reef
2024-05-28 13:46:53.260169 I | ceph-cluster-controller: cluster "rook-ceph": version "18.2.2-0 reef" detected for image "quay.io/ceph/ceph:v18"
2024-05-28 13:46:53.281832 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2024-05-28 13:46:53.298786 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:53.298805 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:53.303355 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-05-28 13:46:53.303412 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-05-28 13:46:53.303520 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2024-05-28 13:46:53.313409 D | op-mon: Acquiring lock for mon orchestration
2024-05-28 13:46:53.313434 D | op-mon: Acquired lock for mon orchestration
2024-05-28 13:46:53.313445 I | op-mon: start running mons
2024-05-28 13:46:53.313451 D | op-mon: establishing ceph cluster info
2024-05-28 13:46:53.315068 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-28 13:46:53.316695 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:46:53.316751 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:46:53.361377 I | ceph-spec: parsing mon endpoints: a=10.105.31.173:6789
2024-05-28 13:46:53.361470 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00163df20], assignment=&{Schedule:map[a:0xc000a7e4c0]}
2024-05-28 13:46:53.806526 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-28 13:46:54.366152 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-28 13:46:54.564179 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-05-28 13:46:54.572788 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:46:54.572826 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:46:54.572836 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:46:54.572871 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-28 13:46:54.666162 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-28 13:46:54.724043 I | ceph-csi: successfully started CSI CephFS driver
2024-05-28 13:46:54.732257 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-28 13:46:54.736470 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-28 13:46:54.736493 I | ceph-csi: CSI NFS driver disabled
2024-05-28 13:46:54.736501 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-28 13:46:54.739390 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-28 13:46:54.739442 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-28 13:46:54.763278 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.31.173:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.31.173:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-28 13:46:54.964204 D | op-config: updating config secret "rook-ceph-config"
2024-05-28 13:46:55.166633 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-28 13:46:55.166687 I | ceph-csi: successfully removed CSI NFS driver
2024-05-28 13:46:55.563100 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:46:55.563173 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:46:55.563470 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:46:55.563540 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:46:56.163455 D | op-cfg-keyring: updating secret for rook-ceph-mons-keyring
2024-05-28 13:46:56.563174 D | op-cfg-keyring: updating secret for rook-ceph-admin-keyring
2024-05-28 13:46:57.166018 I | op-mon: targeting the mon count 1
2024-05-28 13:46:57.172166 D | op-mon: Host network for mon "a" is false
2024-05-28 13:46:57.172272 D | op-mon: mon a already scheduled
2024-05-28 13:46:57.172288 D | op-mon: mons have been scheduled
2024-05-28 13:46:57.178098 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-05-28 13:46:57.178131 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/4032620430 -o /var/lib/rook/4032620430.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:57.627989 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:46:57.628639 I | op-config: applying ceph settings:
[global]
log to file = false
2024-05-28 13:46:57.628717 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3927808867 -o /var/lib/rook/3927808867.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:58.009174 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:46:58.009337 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-05-28 13:46:58.009408 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:46:58.588704 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-05-28 13:46:58.588764 I | op-mon: checking for basic quorum with existing mons
2024-05-28 13:46:58.594040 D | op-k8sutil: creating service rook-ceph-mon-a
2024-05-28 13:46:58.613112 D | op-k8sutil: updating service rook-ceph-mon-a
2024-05-28 13:46:58.624085 I | op-mon: mon "a" cluster IP is 10.105.31.173
2024-05-28 13:46:58.634144 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-28 13:46:58.637010 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.105.31.173:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.105.31.173:6789 mapping:{"node":{"a":{"Name":"minikube-m02","Hostname":"minikube-m02","Address":"192.168.58.3"}}} maxMonId:0 outOfQuorum:]
2024-05-28 13:46:58.763445 D | op-config: updating config secret "rook-ceph-config"
2024-05-28 13:46:59.163945 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-28 13:46:59.164020 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-28 13:46:59.164324 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-28 13:46:59.164376 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-28 13:46:59.573095 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.105.31.173 Port:6789 Zone: NodeName:minikube-m02 DataPathMap:0xc000c189c0 UseHostNetwork:false}
2024-05-28 13:46:59.585298 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-05-28 13:46:59.585606 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2024-05-28 13:46:59.595344 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2024-05-28 13:46:59.595368 I | op-mon: waiting for mon quorum with [a]
2024-05-28 13:46:59.770914 I | op-mon: mons running: [a]
2024-05-28 13:46:59.770975 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:00.293203 I | op-mon: Monitors in quorum: [a]
2024-05-28 13:47:00.293266 I | op-mon: mons created: 1
2024-05-28 13:47:00.293329 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:00.824464 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:47:00.824531 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:47:00.824659 I | op-mon: waiting for mon quorum with [a]
2024-05-28 13:47:00.838674 I | op-mon: mons running: [a]
2024-05-28 13:47:00.838767 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:01.383068 I | op-mon: Monitors in quorum: [a]
2024-05-28 13:47:01.383138 I | ceph-spec: not applying network settings for cluster "rook-ceph" ceph networks
2024-05-28 13:47:01.383161 D | op-mon: mon endpoints used are: a=10.105.31.173:6789
2024-05-28 13:47:01.383173 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-05-28 13:47:01.383185 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-05-28 13:47:01.383198 D | op-mon: Released lock for mon orchestration
2024-05-28 13:47:01.383211 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-05-28 13:47:01.383232 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-05-28 13:47:01.383288 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:01.901230 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-05-28 13:47:01.901329 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:02.606470 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-05-28 13:47:02.606574 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:03.088120 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-05-28 13:47:03.088217 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:03.612893 D | op-cfg-keyring: updating secret for rook-csi-rbd-provisioner
2024-05-28 13:47:03.621627 D | op-cfg-keyring: updating secret for rook-csi-rbd-node
2024-05-28 13:47:03.630749 D | op-cfg-keyring: updating secret for rook-csi-cephfs-provisioner
2024-05-28 13:47:03.638979 D | op-cfg-keyring: updating secret for rook-csi-cephfs-node
2024-05-28 13:47:03.641929 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2024-05-28 13:47:03.641991 I | cephclient: getting or creating ceph auth key "client.crash"
2024-05-28 13:47:03.642073 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:04.081708 D | op-cfg-keyring: updating secret for rook-ceph-crash-collector-keyring
2024-05-28 13:47:04.085168 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2024-05-28 13:47:04.085222 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-05-28 13:47:04.085413 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:04.613240 D | op-cfg-keyring: updating secret for rook-ceph-exporter-keyring
2024-05-28 13:47:04.617251 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2024-05-28 13:47:04.617322 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-05-28 13:47:04.617404 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:05.041555 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-05-28 13:47:05.041623 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-05-28 13:47:05.041702 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:05.489480 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-05-28 13:47:05.489547 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-05-28 13:47:05.489600 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:05.869076 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-05-28 13:47:05.869145 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-05-28 13:47:05.869206 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:06.359563 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-05-28 13:47:06.359704 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-05-28 13:47:06.359759 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:06.825557 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-05-28 13:47:06.826176 I | op-config: applying ceph settings:
[global]
osd_pool_default_size          = 1
bdev_flock_retry               = 20
bluefs_buffered_io             = false
mon_data_avail_warn            = 10
mon_warn_on_pool_no_redundancy = false
2024-05-28 13:47:06.826253 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/1591066722 -o /var/lib/rook/1591066722.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:07.232146 I | op-config: successfully applied settings to the mon configuration database
2024-05-28 13:47:07.232291 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-05-28 13:47:07.232311 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-05-28 13:47:07.232352 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:07.684140 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "my-cluster"
2024-05-28 13:47:07.684324 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-my-cluster" in namespace "rook-ceph"
2024-05-28 13:47:07.684452 D | op-k8sutil: creating secret cluster-peer-token-my-cluster
2024-05-28 13:47:07.697171 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-05-28 13:47:07.718508 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:47:07.718578 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:47:07.719356 I | op-mgr: start running mgr
2024-05-28 13:47:07.731511 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-05-28 13:47:07.731653 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:08.301621 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-05-28 13:47:08.305545 D | op-cfg-keyring: updating secret for rook-ceph-mgr-a-keyring
2024-05-28 13:47:08.309587 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc0011a3ce0}
2024-05-28 13:47:08.325413 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2024-05-28 13:47:08.349194 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2024-05-28 13:47:08.357294 D | op-mgr: expected number 1 of mgrs found
2024-05-28 13:47:08.357376 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-05-28 13:47:08.375587 D | op-k8sutil: updating service rook-ceph-mgr-dashboard
2024-05-28 13:47:08.384476 D | op-k8sutil: creating service rook-ceph-mgr
2024-05-28 13:47:08.409484 D | op-k8sutil: updating service rook-ceph-mgr
2024-05-28 13:47:08.424024 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2024-05-28 13:47:08.424093 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-05-28 13:47:08.424182 I | op-mgr: successful modules: balancer
2024-05-28 13:47:08.424212 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:08.424363 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:08.424492 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:08.445561 I | op-osd: start running osds in namespace "rook-ceph"
2024-05-28 13:47:08.445862 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-05-28 13:47:08.446576 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:47:08.446682 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:47:08.508256 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:08.936369 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:47:08.936453 I | op-osd: placement group status: "all PGs in cluster are clean"
2024-05-28 13:47:09.102127 I | op-osd: CRUSH location=root=default host=minikube-m02
2024-05-28 13:47:09.102164 I | op-osd: 1 osd(s) require migration to new backend store "bluestore".
2024-05-28 13:47:09.102172 I | op-osd: starting migration of the OSD.0
2024-05-28 13:47:09.102180 I | op-osd: removing the OSD deployment "rook-ceph-osd-0"
2024-05-28 13:47:09.102186 D | op-k8sutil: removing rook-ceph-osd-0 deployment if it exists
2024-05-28 13:47:09.102194 I | op-k8sutil: removing deployment rook-ceph-osd-0 if it exists
2024-05-28 13:47:09.110104 I | op-k8sutil: Removed deployment rook-ceph-osd-0
2024-05-28 13:47:09.110286 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:47:09.110319 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:47:09.119038 I | op-k8sutil: "rook-ceph-osd-0" still found. waiting...
2024-05-28 13:47:09.127523 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:47:09.127586 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:47:09.136475 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:47:09.136510 D | nvmeofstorage-controller: rook-ceph/rook-ceph-osd-0-66fdb58bd7-9x9dz is going to be deleted
2024-05-28 13:47:09.136521 D | nvmeofstorage-controller: update event on Pod "rook-ceph/rook-ceph-osd-0-66fdb58bd7-9x9dz"
2024-05-28 13:47:09.136657 D | nvmeofstorage-controller: reconciling NvmeOfStorage. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-66fdb58bd7-9x9dz
2024-05-28 13:47:09.136676 D | nvmeofstorage-controller: Pod "rook-ceph-osd-0-66fdb58bd7-9x9dz" is going be deleted
2024-05-28 13:47:09.141551 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:47:09.141642 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:47:09.141715 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:47:09.141940 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:09.175123 I | op-mgr: successful modules: mgr module(s) from the spec
2024-05-28 13:47:09.175176 D | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:09.175650 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:47:09.175670 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:47:09.182345 I | op-mgr: successful modules: prometheus
2024-05-28 13:47:09.514196 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:47:09.514298 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube-m02 minikube-m03]. pg health: "all PGs in cluster are clean"
2024-05-28 13:47:09.514550 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m02".
2024-05-28 13:47:09.514634 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube-m03".
2024-05-28 13:47:09.518242 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-28 13:47:10.180552 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:47:10.262207 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:10.741037 I | op-mgr: successful modules: orchestrator modules
2024-05-28 13:47:10.838383 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:47:10.846105 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 0
2024-05-28 13:47:10.846158 D | nvmeofstorage-controller: rook-ceph/rook-ceph-osd-0-66fdb58bd7-9x9dz is going to be deleted
2024-05-28 13:47:10.846180 D | nvmeofstorage-controller: update event on Pod "rook-ceph/rook-ceph-osd-0-66fdb58bd7-9x9dz"
2024-05-28 13:47:10.846323 D | nvmeofstorage-controller: reconciling NvmeOfStorage. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-66fdb58bd7-9x9dz
2024-05-28 13:47:10.846362 D | nvmeofstorage-controller: Pod "rook-ceph-osd-0-66fdb58bd7-9x9dz" is going be deleted
2024-05-28 13:47:10.850315 D | ceph-nodedaemon-controller: "rook-ceph-osd-0-66fdb58bd7-9x9dz" is a ceph pod!
2024-05-28 13:47:10.850475 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:47:10.851217 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:47:10.870758 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:47:10.870811 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:47:10.887648 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:47:10.895401 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:47:10.896101 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-28 13:47:10.896141 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:47:10.952035 I | ceph-spec: object "rook-ceph-osd-0" matched on delete, reconciling
2024-05-28 13:47:10.952113 D | ceph-spec: object "rook-ceph-osd-0" did not match on delete
2024-05-28 13:47:10.952154 D | ceph-spec: object "rook-ceph-osd-0" did not match on delete
2024-05-28 13:47:10.952178 D | ceph-spec: object "rook-ceph-osd-0" did not match on delete
2024-05-28 13:47:10.952190 D | ceph-spec: object "rook-ceph-osd-0" did not match on delete
2024-05-28 13:47:10.952202 D | ceph-spec: object "rook-ceph-osd-0" did not match on delete
2024-05-28 13:47:11.123024 I | op-k8sutil: confirmed rook-ceph-osd-0 does not exist
2024-05-28 13:47:11.138180 D | op-osd: !!osds to skip reconcilemap[]
2024-05-28 13:47:11.155267 D | op-osd: 2 of 2 OSD Deployments need update
2024-05-28 13:47:11.155327 D | op-osd: !!updateConfig: provisionConfig: &{/var/lib/rook  /var/lib/rook/rook-ceph}, numUpdatesNeeded: 2, deployments: &{map[1:true 2:true]}, osdsToSkipReconcile: map[]
2024-05-28 13:47:11.155381 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-05-28 13:47:11.158771 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-05-28 13:47:11.158821 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-05-28 13:47:11.167725 I | op-osd: 2 of the 2 storage nodes are valid
2024-05-28 13:47:11.183362 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m02 to start a new one
2024-05-28 13:47:11.195991 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 still exists
2024-05-28 13:47:11.210186 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-wnrwm" is a ceph pod!
2024-05-28 13:47:11.210371 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:47:11.211356 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:47:11.225419 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:47:11.225487 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:47:11.278193 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:47:11.505102 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:47:14.196622 I | op-mgr: the dashboard secret was already generated
2024-05-28 13:47:14.196690 I | op-mgr: setting ceph dashboard "admin" login creds
2024-05-28 13:47:14.197164 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2818902014 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:14.200147 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m02 deleted
2024-05-28 13:47:14.214014 I | op-osd: started OSD provisioning job for node "minikube-m02"
2024-05-28 13:47:14.234098 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m02-h7l7v" is a ceph pod!
2024-05-28 13:47:14.234304 D | ceph-nodedaemon-controller: reconciling node: "minikube-m02"
2024-05-28 13:47:14.235153 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:47:14.236873 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-minikube-m03 to start a new one
2024-05-28 13:47:14.259282 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m02". operation: "updated"
2024-05-28 13:47:14.259352 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:47:14.262553 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 still exists
2024-05-28 13:47:14.279294 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-wh979" is a ceph pod!
2024-05-28 13:47:14.282263 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:47:14.292175 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:47:14.292354 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:47:14.293174 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:47:14.311288 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-28 13:47:14.311345 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:47:14.331464 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:47:14.338558 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:47:14.679065 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/2818902014 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:15.400333 I | op-mgr: successfully set ceph dashboard creds
2024-05-28 13:47:15.400480 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:15.767392 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:16.163550 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:16.542785 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:16.966173 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:17.264697 I | op-k8sutil: batch job rook-ceph-osd-prepare-minikube-m03 deleted
2024-05-28 13:47:17.272615 I | op-osd: started OSD provisioning job for node "minikube-m03"
2024-05-28 13:47:17.272634 D | op-osd: !!statusConfigMapsmap[rook-ceph-osd-minikube-m02-status:{} rook-ceph-osd-minikube-m03-status:{}]
2024-05-28 13:47:17.272645 D | op-osd: !!createConfig: provisionConfig: &{/var/lib/rook  /var/lib/rook/rook-ceph}, awaitingStatusConfigMaps: map[rook-ceph-osd-minikube-m02-status:{} rook-ceph-osd-minikube-m03-status:{}], deployments: &{map[1:true 2:true]}, finishedStatusConfigMaps: map[], deployments: &{map[1:true 2:true]}
2024-05-28 13:47:17.276781 D | op-osd: !!createOSDsForStatusMap: status: &{[] starting false }
2024-05-28 13:47:17.276831 I | op-osd: OSD orchestration status for node minikube-m02 is "starting"
2024-05-28 13:47:17.276859 D | op-osd: !!createOSDsForStatusMap: status: &{[] starting false }
2024-05-28 13:47:17.276887 I | op-osd: OSD orchestration status for node minikube-m03 is "starting"
2024-05-28 13:47:17.286567 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-m03-djk5l" is a ceph pod!
2024-05-28 13:47:17.286847 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:47:17.288327 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:47:17.306324 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-28 13:47:17.306494 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:47:17.325299 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:47:17.331749 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:47:17.347374 I | op-mgr: successful modules: dashboard
2024-05-28 13:47:17.380507 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:17.787268 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:18.096517 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:18.243635 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-28 13:47:18.243766 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:47:18.481585 D | exec: Running command: ceph osd ok-to-stop 1 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:18.899452 D | op-osd: updating OSDs: [1]
2024-05-28 13:47:18.919957 I | op-osd: CRUSH location=root=default host=minikube-m02
2024-05-28 13:47:18.920025 I | op-osd: updating OSD 1 on node "minikube-m02"
2024-05-28 13:47:18.920269 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:47:18.920299 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:47:18.920384 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"minikube-m02\""
2024-05-28 13:47:18.936246 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:47:18.936271 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:47:18.942075 D | op-k8sutil: deployment "rook-ceph-osd-1" did not change. nothing to update
2024-05-28 13:47:18.946490 D | op-osd: !!createOSDsForStatusMap: status: &{[] orchestrating false }
2024-05-28 13:47:18.946517 I | op-osd: OSD orchestration status for node minikube-m03 is "orchestrating"
2024-05-28 13:47:19.047231 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:19.404249 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:19.799464 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:20.221568 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:20.589140 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:20.689260 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:21.004291 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:21.319553 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:21.616018 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:21.964698 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:22.065268 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:22.394882 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:22.689604 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:22.994573 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:23.308979 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:23.409308 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:23.793310 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:24.113848 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:24.422369 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:24.802150 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:24.902650 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:25.349470 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:25.754553 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:26.180391 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:26.556472 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:26.656721 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:27.082862 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:27.386615 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:27.700063 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:28.028025 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:28.128464 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:28.251175 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" matched on update
2024-05-28 13:47:28.251210 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:47:28.569970 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:28.931546 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:29.364881 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:29.785387 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:29.785935 D | op-osd: !!createOSDsForStatusMap: status: &{[{2 ceph 5046a442-9f1f-46a5-b89d-8311c0aa1cd4  nvme /dev/nvme2n1p2   true root=default host=minikube-m03 false raw bluestore  false false  } {1 ceph 77de06c2-fc87-46ec-98f9-cdc97175dde9  nvme /dev/nvme0n1p2   true root=default host=minikube-m03 false raw bluestore  false false  } {3 ceph 347862b9-97db-43be-bbda-b72641d46782  nvme /dev/nvme1n1p2   true root=default host=minikube-m03 false raw bluestore  false false  }] completed false }
2024-05-28 13:47:29.786058 I | op-osd: OSD orchestration status for node minikube-m03 is "completed"
2024-05-28 13:47:29.786088 D | op-osd: not creating deployment for OSD 2 which already exists
2024-05-28 13:47:29.786102 D | op-osd: not creating deployment for OSD 1 which already exists
2024-05-28 13:47:29.786119 I | op-osd: creating OSD 3 on node "minikube-m03"
2024-05-28 13:47:29.786825 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:47:29.786872 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:47:29.787097 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 3 on node \"minikube-m03\""
2024-05-28 13:47:29.801211 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:47:29.801269 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:47:29.831016 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:47:29.831039 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:47:29.831048 D | ceph-spec: object "rook-ceph-osd-minikube-m03-status" did not match on delete
2024-05-28 13:47:29.831062 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-m03-status" config map changes
2024-05-28 13:47:29.832360 D | op-osd: not processing DELETED event for object "rook-ceph-osd-minikube-m03-status"
2024-05-28 13:47:29.856213 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2024-05-28 13:47:29.856611 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:47:29.857155 D | ceph-nodedaemon-controller: "rook-ceph-osd-3-749b8cd4b5-djrj4" is a ceph pod!
2024-05-28 13:47:29.857433 D | ceph-nodedaemon-controller: reconciling node: "minikube-m03"
2024-05-28 13:47:29.860531 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:29.861791 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-28 13:47:29.871573 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:47:29.872115 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:47:29.872516 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:47:29.873772 I | clusterdisruption-controller: osd "rook-ceph-osd-3" is down but no node drain is detected
2024-05-28 13:47:29.874057 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:29.880757 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2024-05-28 13:47:29.880799 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:47:29.883460 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:29.883592 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube-m03". operation: "updated"
2024-05-28 13:47:29.883717 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-28 13:47:29.894986 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2024-05-28 13:47:29.895032 D | ceph-spec: do not reconcile deployments updates
2024-05-28 13:47:29.901434 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-28 13:47:29.907171 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-28 13:47:29.932952 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:30.278679 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:47:30.278713 I | clusterdisruption-controller: osd is down in failure domain "minikube-m03". pg health: "all PGs in cluster are clean"
2024-05-28 13:47:30.278752 I | clusterdisruption-controller: creating temporary blocking pdb "rook-ceph-osd-host-minikube-m02" with maxUnavailable=0 for "host" failure domain "minikube-m02"
2024-05-28 13:47:30.281936 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:47:30.282027 I | clusterdisruption-controller: deleting the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-28 13:47:30.348371 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:30.694463 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:31.043869 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:31.404968 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:31.423886 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:31.505389 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:31.855819 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:32.219417 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:32.497630 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:32.700755 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:33.108179 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:33.208803 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:33.551836 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:33.628896 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:34.017945 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:34.480906 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:34.595943 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:34.861139 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:34.961522 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:35.413355 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:35.657001 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:35.937010 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:36.393337 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:36.704831 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:36.839744 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:36.939852 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:37.300864 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:37.677694 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:38.015434 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:38.447689 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:38.548547 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:39.025337 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:39.444699 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:39.519060 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-28 13:47:39.519144 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-28 13:47:39.519187 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-28 13:47:39.519406 I | clusterdisruption-controller: osd "rook-ceph-osd-3" is down but no node drain is detected
2024-05-28 13:47:39.519446 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:39.892645 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:39.960012 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-28 13:47:39.960074 I | clusterdisruption-controller: osd is down in failure domain "minikube-m03". pg health: "all PGs in cluster are clean"
2024-05-28 13:47:39.960181 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-28 13:47:40.339876 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:40.440111 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:40.908213 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:41.293695 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:41.654869 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:42.021915 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:42.122147 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:42.540837 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:42.951957 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:43.404834 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:43.744775 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:43.845206 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:44.182010 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:44.670094 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:45.136756 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:45.586038 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:45.686180 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:46.145733 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:46.561456 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:46.883178 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:47.277154 I | op-osd: OSD 2 is not ok-to-stop. will try updating it again later
2024-05-28 13:47:47.378305 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:47.818029 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:47.964706 D | operator: number of goroutines 485
2024-05-28 13:47:48.113763 D | op-osd: checking osd processes status.
2024-05-28 13:47:48.113827 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:48.269358 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:48.496531 D | op-osd: validating status of osd.0
2024-05-28 13:47:48.496616 D | op-osd: osd.0 is marked 'DOWN'
2024-05-28 13:47:48.496632 D | op-osd: validating status of osd.1
2024-05-28 13:47:48.496646 D | op-osd: osd.1 is healthy.
2024-05-28 13:47:48.496657 D | op-osd: validating status of osd.2
2024-05-28 13:47:48.496671 D | op-osd: osd.2 is healthy.
2024-05-28 13:47:48.496682 D | op-osd: validating status of osd.3
2024-05-28 13:47:48.496694 D | op-osd: osd.3 is healthy.
2024-05-28 13:47:48.653892 D | exec: Running command: ceph osd ok-to-stop 2 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:48.944543 D | ceph-cluster-controller: checking health of cluster
2024-05-28 13:47:48.944630 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-28 13:47:49.130919 D | op-osd: updating OSDs: [2]
2024-05-28 13:47:49.154842 I | op-osd: CRUSH location=root=default host=minikube-m03
2024-05-28 13:47:49.154883 I | op-osd: updating OSD 2 on node "minikube-m03"
2024-05-28 13:47:49.155347 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-28 13:47:49.155371 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-28 13:47:49.155502 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 2 on node \"minikube-m03\""
2024-05-28 13:47:49.177504 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:47:49.177538 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:47:49.192821 D | op-k8sutil: deployment "rook-ceph-osd-2" did not change. nothing to update
2024-05-28 13:47:49.367012 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}}]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:35 NumOsd:4 NumUpOsd:3 NumInOsd:4 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:116809728 AvailableBytes:14933408980992 TotalBytes:14933525790720 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:138177 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-28 13:47:49.371203 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-28 13:47:49.794591 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:47:49.794657 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":5}}
2024-05-28 13:47:49.794927 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}}]} FSID:92050591-ab74-40a8-8b4c-a81c8434fa01 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:35 NumOsd:4 NumUpOsd:3 NumInOsd:4 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:116809728 AvailableBytes:14933408980992 TotalBytes:14933525790720 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:138177 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-28 13:47:49.794979 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-28 13:47:49.815724 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-05-28 13:47:49.817617 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-28 13:47:49.817648 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-28 13:47:49.824740 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-05-28 13:47:50.528518 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:50.556145 D | nvmeofstorage-controller: OSD Pod found. ceph-osd-id: 3
2024-05-28 13:47:50.598090 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2024-05-28 13:47:50.598142 D | ceph-spec: do not reconcile deployments updates
