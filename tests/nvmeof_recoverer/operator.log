2024/07/02 14:10:24 maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
2024-07-02 14:10:24.509381 I | rookcmd: starting Rook v1.14.0-alpha.0.227.ga75736a04-dirty with arguments '/usr/local/bin/rook ceph operator'
2024-07-02 14:10:24.509402 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO
2024-07-02 14:10:24.509405 I | cephcmd: starting Rook-Ceph operator
2024-07-02 14:10:24.683505 I | cephcmd: base ceph version inside the rook operator image is "ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)"
2024-07-02 14:10:24.689747 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="false" (env var)
2024-07-02 14:10:24.689788 I | operator: watching all namespaces for Ceph CRs
2024-07-02 14:10:24.689916 I | operator: setting up schemes
2024-07-02 14:10:24.695213 I | operator: setting up the controller-runtime manager
2024-07-02 14:10:24.696097 I | ceph-cluster-controller: successfully started
2024-07-02 14:10:24.700071 I | op-k8sutil: ROOK_DISABLE_DEVICE_HOTPLUG="false" (configmap)
2024-07-02 14:10:24.700102 I | ceph-cluster-controller: enabling hotplug orchestration
2024-07-02 14:10:24.700158 I | ceph-nodedaemon-controller: successfully started
2024-07-02 14:10:24.700198 I | ceph-block-pool-controller: successfully started
2024-07-02 14:10:24.700274 I | ceph-object-store-user-controller: successfully started
2024-07-02 14:10:24.700354 I | ceph-object-realm-controller: successfully started
2024-07-02 14:10:24.700385 I | ceph-object-zonegroup-controller: successfully started
2024-07-02 14:10:24.700402 I | ceph-object-zone-controller: successfully started
2024-07-02 14:10:24.700636 I | ceph-object-controller: successfully started
2024-07-02 14:10:24.700726 I | ceph-file-controller: successfully started
2024-07-02 14:10:24.701235 I | ceph-nfs-controller: successfully started
2024-07-02 14:10:24.701341 I | ceph-rbd-mirror-controller: successfully started
2024-07-02 14:10:24.701437 I | ceph-client-controller: successfully started
2024-07-02 14:10:24.701562 I | ceph-filesystem-mirror-controller: successfully started
2024-07-02 14:10:24.701637 I | operator: rook-ceph-operator-config-controller successfully started
2024-07-02 14:10:24.701719 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2024-07-02 14:10:24.701995 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2024-07-02 14:10:24.702059 I | ceph-bucket-topic: successfully started
2024-07-02 14:10:24.702106 I | ceph-bucket-notification: successfully started
2024-07-02 14:10:24.702150 I | ceph-bucket-notification: successfully started
2024-07-02 14:10:24.702188 I | ceph-fs-subvolumegroup-controller: successfully started
2024-07-02 14:10:24.702226 I | blockpool-rados-namespace-controller: successfully started
2024-07-02 14:10:24.702262 I | ceph-cosi-controller: successfully started
2024-07-02 14:10:24.702366 I | nvmeofstorage-controller: successfully started
2024-07-02 14:10:24.702447 I | operator: starting the controller-runtime manager
2024-07-02 14:10:24.832533 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2024-07-02 14:10:24.832553 I | op-k8sutil: ROOK_LOG_LEVEL="DEBUG" (configmap)
2024-07-02 14:10:24.832563 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="true" (configmap)
2024-07-02 14:10:24.832567 I | op-k8sutil: ROOK_DISCOVER_DEVICES_INTERVAL="60m" (configmap)
2024-07-02 14:10:24.832572 I | op-k8sutil: DISCOVER_DAEMON_RESOURCES="" (default)
2024-07-02 14:10:24.832581 I | op-k8sutil: DISCOVER_PRIORITY_CLASS_NAME="" (default)
2024-07-02 14:10:24.836947 I | op-k8sutil: ROOK_WATCH_FOR_NODE_FAILURE="true" (configmap)
2024-07-02 14:10:24.837828 I | op-k8sutil: DISCOVER_TOLERATION="" (default)
2024-07-02 14:10:24.837870 I | op-k8sutil: DISCOVER_TOLERATIONS="" (default)
2024-07-02 14:10:24.837881 I | op-discover: tolerations: []
2024-07-02 14:10:24.837891 I | op-k8sutil: DISCOVER_AGENT_NODE_AFFINITY="" (default)
2024-07-02 14:10:24.837907 I | op-k8sutil: DISCOVER_AGENT_POD_LABELS="" (default)
2024-07-02 14:10:24.840395 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-07-02 14:10:24.840441 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2024-07-02 14:10:24.840507 D | ceph-cluster-controller: "ceph-cluster-controller": no CephCluster resource found in namespace ""
2024-07-02 14:10:24.875536 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-07-02 14:10:24.875553 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2024-07-02 14:10:24.875570 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2024-07-02 14:10:24.875668 I | op-k8sutil: ROOK_CSI_DISABLE_DRIVER="false" (configmap)
2024-07-02 14:10:24.875837 D | ceph-cluster-controller: "ceph-cluster-controller": no CephCluster resource found in namespace ""
2024-07-02 14:10:24.877487 D | ceph-csi: no ceph cluster found not deploying ceph csi driver
2024-07-02 14:10:24.877582 I | ceph-csi: CSI Ceph RBD driver disabled
2024-07-02 14:10:24.877606 I | op-k8sutil: removing daemonset csi-rbdplugin if it exists
2024-07-02 14:10:24.910594 D | op-k8sutil: removing csi-rbdplugin-provisioner deployment if it exists
2024-07-02 14:10:24.910606 I | op-k8sutil: removing deployment csi-rbdplugin-provisioner if it exists
2024-07-02 14:10:24.911854 I | op-discover: rook-discover daemonset started
2024-07-02 14:10:24.911896 I | op-k8sutil: ROOK_CEPH_ALLOW_LOOP_DEVICES="false" (configmap)
2024-07-02 14:10:24.911911 I | operator: rook-ceph-operator-config-controller done reconciling
2024-07-02 14:10:24.913815 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-07-02 14:10:24.913855 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2024-07-02 14:10:24.932915 D | ceph-csi: nvmeof-recoverer-system.rbd.csi.ceph.com CSIDriver not found; skipping deletion.
2024-07-02 14:10:24.932944 I | ceph-csi: successfully removed CSI Ceph RBD driver
2024-07-02 14:10:24.932951 I | ceph-csi: CSI CephFS driver disabled
2024-07-02 14:10:24.932960 I | op-k8sutil: removing daemonset csi-cephfsplugin if it exists
2024-07-02 14:10:24.935827 D | op-k8sutil: removing csi-cephfsplugin-provisioner deployment if it exists
2024-07-02 14:10:24.935862 I | op-k8sutil: removing deployment csi-cephfsplugin-provisioner if it exists
2024-07-02 14:10:25.117289 D | ceph-csi: nvmeof-recoverer-system.cephfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-07-02 14:10:25.117325 I | ceph-csi: successfully removed CSI CephFS driver
2024-07-02 14:10:25.117333 I | ceph-csi: CSI NFS driver disabled
2024-07-02 14:10:25.117342 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-07-02 14:10:25.120391 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-07-02 14:10:25.120425 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-07-02 14:10:25.316896 D | ceph-csi: nvmeof-recoverer-system.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-07-02 14:10:25.316931 I | ceph-csi: successfully removed CSI NFS driver
2024-07-02 14:10:28.760885 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:10:28.761056 D | clusterdisruption-controller: create event from ceph cluster CR
2024-07-02 14:10:28.761243 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:10:28.761473 D | clusterdisruption-controller: reconciling "nvmeof-recoverer/nvmeof-recoverer"
2024-07-02 14:10:28.761666 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:10:28.761706 D | ceph-cluster-controller: create event from a CR
2024-07-02 14:10:28.761989 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "nvmeof-recoverer"
2024-07-02 14:10:28.763604 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2024-07-02 14:10:28.772061 I | ceph-cluster-controller: reconciling ceph cluster in namespace "nvmeof-recoverer"
2024-07-02 14:10:28.772246 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:10:28.772296 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:10:28.772494 D | ceph-csi: csi config map "rook-ceph-csi-config" (in "nvmeof-recoverer-system") has the expected owner; owner id: "a0674aaa-bea5-4a08-9f29-5a560e1a932d"
2024-07-02 14:10:28.772882 D | ceph-cluster-controller: skipping resource "nvmeof-recoverer" update with unchanged spec
2024-07-02 14:10:28.773883 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2024-07-02 14:10:28.776140 D | ceph-cluster-controller: cluster spec successfully validated
2024-07-02 14:10:28.776210 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Detecting Ceph version"
2024-07-02 14:10:28.782251 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2024-07-02 14:10:28.782362 I | op-k8sutil: CSI_DISABLE_HOLDER_PODS="true" (configmap)
2024-07-02 14:10:28.782684 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/daemon-base:latest-reef-devel...
2024-07-02 14:10:28.784324 I | ceph-csi: cluster info for cluster "nvmeof-recoverer" is not ready yet, will retry in 10s, proceeding with ready clusters
2024-07-02 14:10:28.784346 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2024-07-02 14:10:28.784352 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2024-07-02 14:10:28.784357 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2024-07-02 14:10:28.784363 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2024-07-02 14:10:28.784369 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2024-07-02 14:10:28.784373 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2024-07-02 14:10:28.784378 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2024-07-02 14:10:28.784390 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2024-07-02 14:10:28.784398 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2024-07-02 14:10:28.784403 I | op-k8sutil: CSI_ENABLE_LIVENESS="false" (configmap)
2024-07-02 14:10:28.784407 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2024-07-02 14:10:28.784412 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-07-02 14:10:28.784549 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2024-07-02 14:10:28.784583 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2024-07-02 14:10:28.784601 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2024-07-02 14:10:28.784611 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2024-07-02 14:10:28.784616 I | op-k8sutil: CSI_ENABLE_NFS_SNAPSHOTTER="true" (configmap)
2024-07-02 14:10:28.785391 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:10:28.785403 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:10:28.785836 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2024-07-02 14:10:28.785850 I | op-k8sutil: CSI_ENABLE_TOPOLOGY="false" (configmap)
2024-07-02 14:10:28.785854 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2024-07-02 14:10:28.785859 I | op-k8sutil: CSI_ENABLE_METADATA="false" (default)
2024-07-02 14:10:28.785863 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-07-02 14:10:28.785870 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-07-02 14:10:28.785874 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-07-02 14:10:28.785880 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-07-02 14:10:28.785884 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-07-02 14:10:28.785888 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2024-07-02 14:10:28.785892 I | ceph-csi: Kubernetes version is 1.28
2024-07-02 14:10:28.785896 I | op-k8sutil: CSI_LOG_LEVEL="5" (configmap)
2024-07-02 14:10:28.785900 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2024-07-02 14:10:28.785904 I | op-k8sutil: CSI_LEADER_ELECTION_LEASE_DURATION="" (default)
2024-07-02 14:10:28.785908 I | op-k8sutil: CSI_LEADER_ELECTION_RENEW_DEADLINE="" (default)
2024-07-02 14:10:28.785914 I | op-k8sutil: CSI_LEADER_ELECTION_RETRY_PERIOD="" (default)
2024-07-02 14:10:28.789475 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2024-07-02 14:10:28.789494 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.11.0" (default)
2024-07-02 14:10:28.789502 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1" (default)
2024-07-02 14:10:28.789508 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v4.0.1" (default)
2024-07-02 14:10:28.789513 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v4.5.1" (default)
2024-07-02 14:10:28.789542 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2" (default)
2024-07-02 14:10:28.789548 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.10.1" (default)
2024-07-02 14:10:28.789552 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2024-07-02 14:10:28.789557 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.8.0" (default)
2024-07-02 14:10:28.789561 I | op-k8sutil: CSI_TOPOLOGY_DOMAIN_LABELS="" (default)
2024-07-02 14:10:28.789564 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2024-07-02 14:10:28.789568 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2024-07-02 14:10:28.789571 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2024-07-02 14:10:28.789576 I | op-k8sutil: CSI_CLUSTER_NAME="" (default)
2024-07-02 14:10:28.789580 I | op-k8sutil: ROOK_CSI_IMAGE_PULL_POLICY="IfNotPresent" (default)
2024-07-02 14:10:28.789583 I | op-k8sutil: CSI_CEPHFS_KERNEL_MOUNT_OPTIONS="" (default)
2024-07-02 14:10:28.789588 I | op-k8sutil: CSI_CEPHFS_ATTACH_REQUIRED="true" (configmap)
2024-07-02 14:10:28.789592 I | op-k8sutil: CSI_RBD_ATTACH_REQUIRED="true" (configmap)
2024-07-02 14:10:28.789596 I | op-k8sutil: CSI_NFS_ATTACH_REQUIRED="true" (configmap)
2024-07-02 14:10:28.789601 I | op-k8sutil: CSI_DRIVER_NAME_PREFIX="nvmeof-recoverer-system" (default)
2024-07-02 14:10:28.791249 I | op-k8sutil: CSI_ENABLE_VOLUME_GROUP_SNAPSHOT="true" (configmap)
2024-07-02 14:10:28.791284 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-07-02 14:10:28.791372 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2024-07-02 14:10:28.791399 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2024-07-02 14:10:28.963745 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-07-02 14:10:55.087016 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-07-02 14:10:55.111878 I | ceph-spec: detected ceph image version: "18.2.2-1573 reef"
2024-07-02 14:10:55.111925 I | ceph-cluster-controller: validating ceph version from provided image
2024-07-02 14:10:55.114696 D | ceph-cluster-controller: cluster not initialized, nothing to validate. clusterInfo is nil
2024-07-02 14:10:55.114767 I | ceph-cluster-controller: cluster "nvmeof-recoverer": version "18.2.2-1573 reef" detected for image "quay.io/ceph/daemon-base:latest-reef-devel"
2024-07-02 14:10:55.137961 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:10:55.138013 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:10:55.138030 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:10:55.138039 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:10:55.141521 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring the Ceph cluster"
2024-07-02 14:10:55.142303 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:10:55.142329 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:10:55.151597 E | ceph-spec: failed to update cluster condition to {Type:Progressing Status:True Reason:ClusterProgressing Message:Configuring the Ceph cluster LastHeartbeatTime:2024-07-02 14:10:55.141509035 +0000 UTC m=+30.667856854 LastTransitionTime:2024-07-02 14:10:55.141508684 +0000 UTC m=+30.667856574}. failed to update object "nvmeof-recoverer/nvmeof-recoverer" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "nvmeof-recoverer": the object has been modified; please apply your changes to the latest version and try again
2024-07-02 14:10:55.155158 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-07-02 14:10:55.155183 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-07-02 14:10:55.155220 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring Ceph Mons"
2024-07-02 14:10:55.168962 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:10:55.168983 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:10:55.168989 I | op-mon: start running mons
2024-07-02 14:10:55.168993 D | op-mon: establishing ceph cluster info
2024-07-02 14:10:55.169735 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:10:55.169769 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:10:55.171345 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/nvmeof-recoverer/mon.keyring --gen-key -n mon. --cap mon 'allow *'
2024-07-02 14:10:55.231805 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/nvmeof-recoverer/client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mgr 'allow *' --cap mds 'allow'
2024-07-02 14:10:55.314668 I | ceph-spec: creating mon secrets for a new cluster
2024-07-02 14:10:55.334041 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2024-07-02 14:10:55.338994 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"nvmeof-recoverer","monitors":[],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data: mapping:{"node":{}} maxMonId:-1 outOfQuorum:]
2024-07-02 14:10:55.491492 D | op-config: creating config secret "rook-ceph-config"
2024-07-02 14:10:55.694453 D | op-config: updating config secret "rook-ceph-config"
2024-07-02 14:10:56.091548 I | cephclient: writing config file /var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config
2024-07-02 14:10:56.094081 I | cephclient: generated admin config in /var/lib/rook/nvmeof-recoverer
2024-07-02 14:10:56.094118 D | ceph-csi: using "nvmeof-recoverer-system" for csi configmap namespace
2024-07-02 14:10:56.690796 D | op-cfg-keyring: creating secret for rook-ceph-mons-keyring
2024-07-02 14:10:57.091626 D | op-cfg-keyring: creating secret for rook-ceph-admin-keyring
2024-07-02 14:10:57.692028 I | op-mon: targeting the mon count 1
2024-07-02 14:10:57.695959 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP: Port:6789 Zone: NodeName: DataPathMap:0xc001886de0 UseHostNetwork:false}
2024-07-02 14:10:57.696055 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:10:57.720884 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2024-07-02 14:10:57.745702 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-07-02 14:10:57.745728 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:10:57.770356 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-07-02 14:10:57.770370 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:10:57.807070 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-07-02 14:10:57.807093 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:10:58.091812 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to qemu2
2024-07-02 14:10:58.091836 I | op-mon: mon a assigned to node qemu2
2024-07-02 14:10:58.091841 D | op-mon: using internal IP 192.168.100.12 for node qemu2
2024-07-02 14:10:58.091860 D | op-mon: mons have been scheduled
2024-07-02 14:10:58.096866 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2024-07-02 14:10:58.104007 I | op-mon: creating mon a
2024-07-02 14:10:58.104074 D | op-k8sutil: creating service rook-ceph-mon-a
2024-07-02 14:10:58.104497 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-07-02 14:10:58.104530 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:10:58.115989 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-07-02 14:10:58.116009 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:10:58.197038 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-07-02 14:10:58.197091 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:10:58.300605 D | op-k8sutil: created service rook-ceph-mon-a
2024-07-02 14:10:58.300637 I | op-mon: mon "a" cluster IP is 10.43.224.72
2024-07-02 14:10:58.694021 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-07-02 14:10:58.894588 D | op-mon: mons were added or removed from the endpoints cm
2024-07-02 14:10:58.894626 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-07-02 14:10:58.894714 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"nvmeof-recoverer","monitors":["10.43.224.72:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.43.224.72:6789 mapping:{"node":{"a":{"Name":"qemu2","Hostname":"qemu2","Address":"192.168.100.12"}}} maxMonId:-1 outOfQuorum:]
2024-07-02 14:10:58.894747 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-07-02 14:10:58.894781 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-07-02 14:10:58.894802 D | op-mon: mons were added or removed from the endpoints cm
2024-07-02 14:10:58.894812 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-07-02 14:10:59.090553 D | ceph-spec: found existing monitor secrets for cluster nvmeof-recoverer
2024-07-02 14:10:59.447314 D | op-config: updating config secret "rook-ceph-config"
2024-07-02 14:10:59.586359 I | ceph-spec: parsing mon endpoints: a=10.43.224.72:6789
2024-07-02 14:10:59.586418 I | ceph-spec: updating obsolete maxMonID -1 to actual value 0
2024-07-02 14:10:59.586490 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0014ffb60], assignment=&{Schedule:map[a:0xc000adebc0]}
2024-07-02 14:10:59.586567 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2024-07-02 14:10:59.586591 I | op-k8sutil: ROOK_OBC_PROVISIONER_NAME_PREFIX="" (default)
2024-07-02 14:10:59.586629 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "nvmeof-recoverer.ceph.rook.io/bucket"
2024-07-02 14:10:59.587933 I | op-bucket-prov: successfully reconciled bucket provisioner
I0702 14:10:59.588051       1 manager.go:135] "msg"="starting provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="nvmeof-recoverer.ceph.rook.io/bucket"
2024-07-02 14:10:59.692122 D | ceph-spec: object "rook-ceph-config" matched on update
2024-07-02 14:10:59.692147 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2024-07-02 14:10:59.890104 I | cephclient: writing config file /var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config
2024-07-02 14:10:59.890320 I | cephclient: generated admin config in /var/lib/rook/nvmeof-recoverer
2024-07-02 14:10:59.890346 D | ceph-csi: using "nvmeof-recoverer-system" for csi configmap namespace
2024-07-02 14:11:00.298037 I | op-mon: 0 of 1 expected mons are ready. creating or updating deployments without checking quorum in attempt to achieve a healthy mon cluster
2024-07-02 14:11:00.298107 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.43.224.72 Port:6789 Zone: NodeName: DataPathMap:0xc001886de0 UseHostNetwork:false}
2024-07-02 14:11:00.298195 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:11:00.308021 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-07-02 14:11:00.308058 D | op-mon: Starting mon: rook-ceph-mon-a
2024-07-02 14:11:00.341931 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-07-02 14:11:00.341978 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:00.347642 D | ceph-nodedaemon-controller: "rook-ceph-mon-a-6cf448969c-zrzmb" is a ceph pod!
2024-07-02 14:11:00.347910 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:00.348484 D | ceph-nodedaemon-controller: secret "rook-ceph-crash-collector-keyring" in namespace "nvmeof-recoverer" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2024-07-02 14:11:00.368246 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-07-02 14:11:00.368303 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:00.391315 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-07-02 14:11:00.391343 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:00.490101 I | op-mon: updating maxMonID from -1 to 0
2024-07-02 14:11:00.605150 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-07-02 14:11:00.605193 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-07-02 14:11:00.605263 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-07-02 14:11:00.605333 D | ceph-spec: do not reconcile "rook-ceph-mon-a-canary" on monitor canary deployments
2024-07-02 14:11:00.605374 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-07-02 14:11:00.605406 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-07-02 14:11:00.691853 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-07-02 14:11:00.691896 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-07-02 14:11:01.094633 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-07-02 14:11:01.292368 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"nvmeof-recoverer","monitors":["10.43.224.72:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.43.224.72:6789 mapping:{"node":{"a":{"Name":"qemu2","Hostname":"qemu2","Address":"192.168.100.12"}}} maxMonId:0 outOfQuorum:]
2024-07-02 14:11:01.292414 I | op-mon: waiting for mon quorum with [a]
2024-07-02 14:11:01.493396 I | op-mon: mon a is not yet running
2024-07-02 14:11:01.493444 I | op-mon: mons running: []
2024-07-02 14:11:01.493474 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:14.462238 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-07-02 14:11:14.472173 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-07-02 14:11:14.477942 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:14.477977 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:14.477987 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:14.478017 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:14.487467 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2024-07-02 14:11:14.487513 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2024-07-02 14:11:14.487525 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2024-07-02 14:11:14.487534 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2024-07-02 14:11:14.487544 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2024-07-02 14:11:14.487554 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME="" (default)
2024-07-02 14:11:14.487563 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME_MOUNT="" (default)
2024-07-02 14:11:14.496918 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2024-07-02 14:11:14.496953 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2024-07-02 14:11:14.496965 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2024-07-02 14:11:14.504711 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-07-02 14:11:14.504729 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2024-07-02 14:11:14.504735 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2024-07-02 14:11:14.504739 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2024-07-02 14:11:14.504743 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME="" (default)
2024-07-02 14:11:14.504747 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME_MOUNT="" (default)
2024-07-02 14:11:14.852922 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2024-07-02 14:11:14.852968 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2024-07-02 14:11:14.852981 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2024-07-02 14:11:16.640411 D | op-mon: failed to get quorum_status. mon quorum status failed: exit status 1
2024-07-02 14:11:17.611292 I | ceph-csi: successfully started CSI CephFS driver
2024-07-02 14:11:17.611317 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="File" (configmap)
2024-07-02 14:11:17.710867 I | ceph-csi: CSIDriver object created for driver "nvmeof-recoverer-system.rbd.csi.ceph.com"
2024-07-02 14:11:17.710885 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="File" (configmap)
2024-07-02 14:11:17.726302 I | ceph-csi: CSIDriver object created for driver "nvmeof-recoverer-system.cephfs.csi.ceph.com"
2024-07-02 14:11:17.726339 I | ceph-csi: CSI NFS driver disabled
2024-07-02 14:11:17.726350 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-07-02 14:11:17.728873 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-07-02 14:11:17.728914 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-07-02 14:11:17.747059 D | ceph-csi: nvmeof-recoverer-system.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-07-02 14:11:17.747096 I | ceph-csi: successfully removed CSI NFS driver
2024-07-02 14:11:21.079629 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-07-02 14:11:21.079664 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:21.646449 I | op-mon: mons running: [a]
2024-07-02 14:11:21.646495 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:21.971431 I | op-mon: Monitors in quorum: [a]
2024-07-02 14:11:21.971479 I | op-mon: mons created: 1
2024-07-02 14:11:21.971538 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:22.312445 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1}}
2024-07-02 14:11:22.312489 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1}}
2024-07-02 14:11:22.312559 I | op-mon: waiting for mon quorum with [a]
2024-07-02 14:11:22.317206 I | op-mon: mons running: [a]
2024-07-02 14:11:22.317245 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:22.687263 I | op-mon: Monitors in quorum: [a]
2024-07-02 14:11:22.687721 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-07-02 14:11:22.687794 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/478545554 -o /var/lib/rook/478545554.out --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:22.954158 I | op-config: successfully applied settings to the mon configuration database
2024-07-02 14:11:22.954595 I | op-config: applying ceph settings:
[global]
log to file = true
2024-07-02 14:11:22.954643 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/2410359096 -o /var/lib/rook/2410359096.out --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:23.340364 I | op-config: successfully applied settings to the mon configuration database
2024-07-02 14:11:23.340487 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-07-02 14:11:23.340525 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:23.664476 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-07-02 14:11:23.664497 I | ceph-spec: not applying network settings for cluster "nvmeof-recoverer" ceph networks
2024-07-02 14:11:23.664502 D | op-mon: mon endpoints used are: a=10.43.224.72:6789
2024-07-02 14:11:23.664505 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-07-02 14:11:23.664507 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-07-02 14:11:23.664513 D | op-mon: Released lock for mon orchestration
2024-07-02 14:11:23.664517 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-07-02 14:11:23.664520 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-07-02 14:11:23.664532 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:24.031108 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-07-02 14:11:24.031175 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:24.332772 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-07-02 14:11:24.332857 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:24.690307 D | operator: number of goroutines 478
2024-07-02 14:11:24.698360 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-07-02 14:11:24.698435 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:25.116304 D | op-cfg-keyring: creating secret for rook-csi-rbd-provisioner
2024-07-02 14:11:25.142954 D | op-cfg-keyring: creating secret for rook-csi-rbd-node
2024-07-02 14:11:25.152727 D | op-cfg-keyring: creating secret for rook-csi-cephfs-provisioner
2024-07-02 14:11:25.162320 D | op-cfg-keyring: creating secret for rook-csi-cephfs-node
2024-07-02 14:11:25.168281 I | ceph-csi: created kubernetes csi secrets for cluster "nvmeof-recoverer"
2024-07-02 14:11:25.168327 I | cephclient: getting or creating ceph auth key "client.crash"
2024-07-02 14:11:25.168358 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:25.555624 D | op-cfg-keyring: creating secret for rook-ceph-crash-collector-keyring
2024-07-02 14:11:25.566312 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "nvmeof-recoverer"
2024-07-02 14:11:25.566457 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-07-02 14:11:25.566520 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:25.966031 D | op-cfg-keyring: creating secret for rook-ceph-exporter-keyring
2024-07-02 14:11:25.974058 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "nvmeof-recoverer"
2024-07-02 14:11:25.974120 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-07-02 14:11:25.974164 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:26.249772 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-07-02 14:11:26.249812 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-07-02 14:11:26.249840 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:26.601514 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-07-02 14:11:26.601590 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-07-02 14:11:26.601623 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:26.905353 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-07-02 14:11:26.905372 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-07-02 14:11:26.905386 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:27.204310 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-07-02 14:11:27.204353 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-07-02 14:11:27.204382 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:27.507855 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-07-02 14:11:27.507945 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:27.766140 D | ceph-csi: csi config map "rook-ceph-csi-config" (in "nvmeof-recoverer-system") has the expected owner; owner id: "a0674aaa-bea5-4a08-9f29-5a560e1a932d"
2024-07-02 14:11:27.775832 D | ceph-spec: found existing monitor secrets for cluster nvmeof-recoverer
2024-07-02 14:11:27.777816 I | ceph-spec: parsing mon endpoints: a=10.43.224.72:6789
2024-07-02 14:11:27.777926 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc001480fc0], assignment=&{Schedule:map[a:0xc0017406c0]}
2024-07-02 14:11:27.777968 D | ceph-csi: cluster "nvmeof-recoverer/nvmeof-recoverer": not deploying the ceph-csi plugin holder
2024-07-02 14:11:27.778064 D | ceph-csi: using "nvmeof-recoverer-system" for csi configmap namespace
2024-07-02 14:11:27.782902 I | ceph-csi: Kubernetes version is 1.28
2024-07-02 14:11:27.790726 I | ceph-cluster-controller: updating set-full-ratio from 0.95 to 0.96
2024-07-02 14:11:27.790778 D | exec: Running command: ceph osd set-full-ratio 0.96 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:27.795008 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-07-02 14:11:27.797684 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-07-02 14:11:28.121365 I | ceph-cluster-controller: updating set-backfillfull-ratio from 0.90 to 0.91
2024-07-02 14:11:28.121425 D | exec: Running command: ceph osd set-backfillfull-ratio 0.91 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:29.127268 I | ceph-cluster-controller: updating set-nearfull-ratio from 0.85 to 0.88
2024-07-02 14:11:29.127299 D | exec: Running command: ceph osd set-nearfull-ratio 0.88 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:30.188166 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-07-02 14:11:30.188194 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-07-02 14:11:30.188212 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:30.349490 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:30.350238 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:30.388509 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "created"
2024-07-02 14:11:30.407850 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "created"
2024-07-02 14:11:30.408152 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:30.421343 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:30.421368 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.426373 D | op-k8sutil: created service rook-ceph-exporter
2024-07-02 14:11:30.428127 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:30.428174 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:30.443068 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:30.443206 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:30.443600 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:30.445264 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:30.445298 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.449271 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu2-84885c846c-vkjkp" is a ceph pod!
2024-07-02 14:11:30.449767 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:30.449794 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.463268 D | ceph-nodedaemon-controller: crash collector unchanged on node "qemu2"
2024-07-02 14:11:30.465659 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:30.465673 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.470544 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:30.470577 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.475495 D | ceph-nodedaemon-controller: ceph exporter unchanged on node "qemu2"
2024-07-02 14:11:30.476565 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:30.476587 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:30.477946 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:30.478043 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:30.478356 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:30.489024 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:30.489129 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.489302 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:30.499607 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:30.499627 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:30.503048 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "nvmeof-recoverer"
2024-07-02 14:11:30.503193 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-nvmeof-recoverer" in namespace "nvmeof-recoverer"
2024-07-02 14:11:30.503215 D | op-k8sutil: creating secret cluster-peer-token-nvmeof-recoverer
2024-07-02 14:11:30.509635 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:30.510499 D | op-k8sutil: created secret cluster-peer-token-nvmeof-recoverer
2024-07-02 14:11:30.510580 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-07-02 14:11:30.516399 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:30.516417 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:30.519597 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:30.522335 I | op-mgr: start running mgr
2024-07-02 14:11:30.522933 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:11:30.522961 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:11:30.525402 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-07-02 14:11:30.525448 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:30.885120 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-07-02 14:11:30.887797 D | op-cfg-keyring: creating secret for rook-ceph-mgr-a-keyring
2024-07-02 14:11:30.894668 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc00102c9f0}
2024-07-02 14:11:30.894758 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:11:30.911678 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-07-02 14:11:30.911699 D | exec: Running command: ceph config set mon auth_allow_insecure_global_id_reclaim false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:30.937210 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-07-02 14:11:30.937227 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.944295 D | ceph-nodedaemon-controller: "rook-ceph-mgr-a-5f576657f6-v4s7b" is a ceph pod!
2024-07-02 14:11:30.944372 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:30.944581 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:30.957907 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:30.957925 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.958020 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:30.964889 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-07-02 14:11:30.964904 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.967683 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:30.967704 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:30.967960 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:30.967971 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.978496 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:30.978523 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.991484 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:30.997508 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:30.997539 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:30.999746 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-07-02 14:11:30.999754 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:31.000644 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:31.000660 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:31.002766 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:31.002901 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:31.003230 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:31.012288 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:31.012302 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:31.012784 D | ceph-nodedaemon-controller: crash collector unchanged on node "qemu2"
2024-07-02 14:11:31.020078 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:31.020096 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:31.022171 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:31.022180 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:31.031684 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:31.031695 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:31.037297 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:31.046371 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:31.046382 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:31.121198 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-07-02 14:11:31.121215 I | op-config: insecure global ID is now disabled
2024-07-02 14:11:31.124866 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5f576657f6" is progressing.}] CollisionCount:<nil>}
2024-07-02 14:11:31.212201 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:31.212235 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:31.214597 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:31.214663 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:31.214874 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:31.220057 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:31.225761 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:31.225773 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:31.428142 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:31.534414 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-07-02 14:11:32.023471 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:32.023497 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:32.026732 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:32.212369 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-07-02 14:11:32.228154 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:32.228180 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:32.228185 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:32.228193 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-07-02 14:11:32.289865 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-07-02 14:11:32.342160 I | ceph-csi: successfully started CSI CephFS driver
2024-07-02 14:11:32.347032 I | ceph-csi: CSIDriver object updated for driver "nvmeof-recoverer-system.rbd.csi.ceph.com"
2024-07-02 14:11:32.350764 I | ceph-csi: CSIDriver object updated for driver "nvmeof-recoverer-system.cephfs.csi.ceph.com"
2024-07-02 14:11:32.350779 I | ceph-csi: CSI NFS driver disabled
2024-07-02 14:11:32.350783 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-07-02 14:11:32.353344 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-07-02 14:11:32.353362 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-07-02 14:11:32.420379 D | ceph-csi: nvmeof-recoverer-system.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-07-02 14:11:32.420398 I | ceph-csi: successfully removed CSI NFS driver
2024-07-02 14:11:33.108277 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu2-84885c846c-vkjkp" is a ceph pod!
2024-07-02 14:11:33.108378 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:33.108697 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:33.123977 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:33.136399 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:33.136475 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:33.140818 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:33.140842 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:33.142796 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu2-5bb99789d9-sth4x" is a ceph pod!
2024-07-02 14:11:33.151101 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:33.157849 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:33.157887 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:33.159644 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:33.159810 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:33.160139 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:33.167415 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:33.167434 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:33.170798 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:33.180141 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:33.180162 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:33.218674 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:33.613811 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:33.613865 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:33.617944 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:34.128799 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5f576657f6" is progressing.}] CollisionCount:<nil>}
2024-07-02 14:11:34.921363 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:34.921395 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:34.921428 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:34.922086 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:34.928759 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:34.941920 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:34.941950 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:34.952053 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:34.956518 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:34.956560 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:34.958065 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:35.880640 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:11:35.880691 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:35.938104 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:35.938119 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:35.938185 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:35.938442 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:35.943223 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:35.954654 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:35.954671 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:35.964606 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:35.970664 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:35.970684 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:35.974094 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:35.974814 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:35.974848 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:11:35.974869 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:35.975282 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:35.990334 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:36.002757 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:36.002800 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:36.015323 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:36.023985 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:36.024031 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:36.026585 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:37.136542 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5f576657f6" is progressing.}] CollisionCount:<nil>}
2024-07-02 14:11:40.142010 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5f576657f6" is progressing.}] CollisionCount:<nil>}
2024-07-02 14:11:43.148691 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5f576657f6" is progressing.}] CollisionCount:<nil>}
2024-07-02 14:11:46.155128 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5f576657f6" is progressing.}] CollisionCount:<nil>}
2024-07-02 14:11:49.163229 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-07-02 14:11:30 +0000 UTC LastTransitionTime:2024-07-02 14:11:30 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5f576657f6" is progressing.}] CollisionCount:<nil>}
2024-07-02 14:11:51.683811 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-07-02 14:11:51.683849 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:11:52.170629 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2024-07-02 14:11:52.181591 D | op-mgr: expected number 1 of mgrs found
2024-07-02 14:11:52.184474 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-07-02 14:11:52.196262 D | op-k8sutil: created service rook-ceph-mgr-dashboard
2024-07-02 14:11:52.196319 D | op-k8sutil: creating service rook-ceph-mgr
2024-07-02 14:11:52.206954 D | op-k8sutil: created service rook-ceph-mgr
2024-07-02 14:11:52.212941 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:52.212966 I | op-mgr: successful modules: mgr module(s) from the spec
2024-07-02 14:11:52.212987 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:52.213062 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring Ceph OSDs"
2024-07-02 14:11:52.213300 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-07-02 14:11:52.213369 I | op-mgr: successful modules: balancer
2024-07-02 14:11:52.232263 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:11:52.232326 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:11:52.234461 I | op-osd: start running osds in namespace "nvmeof-recoverer"
2024-07-02 14:11:52.234525 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-07-02 14:11:52.234557 I | op-osd: no replacement of osds is requested
2024-07-02 14:11:52.243742 D | op-osd: 0 of 0 OSD Deployments need update
2024-07-02 14:11:52.243775 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-07-02 14:11:52.248105 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-07-02 14:11:52.248162 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-07-02 14:11:52.255139 I | op-osd: 2 of the 2 storage nodes are valid
2024-07-02 14:11:52.287396 I | op-osd: started OSD provisioning job for node "qemu1"
2024-07-02 14:11:52.315430 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-qemu1-4x497" is a ceph pod!
2024-07-02 14:11:52.315645 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:11:52.316625 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:52.318149 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:52.318175 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:52.325285 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:52.326131 I | op-osd: started OSD provisioning job for node "qemu2"
2024-07-02 14:11:52.330229 I | op-osd: OSD orchestration status for node qemu1 is "starting"
2024-07-02 14:11:52.330257 I | op-osd: OSD orchestration status for node qemu2 is "starting"
2024-07-02 14:11:52.343180 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-qemu2-66pvs" is a ceph pod!
2024-07-02 14:11:52.343334 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:11:52.343851 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:11:52.364008 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:52.376155 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:11:52.376193 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:11:52.397293 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:11:52.775432 I | op-mgr: successful modules: prometheus
2024-07-02 14:11:52.792892 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:11:52.792959 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:11:52.795825 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:11:56.405398 D | ceph-spec: object "rook-ceph-osd-qemu2-status" matched on update
2024-07-02 14:11:56.405430 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu2-status"
2024-07-02 14:11:56.405868 I | op-osd: OSD orchestration status for node qemu2 is "orchestrating"
2024-07-02 14:11:57.799278 I | op-mgr: setting ceph dashboard "admin" login creds
2024-07-02 14:11:57.799918 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/1226801932 administrator --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:11:59.396505 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/1226801932 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:00.087760 I | op-mgr: successfully set ceph dashboard creds
2024-07-02 14:12:00.087840 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:00.510014 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:00.906171 I | op-config: setting "mgr"="mgr/dashboard/ssl"="false" option to the mon configuration database
2024-07-02 14:12:00.906232 D | exec: Running command: ceph config set mgr mgr/dashboard/ssl false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:01.257210 I | op-config: successfully set "mgr"="mgr/dashboard/ssl"="false" option to the mon configuration database
2024-07-02 14:12:01.257274 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:01.667120 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:02.084906 I | op-config: setting "mgr"="mgr/dashboard/PROMETHEUS_API_SSL_VERIFY"="false" option to the mon configuration database
2024-07-02 14:12:02.084988 D | exec: Running command: ceph config set mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:02.560374 I | op-config: successfully set "mgr"="mgr/dashboard/PROMETHEUS_API_SSL_VERIFY"="false" option to the mon configuration database
2024-07-02 14:12:02.560449 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:03.023826 I | op-config: setting "mgr"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2024-07-02 14:12:03.023903 D | exec: Running command: ceph config set mgr mgr/dashboard/server_port 7000 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:03.458118 I | op-config: successfully set "mgr"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2024-07-02 14:12:03.458172 I | op-config: deleting "mgr.a" "mgr/dashboard/url_prefix" option from the mon configuration database
2024-07-02 14:12:03.458205 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/url_prefix --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:03.941124 I | op-config: successfully deleted "mgr/dashboard/url_prefix" option from the mon configuration database
2024-07-02 14:12:03.941149 I | op-config: deleting "mgr.a" "mgr/dashboard/ssl" option from the mon configuration database
2024-07-02 14:12:03.941171 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/ssl --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:04.339527 I | op-config: successfully deleted "mgr/dashboard/ssl" option from the mon configuration database
2024-07-02 14:12:04.339598 I | op-config: deleting "mgr.a" "mgr/dashboard/PROMETHEUS_API_HOST" option from the mon configuration database
2024-07-02 14:12:04.339659 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:04.848875 I | op-config: successfully deleted "mgr/dashboard/PROMETHEUS_API_HOST" option from the mon configuration database
2024-07-02 14:12:04.848918 I | op-config: deleting "mgr.a" "mgr/dashboard/PROMETHEUS_API_SSL_VERIFY" option from the mon configuration database
2024-07-02 14:12:04.848947 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:05.316517 I | op-config: successfully deleted "mgr/dashboard/PROMETHEUS_API_SSL_VERIFY" option from the mon configuration database
2024-07-02 14:12:05.316554 I | op-config: deleting "mgr.a" "mgr/dashboard/server_port" option from the mon configuration database
2024-07-02 14:12:05.316581 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/server_port --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:05.814961 I | op-config: successfully deleted "mgr/dashboard/server_port" option from the mon configuration database
2024-07-02 14:12:05.815002 I | op-config: deleting "mgr.a" "mgr/dashboard/ssl_server_port" option from the mon configuration database
2024-07-02 14:12:05.815034 D | exec: Running command: ceph config rm mgr.a mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:06.243579 I | op-config: successfully deleted "mgr/dashboard/ssl_server_port" option from the mon configuration database
2024-07-02 14:12:06.243604 I | op-mgr: All per-daemon mgr configuration has been deleted successfully.
2024-07-02 14:12:06.243608 I | op-mgr: dashboard config has changed. restarting the dashboard module
2024-07-02 14:12:06.243612 I | op-mgr: restarting the mgr module: dashboard
2024-07-02 14:12:06.243631 D | exec: Running command: ceph mgr module disable dashboard --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:07.416690 I | op-osd: OSD orchestration status for node qemu2 is "completed"
2024-07-02 14:12:07.416752 D | ceph-spec: object "rook-ceph-osd-qemu2-status" matched on update
2024-07-02 14:12:07.416780 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu2-status"
2024-07-02 14:12:07.416790 I | op-osd: creating OSD 0 on node "qemu2"
2024-07-02 14:12:07.417407 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:12:07.417504 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-07-02 14:12:07.417564 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-07-02 14:12:07.417797 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Processing OSD 0 on node \"qemu2\""
2024-07-02 14:12:07.436187 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:07.436226 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:07.464470 D | op-osd: not processing DELETED event for object "rook-ceph-osd-qemu2-status"
2024-07-02 14:12:07.464500 D | ceph-spec: do not reconcile on "rook-ceph-osd-qemu2-status" config map changes
2024-07-02 14:12:07.464523 D | ceph-spec: object "rook-ceph-osd-qemu2-status" did not match on delete
2024-07-02 14:12:07.464536 D | ceph-spec: object "rook-ceph-osd-qemu2-status" did not match on delete
2024-07-02 14:12:07.464539 D | ceph-spec: object "rook-ceph-osd-qemu2-status" did not match on delete
2024-07-02 14:12:07.490538 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-07-02 14:12:07.490552 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.495657 D | ceph-nodedaemon-controller: "rook-ceph-osd-0-5fc7d58679-k2d48" is a ceph pod!
2024-07-02 14:12:07.495761 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:07.496119 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:07.512184 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:07.512315 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:12:07.512350 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.523320 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:07.523349 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:07.523702 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:12:07.523734 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.532212 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-07-02 14:12:07.532242 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.543940 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:07.551737 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:07.551774 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:07.554070 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:07.554155 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:07.554579 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:07.557469 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:12:07.557478 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.560995 D | ceph-nodedaemon-controller: crash collector unchanged on node "qemu2"
2024-07-02 14:12:07.566431 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:12:07.566455 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.573942 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-07-02 14:12:07.573955 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.574216 D | ceph-nodedaemon-controller: ceph exporter unchanged on node "qemu2"
2024-07-02 14:12:07.575471 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:07.575520 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:07.577607 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:07.577661 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:07.577881 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:07.593288 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:07.603525 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:12:07.603541 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.603580 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:12:07.603584 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.606197 D | ceph-nodedaemon-controller: ceph exporter unchanged on node "qemu2"
2024-07-02 14:12:07.607200 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:07.607222 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:07.609101 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:07.609186 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:07.609203 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:07.609505 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:07.620675 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:07.625140 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:12:07.625156 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.631927 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:12:07.631946 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:07.637761 D | ceph-nodedaemon-controller: ceph exporter unchanged on node "qemu2"
2024-07-02 14:12:07.638681 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:07.638707 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:07.640585 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:07.640652 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:07.640955 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:07.651423 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:07.658795 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:07.658892 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:07.669729 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:07.675656 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:07.675672 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:07.677617 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:08.620561 I | op-mgr: successful modules: dashboard
2024-07-02 14:12:11.196441 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:12:11.196453 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:11.196505 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:11.196736 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:11.204120 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:11.218794 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:11.218817 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:11.222449 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu2-5bb99789d9-sth4x" is a ceph pod!
2024-07-02 14:12:11.231601 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:11.239382 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:11.239427 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:11.242492 D | ceph-spec: object "rook-ceph-crashcollector-qemu2" matched on update
2024-07-02 14:12:11.242522 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:11.243458 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:11.243505 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:11.243727 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:11.260824 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:12:11.260856 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:11.260967 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:11.261300 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu2-84885c846c-6s7x5" is a ceph pod!
2024-07-02 14:12:11.273032 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:11.273086 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:11.285502 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:11.294166 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:11.294211 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:11.297192 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:11.297286 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:12:11.297778 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:11.307813 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:12:11.307856 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:11.308810 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:11.319718 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:12:11.319746 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:11.331592 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:11.340085 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:11.340143 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:11.342507 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:14.378938 D | ceph-spec: object "rook-ceph-exporter-qemu2" matched on update
2024-07-02 14:12:14.379064 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:16.541036 I | op-osd: OSD orchestration status for node qemu1 is "orchestrating"
2024-07-02 14:12:16.543607 D | ceph-spec: object "rook-ceph-osd-qemu1-status" matched on update
2024-07-02 14:12:16.543655 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu1-status"
2024-07-02 14:12:27.779540 I | op-osd: OSD orchestration status for node qemu1 is "completed"
2024-07-02 14:12:27.779639 I | op-osd: creating OSD 1 on node "qemu1"
2024-07-02 14:12:27.779804 D | ceph-spec: object "rook-ceph-osd-qemu1-status" matched on update
2024-07-02 14:12:27.779857 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu1-status"
2024-07-02 14:12:27.780031 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:12:27.780100 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-07-02 14:12:27.780120 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-07-02 14:12:27.780368 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Processing OSD 1 on node \"qemu1\""
2024-07-02 14:12:27.798747 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:27.798793 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:27.840237 D | ceph-spec: object "rook-ceph-osd-qemu1-status" did not match on delete
2024-07-02 14:12:27.840338 D | ceph-spec: object "rook-ceph-osd-qemu1-status" did not match on delete
2024-07-02 14:12:27.840369 D | ceph-spec: do not reconcile on "rook-ceph-osd-qemu1-status" config map changes
2024-07-02 14:12:27.840391 D | ceph-spec: object "rook-ceph-osd-qemu1-status" did not match on delete
2024-07-02 14:12:27.843559 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:27.866906 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-7f8b5698c9-8hrh5" is a ceph pod!
2024-07-02 14:12:27.867066 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:12:27.867746 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:27.868747 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:12:27.868780 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.878061 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "created"
2024-07-02 14:12:27.887004 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "created"
2024-07-02 14:12:27.887049 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:27.905065 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:12:27.905102 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.905479 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:12:27.905513 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.914891 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:12:27.914919 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.920018 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu1-7c47d746d4-zmmwj" is a ceph pod!
2024-07-02 14:12:27.921948 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:27.936483 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:12:27.936519 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.938477 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:27.938610 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:27.939918 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:12:27.939953 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.941090 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:27.941320 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:12:27.941998 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:27.947844 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:12:27.947889 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.953713 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:12:27.955550 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:12:27.955577 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.961517 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:12:27.961576 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:27.968650 D | ceph-nodedaemon-controller: ceph exporter unchanged on node "qemu1"
2024-07-02 14:12:27.970083 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:27.970131 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:27.972595 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:27.972701 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:12:27.973351 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:27.984989 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:12:27.992903 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:12:27.992945 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:28.003724 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:28.011955 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:28.011988 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:28.014179 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:28.262239 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-07-02 14:12:28.262279 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:28.355614 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:12:28.355652 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:12:28.355760 D | exec: Running command: ceph osd require-osd-release reef --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:28.862560 D | cephclient: 
2024-07-02 14:12:28.862614 I | cephclient: successfully disallowed pre-reef osds and enabled all new reef-only functionality
2024-07-02 14:12:28.866387 D | op-osd: successfully deleted key rotation cron jobs
2024-07-02 14:12:28.866471 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:29.384165 I | op-osd: finished running OSDs in namespace "nvmeof-recoverer"
2024-07-02 14:12:29.384185 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "nvmeof-recoverer"
2024-07-02 14:12:29.384220 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:12:29.385700 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:29.385736 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:29.394452 E | ceph-spec: failed to update cluster condition to {Type:Ready Status:True Reason:ClusterCreated Message:Cluster created successfully LastHeartbeatTime:2024-07-02 14:12:29.384216221 +0000 UTC m=+124.910563975 LastTransitionTime:2024-07-02 14:12:29.384216127 +0000 UTC m=+124.910563893}. failed to update object "nvmeof-recoverer/nvmeof-recoverer" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "nvmeof-recoverer": the object has been modified; please apply your changes to the latest version and try again
2024-07-02 14:12:29.394500 D | ceph-csi: using "nvmeof-recoverer-system" for csi configmap namespace
2024-07-02 14:12:29.394514 I | ceph-cluster-controller: reporting cluster telemetry
2024-07-02 14:12:29.394543 D | op-config: setting "rook/version"="v1.14.0-alpha.0.227.ga75736a04-dirty" option in the mon config-key store
2024-07-02 14:12:29.394559 D | exec: Running command: ceph config-key set rook/version v1.14.0-alpha.0.227.ga75736a04-dirty --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:29.397688 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "nvmeof-recoverer"
2024-07-02 14:12:29.397744 I | op-osd: ceph osd status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:12:29.397754 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "nvmeof-recoverer"
2024-07-02 14:12:29.397768 I | ceph-cluster-controller: ceph status check interval is 5s
2024-07-02 14:12:29.397776 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "nvmeof-recoverer"
2024-07-02 14:12:29.397809 D | ceph-cluster-controller: successfully configured CephCluster "nvmeof-recoverer/nvmeof-recoverer"
2024-07-02 14:12:29.397837 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:12:29.397857 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:12:29.397861 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:12:29.953700 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:13 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28372992 AvailableBytes:3840727609344 TotalBytes:3840755982336 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:12:29.961405 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:29.966002 D | telemetry: set telemetry key: rook/version=v1.14.0-alpha.0.227.ga75736a04-dirty
2024-07-02 14:12:29.967360 D | op-config: setting "rook/kubernetes/version"="v1.28.10+rke2r1" option in the mon config-key store
2024-07-02 14:12:29.967379 D | exec: Running command: ceph config-key set rook/kubernetes/version v1.28.10+rke2r1 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:30.477180 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:12:30.477234 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:12:30.477393 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:13 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28372992 AvailableBytes:3840727609344 TotalBytes:3840755982336 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:12:30.477428 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:12:30.497390 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:30.497429 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:30.521029 D | telemetry: set telemetry key: rook/kubernetes/version=v1.28.10+rke2r1
2024-07-02 14:12:30.521071 D | op-config: setting "rook/csi/version"="v3.11.0" option in the mon config-key store
2024-07-02 14:12:30.521102 D | exec: Running command: ceph config-key set rook/csi/version v3.11.0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:31.059995 D | telemetry: set telemetry key: rook/csi/version=v3.11.0
2024-07-02 14:12:31.060023 D | op-config: setting "rook/cluster/mon/max-id"="0" option in the mon config-key store
2024-07-02 14:12:31.060043 D | exec: Running command: ceph config-key set rook/cluster/mon/max-id 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:31.670747 D | telemetry: set telemetry key: rook/cluster/mon/max-id=0
2024-07-02 14:12:31.670778 D | op-config: setting "rook/cluster/mon/count"="1" option in the mon config-key store
2024-07-02 14:12:31.670792 D | exec: Running command: ceph config-key set rook/cluster/mon/count 1 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:32.248448 D | telemetry: set telemetry key: rook/cluster/mon/count=1
2024-07-02 14:12:32.248495 D | op-config: setting "rook/cluster/mon/allow-multiple-per-node"="true" option in the mon config-key store
2024-07-02 14:12:32.248528 D | exec: Running command: ceph config-key set rook/cluster/mon/allow-multiple-per-node true --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:32.571303 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:12:32.571322 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:32.841923 D | telemetry: set telemetry key: rook/cluster/mon/allow-multiple-per-node=true
2024-07-02 14:12:32.841974 D | op-config: setting "rook/cluster/mon/pvc/enabled"="false" option in the mon config-key store
2024-07-02 14:12:32.842014 D | exec: Running command: ceph config-key set rook/cluster/mon/pvc/enabled false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:33.454473 D | telemetry: set telemetry key: rook/cluster/mon/pvc/enabled=false
2024-07-02 14:12:33.454500 D | op-config: setting "rook/cluster/mon/stretch/enabled"="false" option in the mon config-key store
2024-07-02 14:12:33.454513 D | exec: Running command: ceph config-key set rook/cluster/mon/stretch/enabled false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:33.519667 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:12:33.519702 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:33.519793 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:12:33.520457 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:12:33.541899 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:12:33.553923 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:12:33.553942 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:12:33.566603 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:12:33.575589 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:12:33.575614 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:12:33.578088 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:12:34.096126 D | telemetry: set telemetry key: rook/cluster/mon/stretch/enabled=false
2024-07-02 14:12:34.096184 D | op-config: setting "rook/cluster/storage/device-set/count/total"="0" option in the mon config-key store
2024-07-02 14:12:34.096234 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/total 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:34.698914 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/total=0
2024-07-02 14:12:34.698957 D | op-config: setting "rook/cluster/storage/device-set/count/portable"="0" option in the mon config-key store
2024-07-02 14:12:34.698985 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/portable 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:35.299268 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/portable=0
2024-07-02 14:12:35.299291 D | op-config: setting "rook/cluster/storage/device-set/count/non-portable"="0" option in the mon config-key store
2024-07-02 14:12:35.299307 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/non-portable 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:35.494950 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:12:35.495040 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:12:35.846983 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/non-portable=0
2024-07-02 14:12:35.847037 D | op-config: setting "rook/cluster/network/provider"="" option in the mon config-key store
2024-07-02 14:12:35.847065 D | exec: Running command: ceph config-key set rook/cluster/network/provider  --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:35.989936 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:13 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28372992 AvailableBytes:3840727609344 TotalBytes:3840755982336 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:12:35.996595 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:36.438126 D | telemetry: set telemetry key: rook/cluster/network/provider=
2024-07-02 14:12:36.438173 D | op-config: setting "rook/cluster/external-mode"="false" option in the mon config-key store
2024-07-02 14:12:36.438201 D | exec: Running command: ceph config-key set rook/cluster/external-mode false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:36.474643 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:12:36.474683 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:12:36.474829 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:13 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28372992 AvailableBytes:3840727609344 TotalBytes:3840755982336 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:12:36.474859 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:12:36.496398 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:36.496433 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:37.010701 D | telemetry: set telemetry key: rook/cluster/external-mode=false
2024-07-02 14:12:37.010744 I | ceph-cluster-controller: reporting node telemetry
2024-07-02 14:12:37.017164 D | op-config: setting "rook/node/count/kubernetes-total"="3" option in the mon config-key store
2024-07-02 14:12:37.017219 D | exec: Running command: ceph config-key set rook/node/count/kubernetes-total 3 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:37.621178 D | telemetry: set telemetry key: rook/node/count/kubernetes-total=3
2024-07-02 14:12:37.632073 D | op-config: setting "rook/node/count/with-ceph-daemons"="2" option in the mon config-key store
2024-07-02 14:12:37.632132 D | exec: Running command: ceph config-key set rook/node/count/with-ceph-daemons 2 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:38.258790 D | telemetry: set telemetry key: rook/node/count/with-ceph-daemons=2
2024-07-02 14:12:38.270688 D | op-config: setting "rook/node/count/with-csi-rbd-plugin"="3" option in the mon config-key store
2024-07-02 14:12:38.270741 D | exec: Running command: ceph config-key set rook/node/count/with-csi-rbd-plugin 3 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:38.861458 D | telemetry: set telemetry key: rook/node/count/with-csi-rbd-plugin=3
2024-07-02 14:12:38.869360 D | op-config: setting "rook/node/count/with-csi-cephfs-plugin"="3" option in the mon config-key store
2024-07-02 14:12:38.869378 D | exec: Running command: ceph config-key set rook/node/count/with-csi-cephfs-plugin 3 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:39.398950 D | op-mon: checking health of mons
2024-07-02 14:12:39.399002 D | op-osd: checking osd processes status.
2024-07-02 14:12:39.399111 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:39.399239 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:12:39.399497 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:12:39.405220 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:12:39.405272 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:39.524044 D | telemetry: set telemetry key: rook/node/count/with-csi-cephfs-plugin=3
2024-07-02 14:12:39.530219 D | op-config: setting "rook/node/count/with-csi-nfs-plugin"="0" option in the mon config-key store
2024-07-02 14:12:39.530427 D | exec: Running command: ceph config-key set rook/node/count/with-csi-nfs-plugin 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:39.835313 D | op-osd: validating status of osd.0
2024-07-02 14:12:39.835353 D | op-osd: osd.0 is healthy.
2024-07-02 14:12:39.835361 D | op-osd: validating status of osd.1
2024-07-02 14:12:39.835368 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:12:39.846045 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:12:39.846061 D | op-mon: targeting the mon count 1
2024-07-02 14:12:39.846066 D | op-mon: mon "a" found in quorum
2024-07-02 14:12:39.846069 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:12:39.851323 D | op-mon: skipping check for multiple mons on same node since multiple mons are allowed
2024-07-02 14:12:39.851367 D | op-mon: Released lock for mon orchestration
2024-07-02 14:12:39.851390 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:12:40.112621 D | telemetry: set telemetry key: rook/node/count/with-csi-nfs-plugin=0
2024-07-02 14:12:41.496337 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:12:41.496513 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:12:42.084891 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:15 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28372992 AvailableBytes:3840727609344 TotalBytes:3840755982336 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:12:42.091660 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:42.656058 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:12:42.656120 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:12:42.656280 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:15 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28372992 AvailableBytes:3840727609344 TotalBytes:3840755982336 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:12:42.656306 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:12:42.680465 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:42.680504 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:47.677793 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:12:47.677863 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:12:48.220702 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:55619584 AvailableBytes:7681456345088 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:12:48.227495 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:48.589610 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:12:48.589647 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:12:48.687263 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:12:48.687319 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:12:48.687456 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:55619584 AvailableBytes:7681456345088 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:12:48.687487 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:12:48.709728 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:48.709759 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:49.835974 D | op-osd: checking osd processes status.
2024-07-02 14:12:49.836052 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:49.852543 D | op-mon: checking health of mons
2024-07-02 14:12:49.852589 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:12:49.852600 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:12:49.859051 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:12:49.859107 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:50.380246 D | op-osd: validating status of osd.0
2024-07-02 14:12:50.380284 D | op-osd: osd.0 is healthy.
2024-07-02 14:12:50.380290 D | op-osd: validating status of osd.1
2024-07-02 14:12:50.380296 D | op-osd: osd.1 is healthy.
2024-07-02 14:12:50.408076 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:12:50.408119 D | op-mon: targeting the mon count 1
2024-07-02 14:12:50.408134 D | op-mon: mon "a" found in quorum
2024-07-02 14:12:50.408142 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:12:50.414319 D | op-mon: Released lock for mon orchestration
2024-07-02 14:12:50.414345 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:12:53.708462 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:12:53.708558 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:12:54.291284 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:59669 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:12:54.298632 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:12:54.974962 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:12:54.975004 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:12:54.975145 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:59669 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:12:54.975177 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:12:54.998439 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:12:54.998478 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:12:59.995440 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:12:59.995510 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:00.380793 D | op-osd: checking osd processes status.
2024-07-02 14:13:00.380874 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:00.415341 D | op-mon: checking health of mons
2024-07-02 14:13:00.415382 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:13:00.415394 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:13:00.421679 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:13:00.421726 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:00.564879 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:49183 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:00.570020 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:00.915941 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:13:00.915985 D | op-mon: targeting the mon count 1
2024-07-02 14:13:00.916001 D | op-mon: mon "a" found in quorum
2024-07-02 14:13:00.916009 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:13:00.916740 D | op-osd: validating status of osd.0
2024-07-02 14:13:00.916754 D | op-osd: osd.0 is healthy.
2024-07-02 14:13:00.916756 D | op-osd: validating status of osd.1
2024-07-02 14:13:00.916758 D | op-osd: osd.1 is healthy.
2024-07-02 14:13:00.921643 D | op-mon: Released lock for mon orchestration
2024-07-02 14:13:00.921690 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:13:01.021675 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:01.021723 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:01.024308 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:49183 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:01.024348 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:01.072190 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:01.072226 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:06.073795 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:06.082120 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:06.646552 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:06.662287 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:07.209164 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:07.209214 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:07.209407 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:07.209570 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:07.232865 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:07.232907 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:10.916835 D | op-osd: checking osd processes status.
2024-07-02 14:13:10.916911 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:10.927960 D | op-mon: checking health of mons
2024-07-02 14:13:10.928013 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:13:10.928022 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:13:10.936468 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:13:10.936523 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:11.398129 D | op-osd: validating status of osd.0
2024-07-02 14:13:11.398169 D | op-osd: osd.0 is healthy.
2024-07-02 14:13:11.398177 D | op-osd: validating status of osd.1
2024-07-02 14:13:11.398184 D | op-osd: osd.1 is healthy.
2024-07-02 14:13:11.416323 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:13:11.416342 D | op-mon: targeting the mon count 1
2024-07-02 14:13:11.416353 D | op-mon: mon "a" found in quorum
2024-07-02 14:13:11.416355 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:13:11.421891 D | op-mon: Released lock for mon orchestration
2024-07-02 14:13:11.421940 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:13:12.232118 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:12.232179 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:12.816051 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:12.826121 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:13.414120 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:13.414138 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:13.414211 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:13.414224 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:13.438262 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:13.438304 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:18.434852 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:18.434922 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:18.997033 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:19.003740 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:19.407326 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:19.407375 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:19.407534 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:19.407574 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:19.434740 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:19.434756 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:21.401409 D | op-osd: checking osd processes status.
2024-07-02 14:13:21.401492 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:21.423051 D | op-mon: checking health of mons
2024-07-02 14:13:21.423093 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:13:21.423101 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:13:21.429659 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:13:21.429703 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:21.850364 D | op-osd: validating status of osd.0
2024-07-02 14:13:21.850410 D | op-osd: osd.0 is healthy.
2024-07-02 14:13:21.850418 D | op-osd: validating status of osd.1
2024-07-02 14:13:21.850425 D | op-osd: osd.1 is healthy.
2024-07-02 14:13:21.883544 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:13:21.883586 D | op-mon: targeting the mon count 1
2024-07-02 14:13:21.883602 D | op-mon: mon "a" found in quorum
2024-07-02 14:13:21.883609 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:13:21.889689 D | op-mon: Released lock for mon orchestration
2024-07-02 14:13:21.889734 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:13:24.435812 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:24.435870 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:25.031997 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:25.039261 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:25.623947 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:25.624002 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:25.624162 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:17 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:25.624197 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:25.646016 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:25.646055 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:28.176848 D | ceph-spec: create event from a CR: "nvmeofstorage-pbssd1"
2024-07-02 14:13:28.177240 D | nvmeofstorage-controller: reconciling NvmeOfStorage. Request.Namespace: nvmeof-recoverer, Request.Name: nvmeofstorage-pbssd1
2024-07-02 14:13:28.191437 D | exec: Running command: ceph osd crush move osd.1 host=fabric-host-pbssd1 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:29.427807 D | nvmeofstorage-controller: Successfully updated CRUSH Map. osdID: 1, srcHost: qemu1, destHost: fabric-host-pbssd1
2024-07-02 14:13:29.443077 D | exec: Running command: ceph osd crush move osd.0 host=fabric-host-pbssd1 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:30.450436 D | nvmeofstorage-controller: Successfully updated CRUSH Map. osdID: 0, srcHost: qemu2, destHost: fabric-host-pbssd1
2024-07-02 14:13:30.460588 D | ceph-spec: update event from a CR: "nvmeofstorage-pbssd1"
2024-07-02 14:13:30.460624 D | nvmeofstorage-controller: successfully configured NvmeOfStorage "nvmeof-recoverer/nvmeofstorage-pbssd1"
2024-07-02 14:13:30.646261 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:30.646346 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:31.154531 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:19 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:31.161964 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:31.672866 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:31.672910 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:31.673076 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:19 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56238080 AvailableBytes:7681455726592 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:31.673112 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:31.695561 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:31.695601 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:31.850900 D | op-osd: checking osd processes status.
2024-07-02 14:13:31.850998 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:31.890464 D | op-mon: checking health of mons
2024-07-02 14:13:31.890500 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:13:31.890509 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:13:31.895880 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:13:31.895929 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:32.286796 D | op-osd: validating status of osd.0
2024-07-02 14:13:32.286860 D | op-osd: osd.0 is healthy.
2024-07-02 14:13:32.286870 D | op-osd: validating status of osd.1
2024-07-02 14:13:32.286877 D | op-osd: osd.1 is healthy.
2024-07-02 14:13:32.460515 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:13:32.460552 D | op-mon: targeting the mon count 1
2024-07-02 14:13:32.460573 D | op-mon: mon "a" found in quorum
2024-07-02 14:13:32.460586 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:13:32.466006 D | op-mon: Released lock for mon orchestration
2024-07-02 14:13:32.466052 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:13:36.695866 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:36.695943 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:37.283121 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:19 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56246272 AvailableBytes:7681455718400 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:37.287541 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:37.917062 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:37.917105 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:37.917271 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:19 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56246272 AvailableBytes:7681455718400 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:37.917315 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:37.940371 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:37.940413 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:42.287549 D | op-osd: checking osd processes status.
2024-07-02 14:13:42.287620 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:42.467108 D | op-mon: checking health of mons
2024-07-02 14:13:42.467157 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:13:42.467166 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:13:42.473469 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:13:42.473579 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:42.787385 D | op-osd: validating status of osd.0
2024-07-02 14:13:42.787427 D | op-osd: osd.0 is healthy.
2024-07-02 14:13:42.787435 D | op-osd: validating status of osd.1
2024-07-02 14:13:42.787441 D | op-osd: osd.1 is healthy.
2024-07-02 14:13:42.939427 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:42.939485 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:43.023594 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:13:43.023609 D | op-mon: targeting the mon count 1
2024-07-02 14:13:43.023614 D | op-mon: mon "a" found in quorum
2024-07-02 14:13:43.023616 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:13:43.029642 D | op-mon: Released lock for mon orchestration
2024-07-02 14:13:43.029742 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:13:43.482145 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:19 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56258560 AvailableBytes:7681455706112 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:43.489790 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:44.045688 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:44.045729 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":2},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":4}}
2024-07-02 14:13:44.045871 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:19 NumOsd:2 NumUpOsd:2 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56258560 AvailableBytes:7681455706112 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:44.045903 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:44.068191 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:44.068227 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:48.447512 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:13:48.447563 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:48.471092 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:13:48.471129 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:48.559624 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:13:48.559661 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:49.067301 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:49.067361 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:49.621186 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56258560 AvailableBytes:7681455706112 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:49.628559 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:50.188927 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:13:50.188969 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:13:50.189155 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56258560 AvailableBytes:7681455706112 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:50.189186 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:50.210169 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:13:50.211552 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:50.211590 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:50.218131 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:13:50.218171 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg peering"
2024-07-02 14:13:51.832008 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-7f8b5698c9-8hrh5" is a ceph pod!
2024-07-02 14:13:51.832168 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:13:51.832755 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:13:51.832968 I | ceph-nodedaemon-controller: deleting deployment "rook-ceph-crashcollector-qemu1" for node "qemu1"
2024-07-02 14:13:51.840481 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:13:51.840548 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:13:51.840564 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:13:51.840576 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:13:51.840591 D | ceph-spec: do not reconcile "rook-ceph-crashcollector-qemu1" on rook-ceph-crashcollector
2024-07-02 14:13:51.840604 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:13:51.840688 I | ceph-nodedaemon-controller: successfully removed deployment "rook-ceph-crashcollector-qemu1" in namespace "nvmeof-recoverer" from node "qemu1"
2024-07-02 14:13:51.840801 I | ceph-nodedaemon-controller: deleting deployment "rook-ceph-exporter-qemu1" for node "qemu1"
2024-07-02 14:13:51.849752 I | ceph-nodedaemon-controller: successfully removed deployment "rook-ceph-exporter-qemu1" in namespace "nvmeof-recoverer" from node "qemu1"
2024-07-02 14:13:51.850266 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:13:51.850365 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:13:51.850397 D | ceph-spec: do not reconcile "rook-ceph-exporter-qemu1" on rook-ceph-exporter
2024-07-02 14:13:51.850418 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:13:51.850454 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:13:51.850482 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:13:51.851609 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:13:51.851723 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:13:51.854399 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:13:51.854513 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:13:51.855058 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:13:51.856591 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:13:51.856627 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:13:51.859259 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:13:51.878885 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-bfd7ffdbc-c7bt2" is a ceph pod!
2024-07-02 14:13:51.879220 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:13:51.880149 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:13:51.881165 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:13:51.881190 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.893005 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "created"
2024-07-02 14:13:51.902309 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "created"
2024-07-02 14:13:51.902357 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:13:51.915780 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:13:51.915816 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.920813 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:13:51.920859 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.923404 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:13:51.923509 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.930979 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu1-7c47d746d4-wsp7v" is a ceph pod!
2024-07-02 14:13:51.937027 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:13:51.940252 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:13:51.940294 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.947416 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:13:51.947462 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:13:51.950761 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:13:51.950889 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:13:51.951007 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:13:51.951054 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.951630 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:13:51.960171 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:13:51.960205 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.960793 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:13:51.960825 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.963041 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:13:51.963068 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:51.964789 D | ceph-nodedaemon-controller: crash collector unchanged on node "qemu1"
2024-07-02 14:13:51.974068 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:13:51.974106 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:13:51.982738 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:13:51.989396 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:13:51.989439 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:13:51.991386 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:13:51.991495 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:13:51.992074 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:13:52.002118 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:13:52.011182 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:13:52.011220 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:13:52.020008 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:13:52.026336 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:13:52.026393 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:13:52.028214 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:13:52.787822 D | op-osd: checking osd processes status.
2024-07-02 14:13:52.787912 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:53.030226 D | op-mon: checking health of mons
2024-07-02 14:13:53.030268 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:13:53.030277 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:13:53.036563 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:13:53.036609 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:53.156387 D | op-osd: validating status of osd.0
2024-07-02 14:13:53.156433 D | op-osd: osd.0 is healthy.
2024-07-02 14:13:53.156440 D | op-osd: validating status of osd.1
2024-07-02 14:13:53.156448 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:13:53.561964 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:13:53.562004 D | op-mon: targeting the mon count 1
2024-07-02 14:13:53.562020 D | op-mon: mon "a" found in quorum
2024-07-02 14:13:53.562028 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:13:53.568069 D | op-mon: Released lock for mon orchestration
2024-07-02 14:13:53.568105 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:13:54.906805 D | ceph-spec: object "rook-ceph-exporter-qemu1" matched on update
2024-07-02 14:13:54.906839 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:55.148175 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu1-7c47d746d4-zmmwj" is a ceph pod!
2024-07-02 14:13:55.148353 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:13:55.149136 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:13:55.161289 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:13:55.172290 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:13:55.172338 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:13:55.187010 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:13:55.196684 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:13:55.196733 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:13:55.199629 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:13:55.219291 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:13:55.219340 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:13:55.777495 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:13:55.785473 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:13:55.877486 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" matched on update
2024-07-02 14:13:55.877504 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:13:55.877711 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:13:55.878386 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:13:55.895113 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:13:55.905774 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu1". operation: "updated"
2024-07-02 14:13:55.905817 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:13:55.919576 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:13:55.931271 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:13:55.931317 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:13:55.933806 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:13:56.315058 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:13:56.315102 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:13:56.315282 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:13:56.315324 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:13:56.335837 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:13:56.338331 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:13:56.338345 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:13:56.343585 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:13:56.343622 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg peering"
2024-07-02 14:14:01.344719 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:01.344777 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:01.842251 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:01.850196 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:02.234187 I | nvmeofstorage-controller: OSD Pod "nvmeof-recoverer/rook-ceph-osd-1-bfd7ffdbc-c7bt2" is in CrashLoopBackOff, oldPod.Status.Phase: Running
2024-07-02 14:14:02.234227 D | nvmeofstorage-controller: update event on Pod "nvmeof-recoverer/rook-ceph-osd-1-bfd7ffdbc-c7bt2"
2024-07-02 14:14:02.234328 D | nvmeofstorage-controller: reconciling NvmeOfStorage. Request.Namespace: nvmeof-recoverer, Request.Name: rook-ceph-osd-1-bfd7ffdbc-c7bt2
2024-07-02 14:14:02.234418 D | op-k8sutil: removing rook-ceph-osd-1 deployment if it exists
2024-07-02 14:14:02.234476 I | op-k8sutil: removing deployment rook-ceph-osd-1 if it exists
2024-07-02 14:14:02.242329 I | op-k8sutil: Removed deployment rook-ceph-osd-1
2024-07-02 14:14:02.242990 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:14:02.243027 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:14:02.251845 I | op-k8sutil: "rook-ceph-osd-1" still found. waiting...
2024-07-02 14:14:02.269960 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:14:02.270000 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:14:02.297503 I | nvmeofstorage-controller: OSD Pod "nvmeof-recoverer/rook-ceph-osd-1-bfd7ffdbc-c7bt2" is in CrashLoopBackOff, oldPod.Status.Phase: Running
2024-07-02 14:14:02.297682 D | nvmeofstorage-controller: update event on Pod "nvmeof-recoverer/rook-ceph-osd-1-bfd7ffdbc-c7bt2"
2024-07-02 14:14:02.309246 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:02.309288 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:02.309441 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:02.309469 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:02.336149 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:02.336187 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:02.336196 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:02.343799 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg peering"
2024-07-02 14:14:02.343835 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:02.365211 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:14:02.365243 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:14:03.157109 D | op-osd: checking osd processes status.
2024-07-02 14:14:03.157179 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:03.568827 D | op-mon: checking health of mons
2024-07-02 14:14:03.568870 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:14:03.568878 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:14:03.576251 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:14:03.576306 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:03.656277 D | op-osd: validating status of osd.0
2024-07-02 14:14:03.656312 D | op-osd: osd.0 is healthy.
2024-07-02 14:14:03.656319 D | op-osd: validating status of osd.1
2024-07-02 14:14:03.656326 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:14:04.122048 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:14:04.122087 D | op-mon: targeting the mon count 1
2024-07-02 14:14:04.122103 D | op-mon: mon "a" found in quorum
2024-07-02 14:14:04.122110 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:14:04.128170 D | op-mon: Released lock for mon orchestration
2024-07-02 14:14:04.128210 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:14:04.957705 D | ceph-nodedaemon-controller: "rook-ceph-osd-1-bfd7ffdbc-c7bt2" is a ceph pod!
2024-07-02 14:14:04.957836 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:14:04.958429 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:14:04.958628 I | ceph-nodedaemon-controller: deleting deployment "rook-ceph-crashcollector-qemu1" for node "qemu1"
2024-07-02 14:14:04.963869 I | ceph-nodedaemon-controller: successfully removed deployment "rook-ceph-crashcollector-qemu1" in namespace "nvmeof-recoverer" from node "qemu1"
2024-07-02 14:14:04.964013 I | ceph-nodedaemon-controller: deleting deployment "rook-ceph-exporter-qemu1" for node "qemu1"
2024-07-02 14:14:04.964836 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:14:04.964946 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:14:04.965017 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:14:04.965029 D | ceph-spec: do not reconcile "rook-ceph-crashcollector-qemu1" on rook-ceph-crashcollector
2024-07-02 14:14:04.965039 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:14:04.965059 D | ceph-spec: object "rook-ceph-crashcollector-qemu1" did not match on delete
2024-07-02 14:14:04.969778 I | ceph-nodedaemon-controller: successfully removed deployment "rook-ceph-exporter-qemu1" in namespace "nvmeof-recoverer" from node "qemu1"
2024-07-02 14:14:04.969942 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:14:04.969988 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:14:04.970002 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:14:04.970019 D | ceph-spec: do not reconcile "rook-ceph-exporter-qemu1" on rook-ceph-exporter
2024-07-02 14:14:04.970054 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:14:04.970067 D | ceph-spec: object "rook-ceph-exporter-qemu1" did not match on delete
2024-07-02 14:14:04.971402 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:14:04.971446 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:14:04.974529 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:14:04.974673 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:14:04.975234 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:14:04.976366 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:14:04.976405 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:14:04.979074 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:14:05.014725 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2024-07-02 14:14:05.014767 D | ceph-spec: do not reconcile deployments updates
2024-07-02 14:14:05.090953 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-07-02 14:14:05.090991 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-07-02 14:14:05.090999 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-07-02 14:14:05.091023 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-07-02 14:14:05.091031 D | ceph-spec: object "rook-ceph-osd-1" did not match on delete
2024-07-02 14:14:05.091036 I | ceph-spec: object "rook-ceph-osd-1" matched on delete, reconciling
2024-07-02 14:14:05.091305 I | ceph-cluster-controller: reconciling ceph cluster in namespace "nvmeof-recoverer"
2024-07-02 14:14:05.096389 D | ceph-spec: found existing monitor secrets for cluster nvmeof-recoverer
2024-07-02 14:14:05.100866 I | ceph-spec: parsing mon endpoints: a=10.43.224.72:6789
2024-07-02 14:14:05.100944 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0018f7110], assignment=&{Schedule:map[a:0xc001d2f1c0]}
2024-07-02 14:14:05.113170 D | ceph-cluster-controller: monitoring routine for "mon" is already running
2024-07-02 14:14:05.113206 D | ceph-cluster-controller: monitoring routine for "osd" is already running
2024-07-02 14:14:05.113215 D | ceph-cluster-controller: monitoring routine for "status" is already running
2024-07-02 14:14:05.113223 D | ceph-cluster-controller: cluster spec successfully validated
2024-07-02 14:14:05.113321 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Detecting Ceph version"
2024-07-02 14:14:05.153153 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:05.153192 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:05.153288 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/daemon-base:latest-reef-devel...
2024-07-02 14:14:05.155885 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-07-02 14:14:06.264692 I | op-k8sutil: confirmed rook-ceph-osd-1 does not exist
2024-07-02 14:14:06.275107 I | op-k8sutil: waiting for job nvmeof-conn-control-job to complete...
2024-07-02 14:14:06.278045 D | op-k8sutil: job is still initializing
2024-07-02 14:14:07.344656 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:07.344719 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:07.387274 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-07-02 14:14:07.399919 I | ceph-spec: detected ceph image version: "18.2.2-1573 reef"
2024-07-02 14:14:07.399936 I | ceph-cluster-controller: validating ceph version from provided image
2024-07-02 14:14:07.402207 D | ceph-spec: found existing monitor secrets for cluster nvmeof-recoverer
2024-07-02 14:14:07.407875 I | ceph-spec: parsing mon endpoints: a=10.43.224.72:6789
2024-07-02 14:14:07.407955 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc001396300], assignment=&{Schedule:map[a:0xc001e54e00]}
2024-07-02 14:14:07.411249 I | cephclient: writing config file /var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config
2024-07-02 14:14:07.411542 I | cephclient: generated admin config in /var/lib/rook/nvmeof-recoverer
2024-07-02 14:14:07.411584 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:07.413602 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:14:07.413809 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:14:07.413851 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:14:07.413871 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-07-02 14:14:07.840703 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:07.848206 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:07.953059 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:07.953101 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:07.953191 D | ceph-cluster-controller: both cluster and image spec versions are identical, doing nothing 18.2.2-1573 reef
2024-07-02 14:14:07.953220 I | ceph-cluster-controller: cluster "nvmeof-recoverer": version "18.2.2-1573 reef" detected for image "quay.io/ceph/daemon-base:latest-reef-devel"
2024-07-02 14:14:07.978644 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring the Ceph cluster"
2024-07-02 14:14:07.999789 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:07.999841 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:08.002769 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-07-02 14:14:08.002780 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-07-02 14:14:08.002801 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring Ceph Mons"
2024-07-02 14:14:08.021791 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:14:08.021802 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:14:08.021804 I | op-mon: start running mons
2024-07-02 14:14:08.021806 D | op-mon: establishing ceph cluster info
2024-07-02 14:14:08.022256 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:08.022272 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:08.024466 D | ceph-spec: found existing monitor secrets for cluster nvmeof-recoverer
2024-07-02 14:14:08.026943 I | ceph-spec: parsing mon endpoints: a=10.43.224.72:6789
2024-07-02 14:14:08.026980 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0004c0570], assignment=&{Schedule:map[a:0xc001857340]}
2024-07-02 14:14:08.042893 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-07-02 14:14:08.046825 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"nvmeof-recoverer","monitors":["10.43.224.72:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.43.224.72:6789 mapping:{"node":{"a":{"Name":"qemu2","Hostname":"qemu2","Address":"192.168.100.12"}}} maxMonId:0 outOfQuorum:]
2024-07-02 14:14:08.191859 D | op-config: updating config secret "rook-ceph-config"
2024-07-02 14:14:08.253745 D | ceph-nodedaemon-controller: "rook-ceph-exporter-qemu1-7c47d746d4-wsp7v" is a ceph pod!
2024-07-02 14:14:08.253884 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:14:08.254436 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:14:08.256247 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:14:08.256286 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:14:08.258426 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:08.258443 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:08.258517 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:08.258526 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:08.258943 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:14:08.271060 E | ceph-spec: failed to update cluster condition to {Type:Ready Status:True Reason:ClusterCreated Message:Cluster created successfully LastHeartbeatTime:2024-07-02 14:14:08.258522633 +0000 UTC m=+223.784870397 LastTransitionTime:2024-07-02 14:12:30 +0000 UTC}. failed to update object "nvmeof-recoverer/nvmeof-recoverer" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "nvmeof-recoverer": the object has been modified; please apply your changes to the latest version and try again
2024-07-02 14:14:08.271091 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:08.595405 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:08.595445 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg peering"
2024-07-02 14:14:08.791552 I | cephclient: writing config file /var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config
2024-07-02 14:14:08.791864 I | cephclient: generated admin config in /var/lib/rook/nvmeof-recoverer
2024-07-02 14:14:08.791899 D | ceph-csi: using "nvmeof-recoverer-system" for csi configmap namespace
2024-07-02 14:14:09.391966 D | op-cfg-keyring: updating secret for rook-ceph-mons-keyring
2024-07-02 14:14:09.790654 D | op-cfg-keyring: updating secret for rook-ceph-admin-keyring
2024-07-02 14:14:10.392464 I | op-mon: targeting the mon count 1
2024-07-02 14:14:10.396436 D | op-mon: Host network for mon "a" is false
2024-07-02 14:14:10.396477 D | op-mon: mon a already scheduled
2024-07-02 14:14:10.396484 D | op-mon: mons have been scheduled
2024-07-02 14:14:10.401462 I | op-config: applying ceph settings:
[global]
mon allow pool delete   = true
mon cluster log file    = 
mon allow pool size one = true
2024-07-02 14:14:10.401604 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/2087347008 -o /var/lib/rook/2087347008.out --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:10.836560 I | op-config: successfully applied settings to the mon configuration database
2024-07-02 14:14:10.837000 I | op-config: applying ceph settings:
[global]
log to file = true
2024-07-02 14:14:10.837038 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3108812526 -o /var/lib/rook/3108812526.out --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:11.195219 I | op-config: successfully applied settings to the mon configuration database
2024-07-02 14:14:11.195340 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-07-02 14:14:11.195391 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:11.279348 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:06 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001390f24}
2024-07-02 14:14:11.585489 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-07-02 14:14:11.585568 I | op-mon: checking for basic quorum with existing mons
2024-07-02 14:14:11.588425 D | op-k8sutil: creating service rook-ceph-mon-a
2024-07-02 14:14:11.600647 D | op-k8sutil: updating service rook-ceph-mon-a
2024-07-02 14:14:11.606210 I | op-mon: mon "a" cluster IP is 10.43.224.72
2024-07-02 14:14:11.613066 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-07-02 14:14:11.792072 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"nvmeof-recoverer","monitors":["10.43.224.72:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.43.224.72:6789 mapping:{"node":{"a":{"Name":"qemu2","Hostname":"qemu2","Address":"192.168.100.12"}}} maxMonId:0 outOfQuorum:]
2024-07-02 14:14:11.992346 D | op-config: updating config secret "rook-ceph-config"
2024-07-02 14:14:12.390875 I | cephclient: writing config file /var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config
2024-07-02 14:14:12.391191 I | cephclient: generated admin config in /var/lib/rook/nvmeof-recoverer
2024-07-02 14:14:12.391224 D | ceph-csi: using "nvmeof-recoverer-system" for csi configmap namespace
2024-07-02 14:14:12.798559 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.43.224.72 Port:6789 Zone: NodeName:qemu2 DataPathMap:0xc000df7080 UseHostNetwork:false}
2024-07-02 14:14:12.798725 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:14:12.807948 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-07-02 14:14:12.807988 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2024-07-02 14:14:12.817133 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2024-07-02 14:14:12.817154 I | op-mon: waiting for mon quorum with [a]
2024-07-02 14:14:12.994262 I | op-mon: mons running: [a]
2024-07-02 14:14:12.994288 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:13.496072 I | op-mon: Monitors in quorum: [a]
2024-07-02 14:14:13.496098 I | op-mon: mons created: 1
2024-07-02 14:14:13.496124 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:13.595507 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:13.595666 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:13.657088 D | op-osd: checking osd processes status.
2024-07-02 14:14:13.657163 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:14.030327 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:14.030369 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:14.030439 I | op-mon: waiting for mon quorum with [a]
2024-07-02 14:14:14.034389 I | op-mon: mons running: [a]
2024-07-02 14:14:14.034407 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:14.050267 D | op-osd: validating status of osd.0
2024-07-02 14:14:14.050301 D | op-osd: osd.0 is healthy.
2024-07-02 14:14:14.050309 D | op-osd: validating status of osd.1
2024-07-02 14:14:14.050316 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:14:14.111581 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:14.116462 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:14.128707 D | op-mon: checking health of mons
2024-07-02 14:14:14.128720 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:14:14.526184 I | op-mon: Monitors in quorum: [a]
2024-07-02 14:14:14.526227 I | ceph-spec: not applying network settings for cluster "nvmeof-recoverer" ceph networks
2024-07-02 14:14:14.526239 D | op-mon: mon endpoints used are: a=10.43.224.72:6789
2024-07-02 14:14:14.526252 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-07-02 14:14:14.526258 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-07-02 14:14:14.526285 D | op-mon: Released lock for mon orchestration
2024-07-02 14:14:14.526294 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-07-02 14:14:14.526304 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-07-02 14:14:14.526344 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:14:14.526642 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:14.531828 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:14:14.531875 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:14.648198 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:14.648245 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:14.648390 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:14.648418 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:14.665571 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:14.669719 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:14.669733 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:14.672921 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:14.672963 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg peering"
2024-07-02 14:14:14.997897 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-07-02 14:14:14.997953 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:15.030790 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:14:15.030827 D | op-mon: targeting the mon count 1
2024-07-02 14:14:15.030842 D | op-mon: mon "a" found in quorum
2024-07-02 14:14:15.030849 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:14:15.035312 D | op-mon: Released lock for mon orchestration
2024-07-02 14:14:15.035444 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:14:15.430466 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-07-02 14:14:15.430564 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:16.020827 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-07-02 14:14:16.020922 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:16.280306 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:06 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001fc6a74}
2024-07-02 14:14:16.549195 D | op-cfg-keyring: updating secret for rook-csi-rbd-provisioner
2024-07-02 14:14:16.556682 D | op-cfg-keyring: updating secret for rook-csi-rbd-node
2024-07-02 14:14:16.564461 D | op-cfg-keyring: updating secret for rook-csi-cephfs-provisioner
2024-07-02 14:14:16.575556 D | op-cfg-keyring: updating secret for rook-csi-cephfs-node
2024-07-02 14:14:16.579790 I | ceph-csi: created kubernetes csi secrets for cluster "nvmeof-recoverer"
2024-07-02 14:14:16.579802 I | cephclient: getting or creating ceph auth key "client.crash"
2024-07-02 14:14:16.579812 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:17.162892 D | op-cfg-keyring: updating secret for rook-ceph-crash-collector-keyring
2024-07-02 14:14:17.167557 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "nvmeof-recoverer"
2024-07-02 14:14:17.167598 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-07-02 14:14:17.167627 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:17.704083 D | op-cfg-keyring: updating secret for rook-ceph-exporter-keyring
2024-07-02 14:14:17.708772 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "nvmeof-recoverer"
2024-07-02 14:14:17.708818 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-07-02 14:14:17.708846 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:18.176087 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-07-02 14:14:18.176104 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-07-02 14:14:18.176118 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:18.627399 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-07-02 14:14:18.627539 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-07-02 14:14:18.627580 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:19.027751 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-07-02 14:14:19.027808 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-07-02 14:14:19.027840 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:19.541989 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-07-02 14:14:19.542035 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-07-02 14:14:19.542064 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:19.673637 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:19.673671 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:19.971723 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-07-02 14:14:19.971786 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:20.196243 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:20.203573 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:20.435209 I | ceph-cluster-controller: desired value set-full-ratio=0.96 is already set
2024-07-02 14:14:20.435248 I | ceph-cluster-controller: desired value set-backfillfull-ratio=0.91 is already set
2024-07-02 14:14:20.435258 I | ceph-cluster-controller: desired value set-nearfull-ratio=0.88 is already set
2024-07-02 14:14:20.435270 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-07-02 14:14:20.435278 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-07-02 14:14:20.435305 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:20.703104 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:20.703159 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:20.703422 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:20.703471 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:20.724487 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:20.725896 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:20.725929 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:20.732252 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg peering"
2024-07-02 14:14:20.732285 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:20.896932 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "nvmeof-recoverer"
2024-07-02 14:14:20.897068 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-nvmeof-recoverer" in namespace "nvmeof-recoverer"
2024-07-02 14:14:20.897094 D | op-k8sutil: creating secret cluster-peer-token-nvmeof-recoverer
2024-07-02 14:14:20.920612 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-07-02 14:14:20.940892 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:20.940924 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:20.941973 I | op-mgr: start running mgr
2024-07-02 14:14:20.949384 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-07-02 14:14:20.949430 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:21.279253 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:06 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc0014799b4}
2024-07-02 14:14:21.460979 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-07-02 14:14:21.463989 D | op-cfg-keyring: updating secret for rook-ceph-mgr-a-keyring
2024-07-02 14:14:21.468573 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc00114e900}
2024-07-02 14:14:21.468742 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:14:21.480514 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2024-07-02 14:14:21.493564 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2024-07-02 14:14:21.499226 D | op-mgr: expected number 1 of mgrs found
2024-07-02 14:14:21.499293 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-07-02 14:14:21.512262 D | op-k8sutil: updating service rook-ceph-mgr-dashboard
2024-07-02 14:14:21.520440 D | op-k8sutil: creating service rook-ceph-mgr
2024-07-02 14:14:21.533922 D | op-k8sutil: updating service rook-ceph-mgr
2024-07-02 14:14:21.543501 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Configuring Ceph OSDs"
2024-07-02 14:14:21.543565 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:21.543662 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:21.543750 I | op-mgr: successful modules: mgr module(s) from the spec
2024-07-02 14:14:21.544087 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-07-02 14:14:21.544135 I | op-mgr: successful modules: balancer
2024-07-02 14:14:21.564749 I | op-osd: start running osds in namespace "nvmeof-recoverer"
2024-07-02 14:14:21.564765 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-07-02 14:14:21.564768 I | op-osd: no replacement of osds is requested
2024-07-02 14:14:21.565956 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:21.565987 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:21.576686 D | op-osd: 1 of 1 OSD Deployments need update
2024-07-02 14:14:21.576717 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-07-02 14:14:21.662694 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-07-02 14:14:21.662733 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-07-02 14:14:21.866148 I | op-osd: 2 of the 2 storage nodes are valid
2024-07-02 14:14:22.266425 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-qemu1 to start a new one
2024-07-02 14:14:22.268436 I | op-mgr: successful modules: prometheus
2024-07-02 14:14:22.277271 I | op-k8sutil: batch job rook-ceph-osd-prepare-qemu1 still exists
2024-07-02 14:14:22.289447 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-qemu1-4x497" is a ceph pod!
2024-07-02 14:14:22.289604 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:14:22.290224 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:14:22.291896 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:14:22.291941 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:14:22.294244 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:14:24.051101 D | op-osd: checking osd processes status.
2024-07-02 14:14:24.051163 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:24.521223 D | op-osd: validating status of osd.0
2024-07-02 14:14:24.521262 D | op-osd: osd.0 is healthy.
2024-07-02 14:14:24.521270 D | op-osd: validating status of osd.1
2024-07-02 14:14:24.521277 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:14:25.040354 D | op-mon: checking health of mons
2024-07-02 14:14:25.040403 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:14:25.040414 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:14:25.045941 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:14:25.045993 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:25.280681 I | op-k8sutil: batch job rook-ceph-osd-prepare-qemu1 deleted
2024-07-02 14:14:25.292360 I | op-osd: started OSD provisioning job for node "qemu1"
2024-07-02 14:14:25.303977 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-qemu2 to start a new one
2024-07-02 14:14:25.315518 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-qemu1-nl6cv" is a ceph pod!
2024-07-02 14:14:25.315647 D | ceph-nodedaemon-controller: reconciling node: "qemu1"
2024-07-02 14:14:25.316206 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:14:25.317757 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:14:25.317797 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:14:25.319243 I | op-k8sutil: batch job rook-ceph-osd-prepare-qemu2 still exists
2024-07-02 14:14:25.320818 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:14:25.356469 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-qemu2-66pvs" is a ceph pod!
2024-07-02 14:14:25.356588 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:14:25.357181 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:14:25.369008 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:14:25.381652 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:14:25.381690 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:14:25.393012 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:14:25.402349 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:14:25.402396 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:14:25.405047 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:14:25.598440 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:14:25.598498 D | op-mon: targeting the mon count 1
2024-07-02 14:14:25.598515 D | op-mon: mon "a" found in quorum
2024-07-02 14:14:25.598523 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:14:25.603491 D | op-mon: Released lock for mon orchestration
2024-07-02 14:14:25.603526 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:14:25.733109 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:25.733164 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:26.175717 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:26.182940 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:26.279353 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:06 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001cc4824}
2024-07-02 14:14:26.768951 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:26.768999 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:26.769249 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:26.769285 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:26.787844 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:26.789296 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:26.789333 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:26.795673 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:26.795713 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg peering"
2024-07-02 14:14:27.267088 I | op-mgr: the dashboard secret was already generated
2024-07-02 14:14:27.267136 I | op-mgr: setting ceph dashboard "admin" login creds
2024-07-02 14:14:27.267535 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/290107206 administrator --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:27.789956 D | exec: Running command: ceph dashboard ac-user-set-password admin -i /tmp/290107206 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:28.323249 I | op-k8sutil: batch job rook-ceph-osd-prepare-qemu2 deleted
2024-07-02 14:14:28.334780 I | op-osd: started OSD provisioning job for node "qemu2"
2024-07-02 14:14:28.338448 I | op-osd: OSD orchestration status for node qemu1 is "starting"
2024-07-02 14:14:28.338510 I | op-osd: OSD orchestration status for node qemu2 is "starting"
2024-07-02 14:14:28.359235 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-qemu2-jhvlj" is a ceph pod!
2024-07-02 14:14:28.359422 D | ceph-nodedaemon-controller: reconciling node: "qemu2"
2024-07-02 14:14:28.360253 D | ceph-spec: ceph version found "18.2.2-1573"
2024-07-02 14:14:28.375903 D | ceph-nodedaemon-controller: crash collector successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:14:28.390390 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "qemu2". operation: "updated"
2024-07-02 14:14:28.390443 D | op-k8sutil: creating service rook-ceph-exporter
2024-07-02 14:14:28.416331 D | op-k8sutil: updating service rook-ceph-exporter
2024-07-02 14:14:28.425612 D | op-k8sutil: returning version v1.28.10 instead of v1.28.10+rke2r1
2024-07-02 14:14:28.425662 D | ceph-nodedaemon-controller: deleting cronjob if it exists...
2024-07-02 14:14:28.428215 D | ceph-nodedaemon-controller: cronJob resource not found. Ignoring since object must be deleted.
2024-07-02 14:14:28.441360 I | op-osd: skipping osd checks for ok-to-stop
2024-07-02 14:14:28.441394 D | op-osd: updating OSDs: [0]
2024-07-02 14:14:28.445741 I | op-osd: updating OSD 0 on node "qemu2"
2024-07-02 14:14:28.445911 D | ceph-spec: setting periodicity to "daily". Supported periodicity are hourly, daily, weekly and monthly
2024-07-02 14:14:28.445936 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-07-02 14:14:28.445939 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-07-02 14:14:28.446003 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Progressing". "Processing OSD 0 on node \"qemu2\""
2024-07-02 14:14:28.471181 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:28.471214 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:28.479815 D | op-k8sutil: deployment "rook-ceph-osd-0" did not change. nothing to update
2024-07-02 14:14:28.541626 I | op-mgr: successfully set ceph dashboard creds
2024-07-02 14:14:28.541770 D | exec: Running command: ceph config get mgr mgr/dashboard/url_prefix --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:29.008705 D | exec: Running command: ceph config get mgr mgr/dashboard/ssl --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:29.381074 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_HOST --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:29.833098 D | exec: Running command: ceph config get mgr mgr/dashboard/PROMETHEUS_API_SSL_VERIFY --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:30.222036 D | ceph-spec: object "rook-ceph-osd-qemu2-status" matched on update
2024-07-02 14:14:30.222051 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu2-status"
2024-07-02 14:14:30.222722 I | op-osd: OSD orchestration status for node qemu2 is "orchestrating"
2024-07-02 14:14:30.278816 D | exec: Running command: ceph config get mgr mgr/dashboard/server_port --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:30.418139 I | op-osd: OSD orchestration status for node qemu1 is "orchestrating"
2024-07-02 14:14:30.418476 D | ceph-spec: object "rook-ceph-osd-qemu1-status" matched on update
2024-07-02 14:14:30.418488 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu1-status"
2024-07-02 14:14:30.654506 I | op-mgr: successful modules: dashboard
2024-07-02 14:14:31.280617 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:06 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001aec944}
2024-07-02 14:14:31.796235 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:31.796294 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:32.243338 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:32.249680 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:32.748056 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:32.748146 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:32.748373 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:32.748421 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:32.768315 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:32.772074 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:32.772111 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:32.779974 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg inactive, 1 pg peering"
2024-07-02 14:14:32.780018 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:34.522026 D | op-osd: checking osd processes status.
2024-07-02 14:14:34.522113 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:34.893277 D | ceph-spec: object "rook-ceph-osd-qemu2-status" matched on update
2024-07-02 14:14:34.893297 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu2-status"
2024-07-02 14:14:34.893885 I | op-osd: OSD orchestration status for node qemu2 is "completed"
2024-07-02 14:14:34.893931 D | op-osd: not creating deployment for OSD 0 which already exists
2024-07-02 14:14:34.900017 D | ceph-spec: object "rook-ceph-osd-qemu2-status" did not match on delete
2024-07-02 14:14:34.900055 D | ceph-spec: object "rook-ceph-osd-qemu2-status" did not match on delete
2024-07-02 14:14:34.900066 D | ceph-spec: object "rook-ceph-osd-qemu2-status" did not match on delete
2024-07-02 14:14:34.900075 D | ceph-spec: do not reconcile on "rook-ceph-osd-qemu2-status" config map changes
2024-07-02 14:14:34.900684 D | op-osd: not processing DELETED event for object "rook-ceph-osd-qemu2-status"
2024-07-02 14:14:35.004819 D | op-osd: validating status of osd.0
2024-07-02 14:14:35.004841 D | op-osd: osd.0 is healthy.
2024-07-02 14:14:35.004844 D | op-osd: validating status of osd.1
2024-07-02 14:14:35.004846 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:14:35.476366 D | ceph-spec: object "rook-ceph-osd-qemu1-status" matched on update
2024-07-02 14:14:35.476411 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-qemu1-status"
2024-07-02 14:14:35.476812 I | op-osd: OSD orchestration status for node qemu1 is "completed"
2024-07-02 14:14:35.483513 D | ceph-spec: object "rook-ceph-osd-qemu1-status" did not match on delete
2024-07-02 14:14:35.483630 D | ceph-spec: do not reconcile on "rook-ceph-osd-qemu1-status" config map changes
2024-07-02 14:14:35.483721 D | ceph-spec: object "rook-ceph-osd-qemu1-status" did not match on delete
2024-07-02 14:14:35.483819 D | ceph-spec: object "rook-ceph-osd-qemu1-status" did not match on delete
2024-07-02 14:14:35.486462 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:35.604317 D | op-mon: checking health of mons
2024-07-02 14:14:35.604367 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:14:35.604379 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:14:35.609569 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:14:35.609598 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:35.977650 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:35.977697 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:35.977808 D | exec: Running command: ceph osd require-osd-release reef --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:36.100396 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:14:36.100440 D | op-mon: targeting the mon count 1
2024-07-02 14:14:36.100456 D | op-mon: mon "a" found in quorum
2024-07-02 14:14:36.100464 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:14:36.104840 D | op-mon: Released lock for mon orchestration
2024-07-02 14:14:36.104861 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:14:36.292666 D | cluster-manager: Successfully executed nvmeof connect/disconnect job. output: %sb'NQN:nqn.2023-01.com.samsung.semiconductor:fc641c65-2548-4788-961f-a7ebaab3dc6a:1.3.S63UNG0T619221 disconnected 1 controller(s)'
2024-07-02 14:14:36.292710 D | nvmeofstorage-controller: successfully deleted the OSD deployment. Name: "rook-ceph-osd-1"
2024-07-02 14:14:36.296041 I | op-k8sutil: Removing previous job nvmeof-conn-control-job to start a new one
2024-07-02 14:14:36.305388 I | op-k8sutil: batch job nvmeof-conn-control-job still exists
2024-07-02 14:14:36.408705 D | cephclient: 
2024-07-02 14:14:36.408757 I | cephclient: successfully disallowed pre-reef osds and enabled all new reef-only functionality
2024-07-02 14:14:36.412485 D | op-osd: successfully deleted key rotation cron jobs
2024-07-02 14:14:36.412552 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:36.939606 I | op-osd: finished running OSDs in namespace "nvmeof-recoverer"
2024-07-02 14:14:36.939624 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "nvmeof-recoverer"
2024-07-02 14:14:36.939650 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:36.942339 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:36.942373 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:36.955409 E | ceph-spec: failed to update cluster condition to {Type:Ready Status:True Reason:ClusterCreated Message:Cluster created successfully LastHeartbeatTime:2024-07-02 14:14:36.939647272 +0000 UTC m=+252.465995024 LastTransitionTime:2024-07-02 14:12:30 +0000 UTC}. failed to update object "nvmeof-recoverer/nvmeof-recoverer" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "nvmeof-recoverer": the object has been modified; please apply your changes to the latest version and try again
2024-07-02 14:14:36.955460 D | ceph-csi: using "nvmeof-recoverer-system" for csi configmap namespace
2024-07-02 14:14:36.955598 I | ceph-cluster-controller: reporting cluster telemetry
2024-07-02 14:14:36.955660 D | op-config: setting "rook/version"="v1.14.0-alpha.0.227.ga75736a04-dirty" option in the mon config-key store
2024-07-02 14:14:36.955689 D | exec: Running command: ceph config-key set rook/version v1.14.0-alpha.0.227.ga75736a04-dirty --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:36.958645 D | ceph-cluster-controller: monitoring routine for "mon" is already running
2024-07-02 14:14:36.958677 D | ceph-cluster-controller: monitoring routine for "osd" is already running
2024-07-02 14:14:36.958685 D | ceph-cluster-controller: monitoring routine for "status" is already running
2024-07-02 14:14:36.958729 D | ceph-cluster-controller: successfully configured CephCluster "nvmeof-recoverer/nvmeof-recoverer"
2024-07-02 14:14:37.510966 D | telemetry: set telemetry key: rook/version=v1.14.0-alpha.0.227.ga75736a04-dirty
2024-07-02 14:14:37.512613 D | op-config: setting "rook/kubernetes/version"="v1.28.10+rke2r1" option in the mon config-key store
2024-07-02 14:14:37.512659 D | exec: Running command: ceph config-key set rook/kubernetes/version v1.28.10+rke2r1 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:37.780145 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:37.780204 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:38.097483 D | telemetry: set telemetry key: rook/kubernetes/version=v1.28.10+rke2r1
2024-07-02 14:14:38.097502 D | op-config: setting "rook/csi/version"="v3.11.0" option in the mon config-key store
2024-07-02 14:14:38.097517 D | exec: Running command: ceph config-key set rook/csi/version v3.11.0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:38.354981 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:38.361938 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:38.735872 D | telemetry: set telemetry key: rook/csi/version=v3.11.0
2024-07-02 14:14:38.735916 D | op-config: setting "rook/cluster/mon/max-id"="0" option in the mon config-key store
2024-07-02 14:14:38.735951 D | exec: Running command: ceph config-key set rook/cluster/mon/max-id 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:38.919197 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:38.919239 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:38.919395 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:38.919426 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:38.941513 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:38.943063 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:38.943099 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:38.949380 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:38.949419 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg inactive, 1 pg peering"
2024-07-02 14:14:39.201736 D | telemetry: set telemetry key: rook/cluster/mon/max-id=0
2024-07-02 14:14:39.201779 D | op-config: setting "rook/cluster/mon/count"="1" option in the mon config-key store
2024-07-02 14:14:39.201810 D | exec: Running command: ceph config-key set rook/cluster/mon/count 1 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:39.308396 I | op-k8sutil: batch job nvmeof-conn-control-job deleted
2024-07-02 14:14:39.316610 I | op-k8sutil: waiting for job nvmeof-conn-control-job to complete...
2024-07-02 14:14:39.320011 D | op-k8sutil: job is still initializing
2024-07-02 14:14:39.797230 D | telemetry: set telemetry key: rook/cluster/mon/count=1
2024-07-02 14:14:39.797276 D | op-config: setting "rook/cluster/mon/allow-multiple-per-node"="true" option in the mon config-key store
2024-07-02 14:14:39.797304 D | exec: Running command: ceph config-key set rook/cluster/mon/allow-multiple-per-node true --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:40.367100 D | telemetry: set telemetry key: rook/cluster/mon/allow-multiple-per-node=true
2024-07-02 14:14:40.367123 D | op-config: setting "rook/cluster/mon/pvc/enabled"="false" option in the mon config-key store
2024-07-02 14:14:40.367139 D | exec: Running command: ceph config-key set rook/cluster/mon/pvc/enabled false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:40.864170 D | telemetry: set telemetry key: rook/cluster/mon/pvc/enabled=false
2024-07-02 14:14:40.864214 D | op-config: setting "rook/cluster/mon/stretch/enabled"="false" option in the mon config-key store
2024-07-02 14:14:40.864253 D | exec: Running command: ceph config-key set rook/cluster/mon/stretch/enabled false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:41.362760 D | telemetry: set telemetry key: rook/cluster/mon/stretch/enabled=false
2024-07-02 14:14:41.362777 D | op-config: setting "rook/cluster/storage/device-set/count/total"="0" option in the mon config-key store
2024-07-02 14:14:41.362788 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/total 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:41.923271 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/total=0
2024-07-02 14:14:41.923319 D | op-config: setting "rook/cluster/storage/device-set/count/portable"="0" option in the mon config-key store
2024-07-02 14:14:41.923350 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/portable 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:42.521628 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/portable=0
2024-07-02 14:14:42.521674 D | op-config: setting "rook/cluster/storage/device-set/count/non-portable"="0" option in the mon config-key store
2024-07-02 14:14:42.521703 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/non-portable 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:43.150498 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/non-portable=0
2024-07-02 14:14:43.150546 D | op-config: setting "rook/cluster/network/provider"="" option in the mon config-key store
2024-07-02 14:14:43.150573 D | exec: Running command: ceph config-key set rook/cluster/network/provider  --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:43.729183 D | telemetry: set telemetry key: rook/cluster/network/provider=
2024-07-02 14:14:43.729236 D | op-config: setting "rook/cluster/external-mode"="false" option in the mon config-key store
2024-07-02 14:14:43.729266 D | exec: Running command: ceph config-key set rook/cluster/external-mode false --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:43.949915 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:43.949979 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:44.331671 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:39 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001ed1dc4}
2024-07-02 14:14:44.338158 D | telemetry: set telemetry key: rook/cluster/external-mode=false
2024-07-02 14:14:44.338170 I | ceph-cluster-controller: reporting node telemetry
2024-07-02 14:14:44.345188 D | op-config: setting "rook/node/count/kubernetes-total"="3" option in the mon config-key store
2024-07-02 14:14:44.345241 D | exec: Running command: ceph config-key set rook/node/count/kubernetes-total 3 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:44.443446 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:44.450197 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:44.778360 D | telemetry: set telemetry key: rook/node/count/kubernetes-total=3
2024-07-02 14:14:44.785657 D | op-config: setting "rook/node/count/with-ceph-daemons"="1" option in the mon config-key store
2024-07-02 14:14:44.785708 D | exec: Running command: ceph config-key set rook/node/count/with-ceph-daemons 1 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:44.902316 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:44.902357 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:44.902546 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:44.902579 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:44.921343 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:44.924048 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:44.924059 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:44.929219 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:44.929257 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg inactive, 1 pg peering"
2024-07-02 14:14:45.005131 D | op-osd: checking osd processes status.
2024-07-02 14:14:45.005193 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:45.283920 D | telemetry: set telemetry key: rook/node/count/with-ceph-daemons=1
2024-07-02 14:14:45.294261 D | op-config: setting "rook/node/count/with-csi-rbd-plugin"="3" option in the mon config-key store
2024-07-02 14:14:45.294310 D | exec: Running command: ceph config-key set rook/node/count/with-csi-rbd-plugin 3 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:45.394475 D | op-osd: validating status of osd.0
2024-07-02 14:14:45.394490 D | op-osd: osd.0 is healthy.
2024-07-02 14:14:45.394492 D | op-osd: validating status of osd.1
2024-07-02 14:14:45.394495 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:14:45.858892 D | telemetry: set telemetry key: rook/node/count/with-csi-rbd-plugin=3
2024-07-02 14:14:45.869029 D | op-config: setting "rook/node/count/with-csi-cephfs-plugin"="3" option in the mon config-key store
2024-07-02 14:14:45.869068 D | exec: Running command: ceph config-key set rook/node/count/with-csi-cephfs-plugin 3 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:46.105626 D | op-mon: checking health of mons
2024-07-02 14:14:46.105669 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:14:46.105678 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:14:46.110754 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:14:46.110795 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:46.355550 D | telemetry: set telemetry key: rook/node/count/with-csi-cephfs-plugin=3
2024-07-02 14:14:46.360806 D | op-config: setting "rook/node/count/with-csi-nfs-plugin"="0" option in the mon config-key store
2024-07-02 14:14:46.360856 D | exec: Running command: ceph config-key set rook/node/count/with-csi-nfs-plugin 0 --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:46.661232 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:14:46.661275 D | op-mon: targeting the mon count 1
2024-07-02 14:14:46.661290 D | op-mon: mon "a" found in quorum
2024-07-02 14:14:46.661298 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:14:46.666793 D | op-mon: Released lock for mon orchestration
2024-07-02 14:14:46.666839 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:14:46.933404 D | telemetry: set telemetry key: rook/node/count/with-csi-nfs-plugin=0
2024-07-02 14:14:49.322100 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:39 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001b4dd14}
2024-07-02 14:14:49.930079 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:49.930186 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:50.496633 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:50.504396 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:51.084789 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:51.084830 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:51.084976 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:51.085004 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:51.107598 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:51.110366 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:51.110430 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:51.115456 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:51.115498 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg inactive, 1 pg peering"
2024-07-02 14:14:54.321095 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:39 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001ba1094}
2024-07-02 14:14:55.395537 D | op-osd: checking osd processes status.
2024-07-02 14:14:55.395611 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:55.887755 D | op-osd: validating status of osd.0
2024-07-02 14:14:55.887795 D | op-osd: osd.0 is healthy.
2024-07-02 14:14:55.887803 D | op-osd: validating status of osd.1
2024-07-02 14:14:55.887809 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:14:56.115650 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:14:56.115707 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:14:56.624197 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:14:56.631670 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:56.667145 D | op-mon: checking health of mons
2024-07-02 14:14:56.667185 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:14:56.667193 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:14:56.672115 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:14:56.672167 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:14:57.138744 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:14:57.138782 D | op-mon: targeting the mon count 1
2024-07-02 14:14:57.138799 D | op-mon: mon "a" found in quorum
2024-07-02 14:14:57.138806 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:14:57.143662 D | op-mon: Released lock for mon orchestration
2024-07-02 14:14:57.143704 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:14:57.159973 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:57.160012 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:14:57.160166 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:14:57.160200 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:14:57.180812 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:14:57.182704 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:14:57.182736 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:14:57.188424 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:14:57.188460 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg inactive, 1 pg peering"
2024-07-02 14:14:59.321055 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:39 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001ba16c4}
2024-07-02 14:15:02.189456 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:15:02.189516 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:15:02.698964 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:15:02.706923 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:15:03.556586 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:15:03.556638 D | cephclient: {"mon":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"mgr":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"osd":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":1},"overall":{"ceph version 18.2.2-1573-g9696e87a (9696e87a8460cb20414f7ea730f189addd2e5455) reef (stable)":3}}
2024-07-02 14:15:03.556889 D | ceph-cluster-controller: updating ceph cluster "nvmeof-recoverer" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-07-02 14:15:03.556953 D | ceph-spec: CephCluster "nvmeof-recoverer" status: "Ready". "Cluster created successfully"
2024-07-02 14:15:03.596165 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2024-07-02 14:15:03.597866 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:15:03.597904 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:15:03.617566 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "OSD_DOWN", message: "1 osds down"
2024-07-02 14:15:03.617606 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "PG_AVAILABILITY", message: "Reduced data availability: 1 pg inactive, 1 pg peering"
2024-07-02 14:15:04.322547 D | op-k8sutil: job is still running. Status={Conditions:[] StartTime:2024-07-02 14:14:39 +0000 UTC CompletionTime:<nil> Active:1 Succeeded:0 Failed:0 Terminating:<nil> CompletedIndexes: FailedIndexes:<nil> UncountedTerminatedPods:&UncountedTerminatedPods{Succeeded:[],Failed:[],} Ready:0xc001e35a04}
2024-07-02 14:15:05.888020 D | op-osd: checking osd processes status.
2024-07-02 14:15:05.888091 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:15:06.368096 D | op-osd: validating status of osd.0
2024-07-02 14:15:06.368138 D | op-osd: osd.0 is healthy.
2024-07-02 14:15:06.368146 D | op-osd: validating status of osd.1
2024-07-02 14:15:06.368154 D | op-osd: osd.1 is marked 'DOWN'
2024-07-02 14:15:07.144776 D | op-mon: checking health of mons
2024-07-02 14:15:07.144817 D | op-mon: Acquiring lock for mon orchestration
2024-07-02 14:15:07.144831 D | op-mon: Acquired lock for mon orchestration
2024-07-02 14:15:07.152152 D | op-mon: Checking health for mons in cluster "nvmeof-recoverer"
2024-07-02 14:15:07.152198 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:15:07.705598 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.43.224.72:6789/0 PublicAddr:10.43.224.72:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.43.224.72:3300 Nonce:0} {Type:v1 Addr:10.43.224.72:6789 Nonce:0}]}}]}}
2024-07-02 14:15:07.705636 D | op-mon: targeting the mon count 1
2024-07-02 14:15:07.705652 D | op-mon: mon "a" found in quorum
2024-07-02 14:15:07.705659 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-07-02 14:15:07.710056 D | op-mon: Released lock for mon orchestration
2024-07-02 14:15:07.710079 D | op-mon: ceph mon status in namespace "nvmeof-recoverer" check interval "10s"
2024-07-02 14:15:08.618323 D | ceph-cluster-controller: checking health of cluster
2024-07-02 14:15:08.618356 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring
2024-07-02 14:15:09.164523 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[OSD_DOWN:{Severity:HEALTH_WARN Summary:{Message:1 osds down}} PG_AVAILABILITY:{Severity:HEALTH_WARN Summary:{Message:Reduced data availability: 1 pg inactive, 1 pg peering}}]} FSID:912bd986-ac3b-4c2d-85fa-be3aeda3cb17 ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:20 NumOsd:2 NumUpOsd:1 NumInOsd:2 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:peering Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:56266752 AvailableBytes:7681455697920 TotalBytes:7681511964672 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-07-02 14:15:09.173188 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=nvmeof-recoverer --conf=/var/lib/rook/nvmeof-recoverer/nvmeof-recoverer.config --name=client.admin --keyring=/var/lib/rook/nvmeof-recoverer/client.admin.keyring --format json
2024-07-02 14:15:09.332715 D | cluster-manager: Successfully executed nvmeof connect/disconnect job. output: %s/dev/nvme3n1
2024-07-02 14:15:09.332737 D | nvmeofstorage-controller: successfully reassigned the device. SubNQN: nqn.2023-01.com.samsung.semiconductor:fc641c65-2548-4788-961f-a7ebaab3dc6a:1.3.S63UNG0T619221, DeviceName: /dev/nvme2n1, AttachedNode: qemu2
2024-07-02 14:15:09.360270 D | ceph-spec: found 1 ceph clusters in namespace "nvmeof-recoverer"
2024-07-02 14:15:09.360288 D | ceph-cluster-controller: update event on CephCluster CR
2024-07-02 14:15:09.360452 D | clusterdisruption-controller: reconciling "nvmeof-recoverer/nvmeof-recoverer"
2024-07-02 14:15:09.361630 D | nvmeofstorage-controller: CephCluster updated successfully. oldNode: qemu1, oldDevicePath: /dev/nvme2n1, newNode: qemu2, newDevicePath: /dev/nvme3n1
2024-07-02 14:15:09.361655 D | nvmeofstorage-controller: successfully configured NvmeOfStorage "nvmeof-recoverer/rook-ceph-osd-1-bfd7ffdbc-c7bt2"
2024-07-02 14:15:09.361711 D | nvmeofstorage-controller: reconciling NvmeOfStorage. Request.Namespace: nvmeof-recoverer, Request.Name: rook-ceph-osd-1-bfd7ffdbc-c7bt2
2024-07-02 14:15:09.361721 D | op-k8sutil: removing rook-ceph-osd-1 deployment if it exists
2024-07-02 14:15:09.361726 I | op-k8sutil: removing deployment rook-ceph-osd-1 if it exists
2024-07-02 14:15:09.361876 I | ceph-cluster-controller: CR has changed for "nvmeof-recoverer". diff=  v1.ClusterSpec{
  	CephVersion: {Image: "quay.io/ceph/daemon-base:latest-reef-devel"},
  	Storage: v1.StorageScopeSpec{
  		Nodes: []v1.Node{
  			{
  				Name:      "qemu1",
  				Resources: {},
  				Config:    nil,
  				Selection: v1.Selection{
  					UseAllDevices:        nil,
  					DeviceFilter:         "",
  					DevicePathFilter:     "",
- 					Devices:              []v1.Device{{Name: "/dev/nvme2n1"}},
+ 					Devices:              nil,
  					VolumeClaimTemplates: nil,
  				},
  			},
  			{
  				Name:      "qemu2",
  				Resources: {},
  				Config:    nil,
  				Selection: v1.Selection{
  					UseAllDevices:    nil,
  					DeviceFilter:     "",
  					DevicePathFilter: "",
  					Devices: []v1.Device{
  						{Name: "/dev/nvme2n1"},
+ 						{
+ 							Name:   "/dev/nvme3n1\n",
+ 							Config: map[string]string{"failureDomain": "fabric-host-pbssd1"},
+ 						},
  					},
  					VolumeClaimTemplates: nil,
  				},
  			},
  		},
  		UseAllNodes:           false,
  		OnlyApplyOSDPlacement: false,
  		... // 8 identical fields
  	},
  	Annotations: nil,
  	Labels:      nil,
  	... // 23 identical fields
  }
2024-07-02 14:15:09.361994 I | operator: reloading operator's CRDs manager, cancelling all orchestrations!
2024-07-02 14:15:09.362077 I | op-mon: stopping monitoring of mons in namespace "nvmeof-recoverer"
I0702 14:15:09.362105       1 manager.go:148] "msg"="stopping provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="nvmeof-recoverer.ceph.rook.io/bucket" "reason"="context canceled"
2024-07-02 14:15:09.362154 I | op-osd: stopping monitoring of OSDs in namespace "nvmeof-recoverer"
panic: failed to delete OSD deployment "rook-ceph-osd-1" in namespace "nvmeof-recoverer": failed to delete rook-ceph-osd-1. Delete "https://10.43.0.1:443/apis/apps/v1/namespaces/nvmeof-recoverer/deployments/rook-ceph-osd-1": context canceled [recovered]
	panic: failed to delete OSD deployment "rook-ceph-osd-1" in namespace "nvmeof-recoverer": failed to delete rook-ceph-osd-1. Delete "https://10.43.0.1:443/apis/apps/v1/namespaces/nvmeof-recoverer/deployments/rook-ceph-osd-1": context canceled

goroutine 731 [running]:
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile.func1()
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:116 +0x1e5
panic({0x208ba40?, 0xc00062c2a0?})
	/remote_home/chkang0912/gvm/gos/go1.21.9/src/runtime/panic.go:914 +0x21f
github.com/rook/rook/pkg/operator/ceph/nvmeof_recoverer/nvmeofstorage.(*ReconcileNvmeOfStorage).cleanupOSD(0xc000a90e10, {0xc001ed0720, 0x10}, {{0xc000b3ac00, 0x5d}, 0x481, {0xc001e35340, 0x5}, {0xc001e35360, 0xc}, ...})
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/src/github.com/rook/rook/pkg/operator/ceph/nvmeof_recoverer/nvmeofstorage/controller.go:270 +0x39b
github.com/rook/rook/pkg/operator/ceph/nvmeof_recoverer/nvmeofstorage.(*ReconcileNvmeOfStorage).Reconcile(0xc000a90e10, {0x2c049e0, 0xc0001c90e0}, {{{0xc001ed0720?, 0x5?}, {0xc001748480?, 0xc001b6fd08?}}})
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/src/github.com/rook/rook/pkg/operator/ceph/nvmeof_recoverer/nvmeofstorage/controller.go:180 +0x5bc
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile(0x2c09540?, {0x2c049e0?, 0xc0001c90e0?}, {{{0xc001ed0720?, 0xb?}, {0xc001748480?, 0x0?}}})
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:119 +0xb7
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler(0xc000aaa640, {0x2c04a18, 0xc000a3fd10}, {0x22defc0?, 0xc000ba1080?})
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:316 +0x3cc
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem(0xc000aaa640, {0x2c04a18, 0xc000a3fd10})
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:266 +0x1af
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2()
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:227 +0x79
created by sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2 in goroutine 186
	/remote_home/chkang0912/gvm/pkgsets/go1.21.9/global/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:223 +0x565
